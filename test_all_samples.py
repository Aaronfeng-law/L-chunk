#!/usr/bin/env python3
"""
æ¸¬è©¦æ‰€æœ‰ samples æ–‡ä»¶çš„ line-based chunking
æ•´åˆä¿®æ­£å¾Œçš„ç‰¹æ®Šæ¨™è¨˜æª¢æ¸¬åŠŸèƒ½
"""

import warnings
warnings.filterwarnings('ignore')

from pathlib import Path
from src.lchunk.detectors.adaptive_hybrid import IntelligentHybridDetector

def test_all_samples():
    """æ¸¬è©¦æ‰€æœ‰ samples æ–‡ä»¶"""
    print("ğŸš€ æ¸¬è©¦æ‰€æœ‰ samples æ–‡ä»¶ - Line-Based Chunking")
    print("="*80)
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = IntelligentHybridDetector(model_path if Path(model_path).exists() else None)
    
    # samples ç›®éŒ„
    samples_dir = Path("data/samples")
    
    if not samples_dir.exists():
        print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {samples_dir}")
        return
    
    # ç²å–æ‰€æœ‰ JSON æ–‡ä»¶
    json_files = list(samples_dir.glob("*.json"))
    if not json_files:
        print(f"âŒ åœ¨ {samples_dir} ä¸­æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ:")
    for file in json_files:
        print(f"   - {file.name}")
    print()
    
    # è™•ç†æ¯å€‹æª”æ¡ˆ
    all_results = []
    summary_stats = {
        'total_files': len(json_files),
        'successful_files': 0,
        'failed_files': 0,
        'total_chunks': 0,
        'total_lines': 0,
        'special_markers_found': 0,
        'learning_regions': {'S-D': 0, 'R-D': 0, 'å…¨æ–‡': 0}
    }
    
    for i, json_file in enumerate(json_files, 1):
        print(f"\n[{i}/{len(json_files)}] è™•ç†: {json_file.name}")
        print("-" * 60)
        
        try:
            result = detector.process_single_file(json_file)
            
            if result and result.line_based_chunks:
                all_results.append(result)
                summary_stats['successful_files'] += 1
                summary_stats['total_chunks'] += len(result.line_based_chunks)
                summary_stats['total_lines'] += result.processing_stats['total_lines']
                summary_stats['learning_regions'][result.learning_region] += 1
                
                # è¨ˆç®—ç‰¹æ®Šæ¨™è¨˜æ•¸é‡
                special_chunks = [c for c in result.line_based_chunks 
                                if c.chunk_type in ['main_text', 'facts', 'reasons', 'facts_and_reasons']]
                summary_stats['special_markers_found'] += len(special_chunks)
                
                # é¡¯ç¤ºæª”æ¡ˆçµæœ
                print(f"âœ… æˆåŠŸè™•ç†")
                print(f"   ğŸ“Š å­¸ç¿’æ¨¡å¼: {result.learning_region}")
                print(f"   ğŸ“ ç¸½è¡Œæ•¸: {result.processing_stats['total_lines']:,}")
                print(f"   ğŸ§© ç”Ÿæˆåˆ†å¡Š: {len(result.line_based_chunks)}")
                print(f"   ğŸ¯ ç‰¹æ®Šæ¨™è¨˜: {len(special_chunks)} å€‹")
                
                if special_chunks:
                    print("   ğŸ“ æª¢æ¸¬åˆ°çš„ç‰¹æ®Šæ¨™è¨˜:")
                    for chunk in special_chunks:
                        content = chunk.content_lines[0] if chunk.content_lines else ''
                        print(f"      - {chunk.chunk_type}: è¡Œ {chunk.start_line + 1} ã€Œ{content.strip()}ã€")
                
                # å±¤ç´šçµ±è¨ˆ
                level_stats = {}
                for chunk in result.line_based_chunks:
                    level = chunk.level
                    level_stats[level] = level_stats.get(level, 0) + 1
                
                print("   ğŸ“Š å±¤ç´šåˆ†å¸ƒ:")
                for level in sorted(level_stats.keys()):
                    print(f"      Lv {level}: {level_stats[level]} å€‹åˆ†å¡Š")
                
            else:
                print(f"âŒ è™•ç†å¤±æ•—")
                summary_stats['failed_files'] += 1
                
        except Exception as e:
            print(f"âŒ è™•ç†éŒ¯èª¤: {e}")
            summary_stats['failed_files'] += 1
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š ç¸½çµå ±å‘Š")
    print("="*80)
    
    print(f"ğŸ“ è™•ç†æª”æ¡ˆ: {summary_stats['total_files']} å€‹")
    print(f"âœ… æˆåŠŸ: {summary_stats['successful_files']} å€‹")
    print(f"âŒ å¤±æ•—: {summary_stats['failed_files']} å€‹")
    print(f"ğŸ“ ç¸½è¡Œæ•¸: {summary_stats['total_lines']:,} è¡Œ")
    print(f"ğŸ§© ç¸½åˆ†å¡Šæ•¸: {summary_stats['total_chunks']:,} å€‹")
    print(f"ğŸ¯ ç‰¹æ®Šæ¨™è¨˜ç¸½æ•¸: {summary_stats['special_markers_found']} å€‹")
    
    print(f"\nğŸ“Š å­¸ç¿’å€é–“åˆ†å¸ƒ:")
    for region, count in summary_stats['learning_regions'].items():
        if count > 0:
            print(f"   {region}: {count} å€‹æª”æ¡ˆ")
    
    if all_results:
        print(f"\nğŸ“ˆ å¹³å‡çµ±è¨ˆ:")
        avg_lines = summary_stats['total_lines'] / len(all_results)
        avg_chunks = summary_stats['total_chunks'] / len(all_results)
        avg_markers = summary_stats['special_markers_found'] / len(all_results)
        
        print(f"   å¹³å‡è¡Œæ•¸: {avg_lines:.0f} è¡Œ/æª”æ¡ˆ")
        print(f"   å¹³å‡åˆ†å¡Š: {avg_chunks:.0f} å€‹/æª”æ¡ˆ")
        print(f"   å¹³å‡ç‰¹æ®Šæ¨™è¨˜: {avg_markers:.1f} å€‹/æª”æ¡ˆ")
        
        # è©³ç´°å±¤ç´šçµ±è¨ˆ
        print(f"\nğŸ” è©³ç´°åˆ†æ:")
        
        # åˆä½µæ‰€æœ‰æª”æ¡ˆçš„å±¤ç´šçµ±è¨ˆ
        combined_level_stats = {}
        for result in all_results:
            for chunk in result.line_based_chunks:
                level = chunk.level
                chunk_type = chunk.chunk_type
                key = f"Lv_{level}_{chunk_type}"
                combined_level_stats[key] = combined_level_stats.get(key, 0) + 1
        
        print("   è·¨æª”æ¡ˆå±¤ç´šåˆ†å¸ƒ:")
        for key in sorted(combined_level_stats.keys()):
            print(f"      {key}: {combined_level_stats[key]} å€‹")

if __name__ == "__main__":
    test_all_samples()
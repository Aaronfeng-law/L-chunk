#!/usr/bin/env python3
"""
Comprehensive Analysis Script for L-chunk Project
ç”Ÿæˆå®Œæ•´çš„åˆ¤æ±ºæ›¸åˆ†æçµ±è¨ˆå ±å‘Š
"""

import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
import re
import time
from datetime import datetime

# Import from judgment_splitter
sys.path.append('.')
from .splitter import process_single_file, find_section_patterns, extract_dates_from_text

def analyze_filtered_dataset():
    """åˆ†æå®Œæ•´çš„ filtered æ•¸æ“šé›†"""
    filtered_dir = Path("data/processed/filtered")
    
    if not filtered_dir.exists():
        print(f"âŒ Filtered directory {filtered_dir} not found")
        return None
    
    json_files = list(filtered_dir.glob("*.json"))
    print(f"ğŸ“Š Found {len(json_files)} JSON files in filtered dataset")
    
    # çµ±è¨ˆæ•¸æ“š
    stats = {
        'total_files': len(json_files),
        'successful_files': 0,
        'failed_files': 0,
        'section_stats': defaultdict(list),
        'case_types': Counter(),
        'year_distribution': Counter(),
        'court_types': Counter(),
        'processing_errors': [],
        'section_presence': Counter(),
        'empty_sections': Counter(),
        'file_sizes': [],
        'date_extraction_stats': {
            'files_with_dates': 0,
            'total_dates_found': 0,
            'date_patterns': Counter()
        },
        # æ–°å¢ï¼šç¬¦è™ŸåŒ–æ–‡ä»¶åˆ†é¡çµ±è¨ˆ
        'file_categories': {
            # M=ä¸»æ–‡, F=äº‹å¯¦, R=ç†ç”±, D1=ç¬¬ä¸€å€‹æ—¥æœŸ, D2=ç¬¬äºŒå€‹æ—¥æœŸ, N=ç„¡å…§å®¹
            # ä¾‹å¦‚: MFR = æœ‰ä¸»æ–‡+äº‹å¯¦+ç†ç”±, MF = æœ‰ä¸»æ–‡+äº‹å¯¦, R = åªæœ‰ç†ç”±, N = ç„¡å…§å®¹
        },
        'symbol_categories': {},            # å‹•æ…‹ç”Ÿæˆçš„ç¬¦è™Ÿåˆ†é¡
        'detailed_stats': {
            'has_header': 0,
            'has_main_text': 0,
            'has_facts': 0,
            'has_reasons': 0,
            'has_facts_and_reasons': 0,
            'has_footer': 0
        }
    }
    
    print("ğŸ”„ Processing files...")
    start_time = time.time()
    
    for i, json_file in enumerate(json_files):
        if i % 100 == 0:
            print(f"  Progress: {i}/{len(json_files)} ({i/len(json_files)*100:.1f}%)")
        
        try:
            # åˆ†ææª”æ¡ˆåç¨±æ¨¡å¼ (TPDM,å¹´åº¦,æ¡ˆä»¶é¡å‹,ç·¨è™Ÿ,æ—¥æœŸ,ç‰ˆæœ¬.json)
            filename_parts = json_file.stem.split(',')
            if len(filename_parts) >= 6:
                court = filename_parts[0]
                year = filename_parts[1]
                case_type = filename_parts[2]
                case_number = filename_parts[3]
                date_str = filename_parts[4]
                version = filename_parts[5]
                
                stats['year_distribution'][year] += 1
                stats['case_types'][case_type] += 1
                stats['court_types'][court] += 1
            
            # è™•ç†æª”æ¡ˆå…§å®¹
            success, result = process_single_file(json_file)
            
            if success:
                # è®€å–åŸå§‹ JSON æ•¸æ“šä»¥ç²å–å…¨æ–‡å…§å®¹
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                stats['successful_files'] += 1
                
                # è¨˜éŒ„å„ç« ç¯€çµ±è¨ˆ
                for section_name, content in result['sections'].items():
                    line_count = len(content) if content else 0
                    stats['section_stats'][section_name].append(line_count)
                    
                    if line_count > 0:
                        stats['section_presence'][section_name] += 1
                    else:
                        stats['empty_sections'][section_name] += 1
                
                # æ–°å¢ï¼šè©³ç´°çš„ç« ç¯€å­˜åœ¨çµ±è¨ˆ
                sections = result['sections']
                has_header = bool(sections.get('header', []))
                has_main_text = bool(sections.get('main_text', []))
                has_facts = bool(sections.get('facts', []))
                has_reasons = bool(sections.get('reasons', []))
                has_facts_and_reasons = bool(sections.get('facts_and_reasons', []))
                has_footer = bool(sections.get('footer', []))
                
                # æ›´æ–°è©³ç´°çµ±è¨ˆ
                if has_header: stats['detailed_stats']['has_header'] += 1
                if has_main_text: stats['detailed_stats']['has_main_text'] += 1
                if has_facts: stats['detailed_stats']['has_facts'] += 1
                if has_reasons: stats['detailed_stats']['has_reasons'] += 1
                if has_facts_and_reasons: stats['detailed_stats']['has_facts_and_reasons'] += 1
                if has_footer: stats['detailed_stats']['has_footer'] += 1
                
                # æ–‡ä»¶åˆ†é¡é‚è¼¯ - æ¯å€‹æ–‡ä»¶åªæœƒè¢«åˆ†åˆ°ä¸€å€‹é¡åˆ¥
                file_info = {
                    'filename': json_file.name,
                    'has_header': has_header,
                    'has_main_text': has_main_text,
                    'has_facts': has_facts,
                    'has_reasons': has_reasons,
                    'has_facts_and_reasons': has_facts_and_reasons,
                    'has_footer': has_footer,
                    'facts_lines': len(sections.get('facts', [])),
                    'reasons_lines': len(sections.get('reasons', [])),
                    'combined_lines': len(sections.get('facts_and_reasons', [])),
                    'main_text_lines': len(sections.get('main_text', []))
                }
                
                # æ—¥æœŸæå–çµ±è¨ˆ - ä½¿ç”¨å…¨æ–‡æª¢æ¸¬æ›´å¯é 
                content_text = data['JFULL']
                dates_found = extract_dates_from_text(content_text)
                
                # ç”Ÿæˆç¬¦è™ŸåŒ–åˆ†é¡ - ä¿®æ­£ç‰ˆ
                symbol = ""
                if has_main_text:
                    symbol += "M"
                if has_facts_and_reasons:
                    symbol += "S"  # S = Simplified (äº‹å¯¦åŠç†ç”±åˆä½µ)
                elif has_facts:
                    symbol += "F"
                if has_reasons and not has_facts_and_reasons:
                    symbol += "R"  # åªæœ‰ç•¶æ²’æœ‰åˆä½µæ™‚æ‰åŠ R
                
                # æ—¥æœŸæª¢æ¸¬ - ç°¡åŒ–ç‚ºæ˜¯å¦æœ‰æ—¥æœŸ
                if dates_found:
                    if len(dates_found) >= 2:
                        symbol += "D2"  # æœ‰å¤šå€‹æ—¥æœŸ
                    else:
                        symbol += "D1"  # æœ‰ä¸€å€‹æ—¥æœŸ
                
                # å¦‚æœæ²’æœ‰ä»»ä½•å…§å®¹
                if not symbol or symbol == "":
                    symbol = "N"
                
                # å°‡æ–‡ä»¶æ­¸é¡åˆ°å°æ‡‰çš„ç¬¦è™Ÿé¡åˆ¥
                if symbol not in stats['symbol_categories']:
                    stats['symbol_categories'][symbol] = []
                
                file_info = {
                    'filename': json_file.name,
                    'symbol': symbol,
                    'has_header': has_header,
                    'has_main_text': has_main_text,
                    'has_facts': has_facts,
                    'has_reasons': has_reasons,
                    'has_facts_and_reasons': has_facts_and_reasons,
                    'has_footer': has_footer,
                    'facts_lines': len(sections.get('facts', [])),
                    'reasons_lines': len(sections.get('reasons', [])),
                    'combined_lines': len(sections.get('facts_and_reasons', [])),
                    'main_text_lines': len(sections.get('main_text', [])),
                    'dates_count': len(dates_found),
                    'header_lines': len(sections.get('header', [])),
                    'footer_lines': len(sections.get('footer', []))
                }
                
                stats['symbol_categories'][symbol].append(file_info)
                
                # ä¿æŒèˆŠçš„åˆ†é¡é‚è¼¯ç”¨æ–¼å‘å¾Œå…¼å®¹
                stats['file_categories'] = stats.get('file_categories', {})
                if symbol not in stats['file_categories']:
                    stats['file_categories'][symbol] = []
                stats['file_categories'][symbol].append(file_info)
                
                if dates_found:
                    stats['date_extraction_stats']['files_with_dates'] += 1
                    stats['date_extraction_stats']['total_dates_found'] += len(dates_found)
                    for date in dates_found:
                        if isinstance(date, dict) and 'year' in date and 'month' in date:
                            stats['date_extraction_stats']['date_patterns'][f"{date['year']}å¹´{date['month']}æœˆ"] += 1
                        elif isinstance(date, tuple) and len(date) >= 2:
                            stats['date_extraction_stats']['date_patterns'][f"{date[0]}å¹´{date[1]}æœˆ"] += 1
                
                # æª”æ¡ˆå¤§å°çµ±è¨ˆ
                with open(json_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    stats['file_sizes'].append(len(content))
                    
            else:
                stats['failed_files'] += 1
                error_info = {
                    'filename': json_file.name,
                    'symbol': 'ERROR',
                    'error': str(result)
                }
                stats['processing_errors'].append(error_info)
                # éŒ¯èª¤æ–‡ä»¶ä¹ŸåŠ å…¥ç¬¦è™Ÿåˆ†é¡
                if 'ERROR' not in stats['symbol_categories']:
                    stats['symbol_categories']['ERROR'] = []
                stats['symbol_categories']['ERROR'].append(error_info)
                
        except Exception as e:
            stats['failed_files'] += 1
            error_info = {
                'filename': json_file.name,
                'symbol': 'ERROR',
                'error': f"Exception: {str(e)}"
            }
            stats['processing_errors'].append(error_info)
            # éŒ¯èª¤æ–‡ä»¶ä¹ŸåŠ å…¥ç¬¦è™Ÿåˆ†é¡
            if 'ERROR' not in stats['symbol_categories']:
                stats['symbol_categories']['ERROR'] = []
            stats['symbol_categories']['ERROR'].append(error_info)
    
    processing_time = time.time() - start_time
    stats['processing_time'] = processing_time
    
    return stats

def generate_comprehensive_report(stats):
    """ç”Ÿæˆç¶œåˆçµ±è¨ˆå ±å‘Š"""
    if not stats:
        return
    
    report_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    report = f"""
# L-chunk åˆ¤æ±ºæ›¸åˆ†æç¶œåˆå ±å‘Š
ç”Ÿæˆæ™‚é–“: {report_time}
è™•ç†æ™‚é–“: {stats['processing_time']:.2f} ç§’

## ğŸ“Š æ•´é«”çµ±è¨ˆ

### æª”æ¡ˆè™•ç†çµ±è¨ˆ
- ç¸½æª”æ¡ˆæ•¸: {stats['total_files']:,}
- æˆåŠŸè™•ç†: {stats['successful_files']:,} ({stats['successful_files']/stats['total_files']*100:.1f}%)
- è™•ç†å¤±æ•—: {stats['failed_files']:,} ({stats['failed_files']/stats['total_files']*100:.1f}%)

### å¹´åº¦åˆ†å¸ƒ
"""
    
    # å¹´åº¦åˆ†å¸ƒ
    for year, count in sorted(stats['year_distribution'].items()):
        percentage = count / stats['total_files'] * 100
        report += f"- {year}å¹´: {count:,} ä»¶ ({percentage:.1f}%)\n"
    
    report += "\n### æ¡ˆä»¶é¡å‹åˆ†å¸ƒ\n"
    # æ¡ˆä»¶é¡å‹åˆ†å¸ƒ (å‰10å)
    for case_type, count in stats['case_types'].most_common(10):
        percentage = count / stats['total_files'] * 100
        report += f"- {case_type}: {count:,} ä»¶ ({percentage:.1f}%)\n"
    
    if len(stats['case_types']) > 10:
        report += f"- å…¶ä»– {len(stats['case_types']) - 10} ç¨®é¡å‹...\n"
    
    report += "\n### æ³•é™¢åˆ†å¸ƒ\n"
    for court, count in sorted(stats['court_types'].items()):
        percentage = count / stats['total_files'] * 100
        report += f"- {court}: {count:,} ä»¶ ({percentage:.1f}%)\n"
    
    # ç« ç¯€çµ±è¨ˆ
    report += "\n## ğŸ“„ ç« ç¯€çµæ§‹åˆ†æ\n"
    
    section_names = ['header', 'main_text', 'facts', 'reasons', 'facts_and_reasons', 'footer']
    for section in section_names:
        if section in stats['section_stats']:
            lines = stats['section_stats'][section]
            present_count = stats['section_presence'][section]
            empty_count = stats['empty_sections'][section]
            
            if lines:
                avg_lines = sum(lines) / len(lines)
                max_lines = max(lines)
                min_lines = min(lines)
                median_lines = sorted(lines)[len(lines)//2]
                
                # ç‚ºåˆä½µæ ¼å¼æ·»åŠ ç‰¹æ®Šèªªæ˜
                section_display = section.upper()
                if section == 'facts_and_reasons':
                    section_display = "FACTS_AND_REASONS (åˆä½µæ ¼å¼)"
                
                report += f"\n### {section_display} ç« ç¯€\n"
                report += f"- å­˜åœ¨ç‡: {present_count:,}/{stats['successful_files']:,} ({present_count/stats['successful_files']*100:.1f}%)\n"
                report += f"- ç©ºç« ç¯€: {empty_count:,} ({empty_count/stats['successful_files']*100:.1f}%)\n"
                report += f"- å¹³å‡è¡Œæ•¸: {avg_lines:.1f}\n"
                report += f"- ä¸­ä½æ•¸è¡Œæ•¸: {median_lines}\n"
                report += f"- è¡Œæ•¸ç¯„åœ: {min_lines} - {max_lines}\n"
    
    # ç¬¦è™ŸåŒ–åˆ†é¡çµ±è¨ˆ - æ–°çš„æ¸…æ™°åˆ†é¡ç³»çµ±
    symbol_categories = stats.get('symbol_categories', {})
    total_processed = stats['successful_files']
    
    report += "\n## ğŸ”¤ ç¬¦è™ŸåŒ–æ–‡ä»¶åˆ†é¡çµ±è¨ˆ\n"
    report += "### ğŸ“Š ç¬¦è™Ÿèªªæ˜\n"
    report += "- **M**: ä¸»æ–‡ (Main text)\n"
    report += "- **F**: äº‹å¯¦ (Facts)\n" 
    report += "- **R**: ç†ç”± (Reasons)\n"
    report += "- **S**: äº‹å¯¦åŠç†ç”±åˆä½µç« ç¯€ (Simplified)\n"
    report += "- **D1**: åŒ…å«1å€‹æ—¥æœŸ\n"
    report += "- **D2**: åŒ…å«2å€‹æˆ–ä»¥ä¸Šæ—¥æœŸ\n"
    report += "- **N**: ç„¡æœ‰æ•ˆå…§å®¹\n"
    report += "- **ERROR**: è™•ç†å¤±æ•—\n\n"
    
    # ç¬¦è™Ÿåˆ†é¡çµ±è¨ˆ
    if symbol_categories:
        report += "### ğŸ“‹ æ–‡ä»¶åˆ†é¡çµæœ\n"
        
        # æŒ‰æ–‡ä»¶æ•¸é‡æ’åºé¡¯ç¤º
        sorted_symbols = sorted(symbol_categories.items(), key=lambda x: len(x[1]), reverse=True)
        
        for symbol, files in sorted_symbols:
            count = len(files)
            percentage = count / total_processed * 100 if total_processed > 0 else 0
            
            # è§£é‡‹ç¬¦è™Ÿå«ç¾©
            description = ""
            if symbol == "ERROR":
                description = "è™•ç†å¤±æ•—"
            elif symbol == "N":
                description = "ç„¡æœ‰æ•ˆå…§å®¹"
            else:
                parts = []
                if "M" in symbol:
                    parts.append("ä¸»æ–‡")
                if "S" in symbol:
                    parts.append("äº‹å¯¦åŠç†ç”±åˆä½µ")
                elif "F" in symbol:
                    parts.append("äº‹å¯¦") 
                if "R" in symbol and "S" not in symbol:
                    parts.append("ç†ç”±")
                if "D2" in symbol:
                    parts.append("å¤šå€‹æ—¥æœŸ")
                elif "D1" in symbol:
                    parts.append("å–®å€‹æ—¥æœŸ")
                description = "+".join(parts) if parts else "å…¶ä»–"
            
            report += f"- **{symbol}**: {count:,} ä»½ ({percentage:.1f}%) - {description}\n"
            
            # é¡¯ç¤ºå‰3å€‹æ–‡ä»¶ä¾‹å­
            if count > 0 and symbol != "ERROR":
                report += f"  ä¾‹å­: "
                examples = [f['filename'] for f in files[:3]]
                report += ", ".join(examples)
                if count > 3:
                    report += f" ... (é‚„æœ‰{count-3}å€‹)"
                report += "\n"
    
    # åˆä½µæ ¼å¼vsåˆ†é›¢æ ¼å¼çµ±è¨ˆ - ä¿æŒå‘å¾Œå…¼å®¹
    report += "\n## ğŸ”„ æ–‡ä»¶åˆ†é¡çµ±è¨ˆ (ç²¾ç¢ºè¨ˆç®—)\n"
    
    report += "\n## ğŸ”„ å‚³çµ±åˆ†é¡çµ±è¨ˆ (åŸºæ–¼ç¬¦è™Ÿåˆ†æ)\n"
    
    # åŸºæ–¼ç¬¦è™Ÿé‡æ–°è¨ˆç®—å‚³çµ±åˆ†é¡
    combined_format = []    # åŒ…å«FRçš„æ–‡ä»¶
    separated_format = []   # åŒ…å«MFRæˆ–MFä½†ä¸åŒ…å«FRçš„æ–‡ä»¶  
    procedural_only = []    # åªåŒ…å«Rçš„æ–‡ä»¶
    main_text_only = []     # åªåŒ…å«Mçš„æ–‡ä»¶
    facts_only = []         # åªåŒ…å«Fçš„æ–‡ä»¶
    incomplete = []         # Næˆ–å…¶ä»–ç•°å¸¸çµ„åˆ
    
    for symbol, files in symbol_categories.items():
        if symbol == "ERROR":
            continue
        elif "S" in symbol:
            combined_format.extend(files)
        elif ("M" in symbol and "F" in symbol and "R" in symbol) or ("M" in symbol and "F" in symbol):
            separated_format.extend(files)
        elif symbol == "R" or (symbol.startswith("R") and "M" not in symbol and "F" not in symbol):
            procedural_only.extend(files)
        elif symbol == "M" or (symbol.startswith("M") and "F" not in symbol and "R" not in symbol):
            main_text_only.extend(files)
        elif symbol == "F" or (symbol.startswith("F") and "M" not in symbol and "R" not in symbol):
            facts_only.extend(files)
        else:
            incomplete.extend(files)
    
    report += f"### ğŸ“Š å‚³çµ±åˆ†é¡æ˜ å°„\n"
    report += f"- ğŸ”— **åˆä½µæ ¼å¼** (åŒ…å«S): {len(combined_format):,} ä»½ ({len(combined_format)/total_processed*100:.1f}%)\n"
    report += f"- âœ‚ï¸ **åˆ†é›¢æ ¼å¼** (MFR/MF): {len(separated_format):,} ä»½ ({len(separated_format)/total_processed*100:.1f}%)\n"
    report += f"- âš–ï¸ **ç¨‹åºæ€§æ¡ˆä»¶** (åƒ…R): {len(procedural_only):,} ä»½ ({len(procedural_only)/total_processed*100:.1f}%)\n"
    report += f"- ğŸ“„ **åƒ…æœ‰ä¸»æ–‡** (åƒ…M): {len(main_text_only):,} ä»½ ({len(main_text_only)/total_processed*100:.1f}%)\n"
    report += f"- ğŸ“ **åƒ…æœ‰äº‹å¯¦** (åƒ…F): {len(facts_only):,} ä»½ ({len(facts_only)/total_processed*100:.1f}%)\n"
    report += f"- â“ **å…¶ä»–æ ¼å¼** (Nç­‰): {len(incomplete):,} ä»½ ({len(incomplete)/total_processed*100:.1f}%)\n"
    
    
    # è¨ˆç®—ç¸½åˆ†é¡æ•¸é€²è¡Œé©—è­‰
    total_processed = stats['successful_files']
    categorized_total = (
        len(combined_format) + 
        len(separated_format) + 
        len(procedural_only) + 
        len(main_text_only) + 
        len(facts_only) + 
        len(incomplete)
    )
    
    report += f"\n### âœ… åˆ†é¡é©—è­‰\n"
    report += f"- æˆåŠŸè™•ç†çš„æ–‡ä»¶: {total_processed:,}\n"
    report += f"- å·²åˆ†é¡çš„æ–‡ä»¶: {categorized_total:,}\n"
    report += f"- åˆ†é¡å®Œæ•´æ€§: {'âœ… å®Œæ•´' if categorized_total == total_processed else 'âŒ æœ‰éºæ¼'}\n"
    
    # è©³ç´°çš„ç« ç¯€å­˜åœ¨çµ±è¨ˆ
    report += f"\n### ğŸ“‹ ç« ç¯€å­˜åœ¨çµ±è¨ˆ\n"
    detailed = stats['detailed_stats']
    report += f"- æœ‰æ¨™é¡Œç« ç¯€: {detailed['has_header']:,} ä»½ ({detailed['has_header']/total_processed*100:.1f}%)\n"
    report += f"- æœ‰ä¸»æ–‡ç« ç¯€: {detailed['has_main_text']:,} ä»½ ({detailed['has_main_text']/total_processed*100:.1f}%)\n"
    report += f"- æœ‰äº‹å¯¦ç« ç¯€: {detailed['has_facts']:,} ä»½ ({detailed['has_facts']/total_processed*100:.1f}%)\n"
    report += f"- æœ‰ç†ç”±ç« ç¯€: {detailed['has_reasons']:,} ä»½ ({detailed['has_reasons']/total_processed*100:.1f}%)\n"
    report += f"- æœ‰åˆä½µç« ç¯€: {detailed['has_facts_and_reasons']:,} ä»½ ({detailed['has_facts_and_reasons']/total_processed*100:.1f}%)\n"
    report += f"- æœ‰çµå°¾ç« ç¯€: {detailed['has_footer']:,} ä»½ ({detailed['has_footer']/total_processed*100:.1f}%)\n"
    
    # å„ç¬¦è™Ÿé¡åˆ¥çš„è©³ç´°ä¿¡æ¯
    if symbol_categories:
        # æ‰¾å‡ºæœ€å¸¸è¦‹çš„ç¬¦è™Ÿçµ„åˆ
        sorted_symbols = sorted(symbol_categories.items(), key=lambda x: len(x[1]), reverse=True)
        
        # é¡¯ç¤ºå‰3å€‹æœ€å¸¸è¦‹ç¬¦è™Ÿçš„è©³ç´°ä¿¡æ¯
        for symbol, files in sorted_symbols[:3]:
            if symbol == "ERROR" or not files:
                continue
                
            report += f"\n### ğŸ“Š '{symbol}' é¡åˆ¥è©³æƒ…\n"
            report += f"- æ–‡ä»¶æ•¸é‡: {len(files)} ä»½\n"
            
            # è¨ˆç®—å¹³å‡è¡Œæ•¸
            if files and 'main_text_lines' in files[0]:
                avg_main = sum(f.get('main_text_lines', 0) for f in files) / len(files)
                avg_facts = sum(f.get('facts_lines', 0) for f in files) / len(files)
                avg_reasons = sum(f.get('reasons_lines', 0) for f in files) / len(files)
                avg_combined = sum(f.get('combined_lines', 0) for f in files) / len(files)
                
                report += f"- å¹³å‡ä¸»æ–‡è¡Œæ•¸: {avg_main:.1f}\n"
                report += f"- å¹³å‡äº‹å¯¦è¡Œæ•¸: {avg_facts:.1f}\n"
                report += f"- å¹³å‡ç†ç”±è¡Œæ•¸: {avg_reasons:.1f}\n"
                if avg_combined > 0:
                    report += f"- å¹³å‡åˆä½µè¡Œæ•¸: {avg_combined:.1f}\n"
            
            # é¡¯ç¤ºä¾‹å­æª”æ¡ˆ
            report += f"- ä¾‹å­æª”æ¡ˆ:\n"
            for i, file_info in enumerate(files[:3], 1):
                report += f"  {i}. {file_info['filename']}\n"

    # æ—¥æœŸæå–çµ±è¨ˆ
    report += "\n## ğŸ“… æ—¥æœŸæå–åˆ†æ\n"
    date_stats = stats['date_extraction_stats']
    report += f"- åŒ…å«æ—¥æœŸçš„æª”æ¡ˆ: {date_stats['files_with_dates']:,}/{stats['successful_files']:,} ({date_stats['files_with_dates']/stats['successful_files']*100:.1f}%)\n"
    report += f"- ç¸½æå–æ—¥æœŸæ•¸: {date_stats['total_dates_found']:,}\n"
    
    if date_stats['date_patterns']:
        report += "\n### æ—¥æœŸåˆ†å¸ƒ (å‰10å)\n"
        for date_pattern, count in date_stats['date_patterns'].most_common(10):
            report += f"- {date_pattern}: {count:,} æ¬¡\n"
    
    # æª”æ¡ˆå¤§å°çµ±è¨ˆ
    if stats['file_sizes']:
        report += "\n## ğŸ’¾ æª”æ¡ˆå¤§å°åˆ†æ\n"
        sizes = stats['file_sizes']
        avg_size = sum(sizes) / len(sizes)
        max_size = max(sizes)
        min_size = min(sizes)
        median_size = sorted(sizes)[len(sizes)//2]
        
        report += f"- å¹³å‡å¤§å°: {avg_size/1024:.1f} KB\n"
        report += f"- ä¸­ä½æ•¸å¤§å°: {median_size/1024:.1f} KB\n"
        report += f"- å¤§å°ç¯„åœ: {min_size/1024:.1f} KB - {max_size/1024:.1f} KB\n"
    
    # éŒ¯èª¤çµ±è¨ˆ
    if stats['processing_errors']:
        report += f"\n## âš ï¸ è™•ç†éŒ¯èª¤ ({len(stats['processing_errors'])} ä»¶)\n"
        error_types = Counter()
        for error in stats['processing_errors']:
            error_type = error['error'].split(':')[0]
            error_types[error_type] += 1
        
        for error_type, count in error_types.most_common():
            report += f"- {error_type}: {count} ä»¶\n"
        
        if len(stats['processing_errors']) <= 10:
            report += "\n### è©³ç´°éŒ¯èª¤åˆ—è¡¨\n"
            for error in stats['processing_errors']:
                report += f"- {error['file']}: {error['error']}\n"
    
    # æ¨¡å¼è­˜åˆ¥çµ±è¨ˆ
    report += "\n## ğŸ” æ¨¡å¼è­˜åˆ¥çµæœ\n"
    patterns = find_section_patterns()
    for name, pattern in patterns.items():
        if hasattr(pattern, 'pattern'):
            report += f"- {name}: `{pattern.pattern}`\n"
        else:
            report += f"- {name}: `{pattern}`\n"
    
    return report

def generate_detailed_category_report(stats):
    """ç”Ÿæˆè©³ç´°çš„ç¬¦è™Ÿåˆ†é¡å ±å‘Š"""
    
    symbol_categories = stats.get('symbol_categories', {})
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path("output/analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç‚ºæ¯å€‹ç¬¦è™Ÿé¡åˆ¥ç”Ÿæˆè©³ç´°å ±å‘Š
    for symbol, files in symbol_categories.items():
        if not files:
            continue
            
        report_filename = output_dir / f"symbol_{symbol}_{timestamp}.txt"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"# ç¬¦è™Ÿ '{symbol}' é¡åˆ¥è©³ç´°å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ–‡ä»¶æ•¸é‡: {len(files)}\n\n")
            
            # ç¬¦è™Ÿå«ç¾©èªªæ˜
            f.write("## ç¬¦è™Ÿå«ç¾©\n")
            meanings = []
            if "M" in symbol:
                meanings.append("M = ä¸»æ–‡ (Main text)")
            if "F" in symbol:
                meanings.append("F = äº‹å¯¦ (Facts)")
            if "R" in symbol:
                meanings.append("R = ç†ç”± (Reasons)")
            if "S" in symbol:
                meanings.append("S = äº‹å¯¦åŠç†ç”±åˆä½µ (Simplified)")
            if "D1" in symbol:
                meanings.append("D1 = åŒ…å«1å€‹æ—¥æœŸ")
            if "D2" in symbol:
                meanings.append("D2 = åŒ…å«2å€‹æˆ–ä»¥ä¸Šæ—¥æœŸ")
            if symbol == "N":
                meanings.append("N = ç„¡æœ‰æ•ˆå…§å®¹")
            if symbol == "ERROR":
                meanings.append("ERROR = è™•ç†å¤±æ•—")
            
            for meaning in meanings:
                f.write(f"- {meaning}\n")
            f.write("\n")
            
            # æ–‡ä»¶åˆ—è¡¨
            f.write("## æ–‡ä»¶åˆ—è¡¨\n")
            for i, file_info in enumerate(files, 1):
                f.write(f"{i:4d}. {file_info['filename']}\n")
                
                # é¡¯ç¤ºç« ç¯€çµ±è¨ˆ
                if 'main_text_lines' in file_info:
                    f.write(f"      ä¸»æ–‡: {file_info['main_text_lines']} è¡Œ\n")
                if 'facts_lines' in file_info:
                    f.write(f"      äº‹å¯¦: {file_info['facts_lines']} è¡Œ\n")
                if 'reasons_lines' in file_info:
                    f.write(f"      ç†ç”±: {file_info['reasons_lines']} è¡Œ\n")
                if 'combined_lines' in file_info:
                    f.write(f"      åˆä½µ: {file_info['combined_lines']} è¡Œ\n")
                if 'dates_count' in file_info:
                    f.write(f"      æ—¥æœŸ: {file_info['dates_count']} å€‹\n")
                if 'header_lines' in file_info:
                    f.write(f"      æ¨™é¡Œ: {file_info['header_lines']} è¡Œ\n")
                if 'footer_lines' in file_info:
                    f.write(f"      çµå°¾: {file_info['footer_lines']} è¡Œ\n")
                
                # å¦‚æœæ˜¯éŒ¯èª¤æ–‡ä»¶ï¼Œé¡¯ç¤ºéŒ¯èª¤ä¿¡æ¯
                if 'error' in file_info:
                    f.write(f"      éŒ¯èª¤: {file_info['error']}\n")
                
                f.write("\n")
        
        print(f"ğŸ“„ ç¬¦è™Ÿ '{symbol}' è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {report_filename}")
    
    # ç”Ÿæˆç¬¦è™Ÿçµ±è¨ˆæ‘˜è¦
    summary_filename = f"symbol_summary_{timestamp}.txt"
    with open(summary_filename, 'w', encoding='utf-8') as f:
        f.write("# ç¬¦è™Ÿåˆ†é¡çµ±è¨ˆæ‘˜è¦\n")
        f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## ç¬¦è™Ÿèªªæ˜\n")
        f.write("- M: ä¸»æ–‡ (Main text)\n")
        f.write("- F: äº‹å¯¦ (Facts)\n")
        f.write("- R: ç†ç”± (Reasons)\n")
        f.write("- S: äº‹å¯¦åŠç†ç”±åˆä½µç« ç¯€ (Simplified)\n")
        f.write("- D1: åŒ…å«1å€‹æ—¥æœŸ\n")
        f.write("- D2: åŒ…å«2å€‹æˆ–ä»¥ä¸Šæ—¥æœŸ\n")
        f.write("- N: ç„¡æœ‰æ•ˆå…§å®¹\n")
        f.write("- ERROR: è™•ç†å¤±æ•—\n\n")
        
        f.write("## åˆ†é¡çµ±è¨ˆ\n")
        sorted_symbols = sorted(symbol_categories.items(), key=lambda x: len(x[1]), reverse=True)
        total_files = sum(len(files) for files in symbol_categories.values())
        
        for symbol, files in sorted_symbols:
            count = len(files)
            percentage = count / total_files * 100 if total_files > 0 else 0
            f.write(f"{symbol:>8}: {count:5d} ä»½ ({percentage:5.1f}%)\n")
        
        f.write(f"\nç¸½è¨ˆ: {total_files:5d} ä»½\n")
    
    print(f"ğŸ“Š ç¬¦è™Ÿçµ±è¨ˆæ‘˜è¦å·²ä¿å­˜è‡³: {summary_filename}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ L-chunk ç¶œåˆåˆ†æé–‹å§‹")
    print("=" * 60)
    
    # åŸ·è¡Œåˆ†æ
    stats = analyze_filtered_dataset()
    
    if stats:
        # ç”Ÿæˆå ±å‘Š
        report = generate_comprehensive_report(stats)
        
        # ä¿å­˜å ±å‘Š
        output_dir = Path("output/analysis")
        output_dir.mkdir(parents=True, exist_ok=True)
        report_filename = output_dir / f"comprehensive_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼å ±å‘Šå·²ä¿å­˜è‡³: {report_filename}")
        
        # ä¿å­˜ JSON æ ¼å¼çš„åŸå§‹æ•¸æ“š
        json_filename = output_dir / f"analysis_raw_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # è½‰æ›ä¸èƒ½åºåˆ—åŒ–çš„æ•¸æ“š
        json_stats = dict(stats)
        json_stats['section_stats'] = {k: v for k, v in stats['section_stats'].items()}
        json_stats['case_types'] = dict(stats['case_types'])
        json_stats['year_distribution'] = dict(stats['year_distribution'])
        json_stats['court_types'] = dict(stats['court_types'])
        json_stats['section_presence'] = dict(stats['section_presence'])
        json_stats['empty_sections'] = dict(stats['empty_sections'])
        json_stats['date_extraction_stats']['date_patterns'] = dict(stats['date_extraction_stats']['date_patterns'])
        
        # æ–°å¢ï¼šç¬¦è™Ÿåˆ†é¡æ•¸æ“šå’Œæ–‡ä»¶åˆ†é¡æ•¸æ“š
        json_stats['symbol_categories'] = stats.get('symbol_categories', {})
        json_stats['file_categories'] = stats.get('file_categories', {})
        json_stats['detailed_stats'] = stats['detailed_stats']
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(json_stats, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š åŸå§‹æ•¸æ“šå·²ä¿å­˜è‡³: {json_filename}")
        
        # ç”Ÿæˆè©³ç´°åˆ†é¡å ±å‘Š
        generate_detailed_category_report(stats)
        
        # ç°¡è¦æ‘˜è¦
        print(f"\nğŸ“‹ è™•ç†æ‘˜è¦:")
        print(f"   ç¸½æª”æ¡ˆ: {stats['total_files']:,}")
        print(f"   æˆåŠŸç‡: {stats['successful_files']/stats['total_files']*100:.1f}%")
        print(f"   è™•ç†æ™‚é–“: {stats['processing_time']:.1f} ç§’")
        print(f"   å¹³å‡é€Ÿåº¦: {stats['total_files']/stats['processing_time']:.1f} æª”æ¡ˆ/ç§’")
    
    else:
        print("âŒ åˆ†æå¤±æ•—")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
æ¨¡å‹æ¯”è¼ƒè©•ä¼°å·¥å…·
åŸºæ–¼  "å¥½å“å‘³" åŸå‰‡ï¼šç°¡å–®è€Œæœ‰æ•ˆçš„æ¯”è¼ƒ

æ¯”è¼ƒæ¨¡å‹ï¼š
1. é‚è¼¯å›æ­¸ (Logistic Regression)
2. æ”¯æŒå‘é‡æ©Ÿ (SVM)  
3. éš¨æ©Ÿæ£®æ— (Random Forest)
4. BERT åˆ†é¡å™¨

ç›®æ¨™ï¼šæ‰¾åˆ°æœ€ä½³çš„å±¤ç´šç¬¦è™Ÿæª¢æ¸¬æ¨¡å‹
"""

import pandas as pd
import numpy as np
import json
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# å‚³çµ±æ©Ÿå™¨å­¸ç¿’
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score, 
    recall_score, f1_score, confusion_matrix, roc_auc_score
)

# BERTç›¸é—œ
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


class ModelComparisonEvaluator:
    """æ¨¡å‹æ¯”è¼ƒè©•ä¼°å™¨"""
    
    def __init__(self, output_dir: str = "output/model_comparison"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹å­˜å„²
        self.models = {}
        self.vectorizer = None
        self.results = {}
        
        # BERT ç›¸é—œ
        self.bert_model = None
        self.bert_tokenizer = None
        
    def load_data(self, data_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """è¼‰å…¥è¨“ç·´æ•¸æ“š"""
        print("ğŸ“Š è¼‰å…¥æ•¸æ“š...")
        df = pd.read_csv(data_path)
        
        X = df['line_text'].values
        y = (df['sentiment'] == 'Positive').astype(int).values
        
        print(f"âœ… è¼‰å…¥ {len(df)} æ¢æ•¸æ“š")
        print(f"   æ­£æ¨£æœ¬: {np.sum(y)} ({np.sum(y)/len(y)*100:.1f}%)")
        print(f"   è² æ¨£æœ¬: {len(y)-np.sum(y)} ({(len(y)-np.sum(y))/len(y)*100:.1f}%)")
        
        return X, y
    
    def prepare_traditional_features(self, X_train: np.ndarray, X_test: np.ndarray):
        """ç‚ºå‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹æº–å‚™ç‰¹å¾µ"""
        print("ğŸ”§ æº–å‚™ TF-IDF ç‰¹å¾µ...")
        
        # TF-IDF å‘é‡åŒ–
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )
        
        X_train_tfidf = self.vectorizer.fit_transform(X_train)
        X_test_tfidf = self.vectorizer.transform(X_test)
        
        print(f"âœ… TF-IDF ç‰¹å¾µç¶­åº¦: {X_train_tfidf.shape[1]}")
        
        return X_train_tfidf, X_test_tfidf
    
    def train_traditional_models(self, X_train_tfidf, y_train, X_test_tfidf, y_test):
        """è¨“ç·´å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹"""
        print("\nğŸ¤– è¨“ç·´å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹...")
        
        # æ¨¡å‹é…ç½®
        model_configs = {
            'Logistic Regression': LogisticRegression(
                random_state=42,
                max_iter=1000,
                class_weight='balanced'
            ),
            'SVM': SVC(
                random_state=42,
                probability=True,
                class_weight='balanced',
                kernel='rbf'
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                class_weight='balanced',
                max_depth=10
            )
        }
        
        # è¨“ç·´å’Œè©•ä¼°æ¯å€‹æ¨¡å‹
        for name, model in model_configs.items():
            print(f"\nğŸ”¥ è¨“ç·´ {name}...")
            
            # è¨“ç·´
            model.fit(X_train_tfidf, y_train)
            
            # é æ¸¬
            y_pred = model.predict(X_test_tfidf)
            y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]
            
            # è©•ä¼°
            metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)
            
            # ä¿å­˜æ¨¡å‹å’Œçµæœ
            self.models[name] = model
            self.results[name] = metrics
            
            print(f"âœ… {name} è¨“ç·´å®Œæˆ")
            print(f"   æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
            print(f"   å¬å›ç‡: {metrics['recall']:.4f}")
            print(f"   F1åˆ†æ•¸: {metrics['f1_score']:.4f}")
            print(f"   AUC: {metrics['auc']:.4f}")
    
    def evaluate_bert_model(self, X_test: np.ndarray, y_test: np.ndarray):
        """è©•ä¼°ç¾æœ‰çš„ BERT æ¨¡å‹"""
        print("\nğŸ¤– è©•ä¼° BERT æ¨¡å‹...")
        
        bert_model_path = "models/training/best_model"
        if not Path(bert_model_path).exists():
            print(f"âŒ æ‰¾ä¸åˆ° BERT æ¨¡å‹: {bert_model_path}")
            print("è«‹å…ˆè¨“ç·´ BERT æ¨¡å‹")
            return
        
        # è¼‰å…¥ BERT æ¨¡å‹
        print("ğŸ“¥ è¼‰å…¥ BERT æ¨¡å‹...")
        self.bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_path)
        self.bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_path)
        self.bert_model.eval()
        
        # æ‰¹é‡é æ¸¬
        y_pred_list = []
        y_pred_proba_list = []
        
        batch_size = 16
        for i in range(0, len(X_test), batch_size):
            batch_texts = X_test[i:i+batch_size]
            
            # Tokenization
            inputs = self.bert_tokenizer(
                batch_texts.tolist(),
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
            
            # é æ¸¬
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
                logits = outputs.logits
                probs = F.softmax(logits, dim=1)
                
                # ç²å–é æ¸¬çµæœ
                batch_pred = torch.argmax(logits, dim=1).numpy()
                batch_proba = probs[:, 1].numpy()
                
                y_pred_list.extend(batch_pred)
                y_pred_proba_list.extend(batch_proba)
        
        y_pred = np.array(y_pred_list)
        y_pred_proba = np.array(y_pred_proba_list)
        
        # è©•ä¼°
        metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)
        self.results['BERT'] = metrics
        
        print(f"âœ… BERT è©•ä¼°å®Œæˆ")
        print(f"   æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
        print(f"   å¬å›ç‡: {metrics['recall']:.4f}")
        print(f"   F1åˆ†æ•¸: {metrics['f1_score']:.4f}")
        print(f"   AUC: {metrics['auc']:.4f}")
    
    def _calculate_metrics(self, y_true, y_pred, y_pred_proba) -> Dict:
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        return {
            'accuracy': float(accuracy_score(y_true, y_pred)),
            'precision': float(precision_score(y_true, y_pred)),
            'recall': float(recall_score(y_true, y_pred)),
            'f1_score': float(f1_score(y_true, y_pred)),
            'auc': float(roc_auc_score(y_true, y_pred_proba)),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
        }
    
    def generate_comparison_report(self):
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ¯”è¼ƒå ±å‘Š...")
        
        # å‰µå»ºçµæœ DataFrame
        metrics_df = pd.DataFrame(self.results).T
        metrics_df = metrics_df.drop('confusion_matrix', axis=1)
        
        # æ’åºï¼ˆæŒ‰ F1 åˆ†æ•¸ï¼‰
        metrics_df = metrics_df.sort_values('f1_score', ascending=False)
        
        print("\nğŸ“‹ æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ:")
        print("="*80)
        print(f"{'æ¨¡å‹':<20} {'æº–ç¢ºç‡':<10} {'ç²¾ç¢ºç‡':<10} {'å¬å›ç‡':<10} {'F1åˆ†æ•¸':<10} {'AUC':<10}")
        print("="*80)
        
        for model_name, row in metrics_df.iterrows():
            print(f"{model_name:<20} {row['accuracy']:<10.4f} {row['precision']:<10.4f} "
                  f"{row['recall']:<10.4f} {row['f1_score']:<10.4f} {row['auc']:<10.4f}")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = metrics_df.index[0]
        best_f1 = metrics_df.loc[best_model, 'f1_score']
        
        print("="*80)
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model} (F1åˆ†æ•¸: {best_f1:.4f})")
        
        return metrics_df, best_model
    
    def create_visualizations(self, metrics_df: pd.DataFrame):
        """å‰µå»ºå¯è¦–åŒ–åœ–è¡¨"""
        print("\nğŸ“ˆ å‰µå»ºå¯è¦–åŒ–åœ–è¡¨...")
        
        # è¨­ç½®åœ–è¡¨æ¨£å¼
        plt.style.use('default')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ', fontsize=16, fontweight='bold')
        
        # 1. æ¢å½¢åœ– - å„é …æŒ‡æ¨™æ¯”è¼ƒ
        ax1 = axes[0, 0]
        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
        x = np.arange(len(metrics_df))
        width = 0.15
        
        for i, metric in enumerate(metrics_to_plot):
            ax1.bar(x + i*width, metrics_df[metric], width, label=metric.upper())
        
        ax1.set_xlabel('æ¨¡å‹')
        ax1.set_ylabel('åˆ†æ•¸')
        ax1.set_title('å„é …è©•ä¼°æŒ‡æ¨™æ¯”è¼ƒ')
        ax1.set_xticks(x + width * 2)
        ax1.set_xticklabels(metrics_df.index, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. F1åˆ†æ•¸æ’åº
        ax2 = axes[0, 1]
        colors = ['#2E8B57', '#4682B4', '#DAA520', '#DC143C'][:len(metrics_df)]
        bars = ax2.bar(metrics_df.index, metrics_df['f1_score'], color=colors)
        ax2.set_title('F1åˆ†æ•¸æ¯”è¼ƒ')
        ax2.set_ylabel('F1åˆ†æ•¸')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for bar, value in zip(bars, metrics_df['f1_score']):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,
                    f'{value:.3f}', ha='center', va='bottom')
        
        # 3. ç²¾ç¢ºç‡ vs å¬å›ç‡æ•£é»åœ–
        ax3 = axes[1, 0]
        scatter = ax3.scatter(metrics_df['precision'], metrics_df['recall'], 
                             c=metrics_df['f1_score'], cmap='viridis', s=100, alpha=0.8)
        
        for i, model in enumerate(metrics_df.index):
            ax3.annotate(model, (metrics_df.loc[model, 'precision'], 
                               metrics_df.loc[model, 'recall']),
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax3.set_xlabel('ç²¾ç¢ºç‡')
        ax3.set_ylabel('å¬å›ç‡')
        ax3.set_title('ç²¾ç¢ºç‡ vs å¬å›ç‡')
        ax3.grid(True, alpha=0.3)
        plt.colorbar(scatter, ax=ax3, label='F1åˆ†æ•¸')
        
        # 4. é›·é”åœ–
        ax4 = axes[1, 1]
        ax4.remove()  # ç§»é™¤åŸè»¸
        ax4 = fig.add_subplot(2, 2, 4, projection='polar')
        
        # é¸æ“‡æœ€ä½³çš„å…©å€‹æ¨¡å‹é€²è¡Œé›·é”åœ–æ¯”è¼ƒ
        top_2_models = metrics_df.head(2)
        metrics_radar = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
        
        angles = np.linspace(0, 2*np.pi, len(metrics_radar), endpoint=False).tolist()
        angles += angles[:1]  # é–‰åˆ
        
        for i, (model_name, row) in enumerate(top_2_models.iterrows()):
            values = [row[metric] for metric in metrics_radar]
            values += values[:1]  # é–‰åˆ
            
            ax4.plot(angles, values, 'o-', linewidth=2, label=model_name)
            ax4.fill(angles, values, alpha=0.25)
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels([metric.upper() for metric in metrics_radar])
        ax4.set_title('å‰å…©åæ¨¡å‹é›·é”åœ–æ¯”è¼ƒ')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        chart_path = self.output_dir / 'model_comparison_charts.png'
        plt.savefig(chart_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š åœ–è¡¨å·²ä¿å­˜: {chart_path}")
        
        plt.show()
    
    def save_results(self, metrics_df: pd.DataFrame, best_model: str):
        """ä¿å­˜çµæœ"""
        print("\nğŸ’¾ ä¿å­˜è©•ä¼°çµæœ...")
        
        # ä¿å­˜è©³ç´°çµæœ
        results_file = self.output_dir / f'model_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ CSV å ±å‘Š
        csv_file = self.output_dir / f'model_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
        metrics_df.to_csv(csv_file, encoding='utf-8')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if best_model in self.models:
            best_model_file = self.output_dir / f'best_traditional_model_{best_model.replace(" ", "_").lower()}.pkl'
            with open(best_model_file, 'wb') as f:
                pickle.dump({
                    'model': self.models[best_model],
                    'vectorizer': self.vectorizer,
                    'model_name': best_model
                }, f)
            print(f"ğŸ† æœ€ä½³å‚³çµ±æ¨¡å‹å·²ä¿å­˜: {best_model_file}")
        
        # å‰µå»ºç¸½çµå ±å‘Š
        summary_file = self.output_dir / f'comparison_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        self._create_summary_report(summary_file, metrics_df, best_model)
        
        print(f"ğŸ“‹ è©³ç´°çµæœ: {results_file}")
        print(f"ğŸ“Š CSVå ±å‘Š: {csv_file}")
        print(f"ğŸ“ ç¸½çµå ±å‘Š: {summary_file}")
    
    def _create_summary_report(self, file_path: Path, metrics_df: pd.DataFrame, best_model: str):
        """å‰µå»ºç¸½çµå ±å‘Š"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# æ¨¡å‹æ¯”è¼ƒè©•ä¼°å ±å‘Š\n\n")
            f.write(f"**è©•ä¼°æ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## æ¨¡å‹æ€§èƒ½ç¸½è¦½\n\n")
            f.write("| æ¨¡å‹ | æº–ç¢ºç‡ | ç²¾ç¢ºç‡ | å¬å›ç‡ | F1åˆ†æ•¸ | AUC |\n")
            f.write("|------|--------|--------|--------|--------|----- |\n")
            
            for model_name, row in metrics_df.iterrows():
                f.write(f"| {model_name} | {row['accuracy']:.4f} | {row['precision']:.4f} | "
                       f"{row['recall']:.4f} | {row['f1_score']:.4f} | {row['auc']:.4f} |\n")
            
            f.write(f"\n## ğŸ† æœ€ä½³æ¨¡å‹: {best_model}\n\n")
            
            best_metrics = metrics_df.loc[best_model]
            f.write("### æœ€ä½³æ¨¡å‹æ€§èƒ½:\n")
            f.write(f"- **æº–ç¢ºç‡**: {best_metrics['accuracy']:.4f}\n")
            f.write(f"- **ç²¾ç¢ºç‡**: {best_metrics['precision']:.4f}\n")
            f.write(f"- **å¬å›ç‡**: {best_metrics['recall']:.4f}\n")
            f.write(f"- **F1åˆ†æ•¸**: {best_metrics['f1_score']:.4f}\n")
            f.write(f"- **AUC**: {best_metrics['auc']:.4f}\n\n")
            
            f.write("## æ¨¡å‹åˆ†æ\n\n")
            
            traditional_models = [name for name in metrics_df.index if name != 'BERT']
            if 'BERT' in metrics_df.index:
                bert_f1 = metrics_df.loc['BERT', 'f1_score']
                best_traditional_f1 = max([metrics_df.loc[name, 'f1_score'] for name in traditional_models])
            


def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•æ¨¡å‹æ¯”è¼ƒè©•ä¼°")
    print("="*60)
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶
    data_file = "data/training/project-1-at-2025-10-10-15-05-fea45fba.csv"
    if not Path(data_file).exists():
        print(f"âŒ æ‰¾ä¸åˆ°æ•¸æ“šæ–‡ä»¶: {data_file}")
        return
    
    # åˆå§‹åŒ–è©•ä¼°å™¨
    evaluator = ModelComparisonEvaluator()
    
    # è¼‰å…¥æ•¸æ“š
    X, y = evaluator.load_data(data_file)
    
    # åˆ†å‰²æ•¸æ“š
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æº–å‚™å‚³çµ±æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µ
    X_train_tfidf, X_test_tfidf = evaluator.prepare_traditional_features(X_train, X_test)
    
    # è¨“ç·´å‚³çµ±æ¨¡å‹
    evaluator.train_traditional_models(X_train_tfidf, y_train, X_test_tfidf, y_test)
    
    # è©•ä¼° BERT æ¨¡å‹
    evaluator.evaluate_bert_model(X_test, y_test)
    
    # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
    metrics_df, best_model = evaluator.generate_comparison_report()
    
    # å‰µå»ºå¯è¦–åŒ–
    evaluator.create_visualizations(metrics_df)
    
    # ä¿å­˜çµæœ
    evaluator.save_results(metrics_df, best_model)
    
    print(f"\nğŸ‰ è©•ä¼°å®Œæˆ!")
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model}")
    print(f"ğŸ“ çµæœä¿å­˜åœ¨: {evaluator.output_dir}")


if __name__ == "__main__":
    main()
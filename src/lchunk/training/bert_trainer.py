#!/usr/bin/env python3
"""
BERT å±¤ç´šç¬¦è™Ÿåˆ†é¡å™¨è¨“ç·´å™¨ & å‚³çµ±æ¨¡å‹æ¯”è¼ƒå™¨
åŸºæ–¼  "å–®ä¸€è·è²¬" åŸå‰‡ï¼šå°ˆæ³¨è¨“ç·´å’Œæ¯”è¼ƒ

å°ˆæ³¨æ–¼ï¼š
1. åŠ è¼‰æ¨™è¨»æ•¸æ“š
2. è¨“ç·´ BERT æ¨¡å‹
3. æ¯”è¼ƒå‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ (Logistic Regression, SVM, Random Forest)
4. ä¿å­˜æ¨¡å‹å’Œè©•ä¼°çµæœ
"""

import pandas as pd
import numpy as np
import torch
import json
from pathlib import Path
from datetime import datetime
from typing import Dict
import warnings
warnings.filterwarnings('ignore')

# BERTç›¸é—œ
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import torch.nn.functional as F

class BERTLevelSymbolTrainer:
    """BERT å±¤ç´šç¬¦è™Ÿåˆ†é¡å™¨è¨“ç·´å™¨"""
    
    def __init__(self, output_dir: str = "models/training"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¨¡å‹ç›¸é—œ
        self.bert_model = None
        self.bert_tokenizer = None
        self.trainer = None
        
    def train_classifier(self, training_data_path: str) -> Dict:
        """è¨“ç·´ BERT åˆ†é¡å™¨"""
        print("ğŸ¤– è¨“ç·´ BERT å±¤ç´šç¬¦è™Ÿåˆ†é¡å™¨...")
        
        # è¼‰å…¥è¨“ç·´æ•¸æ“š
        df = pd.read_csv(training_data_path)
        print(f"âœ… è¼‰å…¥ {len(df)} æ¢è¨“ç·´æ•¸æ“š")
        
        # æ•¸æ“šçµ±è¨ˆ
        positive_count = (df['sentiment'] == 'Positive').sum()
        negative_count = (df['sentiment'] == 'Negative').sum()
        print(f"ğŸ“Š æ­£æ¨£æœ¬: {positive_count} ({positive_count/len(df)*100:.1f}%)")
        print(f"ğŸ“Š è² æ¨£æœ¬: {negative_count} ({negative_count/len(df)*100:.1f}%)")
        
        # æº–å‚™æ•¸æ“š
        X = df['line_text'].values
        y = (df['sentiment'] == 'Positive').astype(int).values
        
        # åˆ†å‰²æ•¸æ“š
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"ğŸ“Š è¨“ç·´é›†: {len(X_train)} æ¨£æœ¬")
        print(f"ğŸ“Š æ¸¬è©¦é›†: {len(X_test)} æ¨£æœ¬")
        
        # åˆå§‹åŒ– BERT æ¨¡å‹
        model_name = "bert-base-chinese"
        print(f"ğŸ”§ åˆå§‹åŒ– {model_name} æ¨¡å‹...")
        
        self.bert_tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert_model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=2
        )
        
        # æº–å‚™æ•¸æ“šé›†
        def create_dataset(texts, labels):
            dataset = Dataset.from_dict({
                'text': texts,
                'labels': labels
            })
            return dataset.map(self._tokenize_function, batched=True)
        
        train_dataset = create_dataset(X_train, y_train)
        test_dataset = create_dataset(X_test, y_test)
        
        # è¨“ç·´åƒæ•¸ - åŸºæ–¼ text_classifier.py çš„å„ªåŒ–é…ç½®
        print("âš™ï¸ é…ç½®è¨“ç·´åƒæ•¸...")
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=2,  # é¿å…éæ“¬åˆ
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=self.output_dir / 'logs',
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            save_total_limit=1,
            logging_steps=50,
            report_to=[],  # ç¦ç”¨ wandb ç­‰å¤–éƒ¨æ—¥èªŒ
        )
        
        # æ•¸æ“šæ•´ç†å™¨
        data_collator = DataCollatorWithPadding(tokenizer=self.bert_tokenizer)
        
        # Trainer
        self.trainer = Trainer(
            model=self.bert_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            tokenizer=self.bert_tokenizer,
            data_collator=data_collator,
        )
        
        # è¨“ç·´
        print("ğŸ”¥ é–‹å§‹è¨“ç·´...")
        self.trainer.train()
        
        # è©•ä¼°
        print("ğŸ“Š è©•ä¼°æ¨¡å‹...")
        predictions = self.trainer.predict(test_dataset)
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_pred_proba = F.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()
        
        # è©•ä¼°æŒ‡æ¨™
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        print(f"âœ… BERT è¨“ç·´å®Œæˆ!")
        print(f"   æº–ç¢ºç‡: {accuracy:.4f}")
        print(f"   ç²¾ç¢ºç‡: {precision:.4f}")
        print(f"   å¬å›ç‡: {recall:.4f}")
        print(f"   F1åˆ†æ•¸: {f1:.4f}")
        
        # ä¿å­˜æ¨¡å‹
        best_model_path = self.output_dir / 'best_model'
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {best_model_path}")
        
        self.trainer.save_model(best_model_path)
        self.bert_tokenizer.save_pretrained(best_model_path)
        
        # ä¿å­˜è¨“ç·´ä¿¡æ¯
        training_info = {
            'model_name': model_name,
            'training_data': training_data_path,
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'trained_at': datetime.now().isoformat(),
            'model_path': str(best_model_path)
        }
        
        info_file = self.output_dir / 'training_info.json'
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(training_info, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è©³ç´°åˆ†é¡å ±å‘Š
        report = classification_report(y_test, y_pred, target_names=['æ™®é€šæ–‡æœ¬', 'å±¤ç´šç¬¦è™Ÿ'])
        report_file = self.output_dir / 'classification_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ è¨“ç·´ä¿¡æ¯å·²ä¿å­˜: {info_file}")
        print(f"ğŸ“‹ åˆ†é¡å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return training_info
    
    def _tokenize_function(self, examples):
        """BERT tokenization"""
        return self.bert_tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=512
        )

def main():
    """ä¸»å‡½æ•¸ - åªè² è²¬ BERT è¨“ç·´"""
    print("ï¿½ å•Ÿå‹• BERT å±¤ç´šç¬¦è™Ÿåˆ†é¡å™¨è¨“ç·´")
    print("åŸºæ–¼  'å–®ä¸€è·è²¬' åŸå‰‡ï¼šå°ˆæ³¨è¨“ç·´")
    print("="*60)
    
    # æª¢æŸ¥è¨“ç·´æ•¸æ“š
    training_data = "data/training/project-1-at-2025-10-10-15-05-fea45fba.csv"
    if not Path(training_data).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {training_data}")
        print("è«‹ç¢ºä¿æ¨™è¨»æ•¸æ“šæ–‡ä»¶å­˜åœ¨")
        return
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = BERTLevelSymbolTrainer()
    
    # åŸ·è¡Œ BERT è¨“ç·´
    training_info = trainer.train_classifier(training_data)
    
    print(f"\nğŸ‰ BERT è¨“ç·´å®Œæˆ!")
    print(f"ğŸ† æœ€ä½³æ€§èƒ½: æº–ç¢ºç‡ {training_info['accuracy']:.4f}, å¬å›ç‡ {training_info['recall']:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {training_info['model_path']}")
    
    # GPU è¨­å‚™ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® ä½¿ç”¨ GPU: {gpu_name}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU è¨“ç·´")
    
    def _tokenize_function(self, examples):
        """BERT tokenization"""
        return self.bert_tokenizer(
            examples['text'],
            truncation=True,
            padding=True,
            max_length=512
        )

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹• BERT å±¤ç´šç¬¦è™Ÿåˆ†é¡å™¨è¨“ç·´ & å‚³çµ±æ¨¡å‹æ¯”è¼ƒ")
    print("="*60)
    
    # æª¢æŸ¥è¨“ç·´æ•¸æ“š
    training_data = "data/training/project-1-at-2025-10-10-15-05-fea45fba.csv"
    if not Path(training_data).exists():
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {training_data}")
        print("è«‹ç¢ºä¿æ¨™è¨»æ•¸æ“šæ–‡ä»¶å­˜åœ¨")
        return
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = BERTLevelSymbolTrainer()
    
    # åŸ·è¡Œ BERT è¨“ç·´
    training_info = trainer.train_classifier(training_data)
    
    print(f"\nğŸ‰ BERT è¨“ç·´å®Œæˆ!")
    print(f"ğŸ† æœ€ä½³æ€§èƒ½: æº–ç¢ºç‡ {training_info['accuracy']:.4f}, å¬å›ç‡ {training_info['recall']:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {training_info['model_path']}")
    
    # æ¯”è¼ƒå‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹
    print("\n" + "="*60)
    print("ğŸ”¬ é–‹å§‹æ¯”è¼ƒå‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹")
    comparison_results = trainer.compare_traditional_models(training_data)
    
    # é¡¯ç¤ºæ¯”è¼ƒçµæœ
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¼ƒ:")
    print("-" * 60)
    print(f"{'æ¨¡å‹':<20} {'æº–ç¢ºç‡':<8} {'ç²¾ç¢ºç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•¸':<8}")
    print("-" * 60)
    
    for model_name, metrics in comparison_results.items():
        print(f"{model_name:<20} {metrics['accuracy']:<8.4f} {metrics['precision']:<8.4f} {metrics['recall']:<8.4f} {metrics['f1_score']:<8.4f}")
    
    # èˆ‡ BERT æ¯”è¼ƒ
    bert_metrics = {
        'accuracy': training_info['accuracy'],
        'precision': training_info['precision'],
        'recall': training_info['recall'],
        'f1_score': training_info['f1_score']
    }
    
    print("-" * 60)
    print(f"{'BERT':<20} {bert_metrics['accuracy']:<8.4f} {bert_metrics['precision']:<8.4f} {bert_metrics['recall']:<8.4f} {bert_metrics['f1_score']:<8.4f}")
    
    # GPU è¨­å‚™ä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® ä½¿ç”¨ GPU: {gpu_name}")
    else:
        print("ğŸ’» ä½¿ç”¨ CPU è¨“ç·´")

if __name__ == "__main__":
    main()
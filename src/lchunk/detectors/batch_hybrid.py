#!/usr/bin/env python3
"""
æ··åˆæ‰¹æ¬¡å±¤ç´šæª¢æ¸¬å™¨
åŸºæ–¼ Linus "æ¼¸é€²å¼éæ¿¾" åŸå‰‡çš„æ‰¹æ¬¡è™•ç†ç‰ˆæœ¬

çµåˆï¼š
1. è¦å‰‡æª¢æ¸¬ï¼šé›¶å®¹å¿æ ¼å¼æª¢æŸ¥
2. BERTåˆ†é¡ï¼šç²¾ç¢ºèªç¾©ç†è§£
3. æ‰¹æ¬¡è™•ç†ï¼šé«˜æ•ˆè™•ç†å¤§é‡æ–‡æª”

Linuså¼è¨­è¨ˆåŸå‰‡ï¼š
- "Good programmers worry about data structures" - å„ªåŒ–æ‰¹æ¬¡æ•¸æ“šæµ
- "Talk is cheap. Show me the code" - å¯¦éš›æ¸¬è©¦è­‰æ˜æ•ˆæœ
"""

import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å°å…¥æ··åˆæª¢æ¸¬å™¨
from .hybrid import HybridLevelSymbolDetector, HybridDetectionResult

@dataclass
class HybridProcessingStats:
    """æ··åˆè™•ç†çµ±è¨ˆè³‡æ–™"""
    file_path: str
    file_name: str
    start_time: float
    end_time: float
    processing_time: float
    total_lines: int
    candidate_lines: int  # è¦å‰‡æª¢æ¸¬å€™é¸è¡Œæ•¸
    bert_processed_lines: int  # BERTè™•ç†è¡Œæ•¸
    total_markers: int
    ultra_strict_markers: int
    bert_refined_markers: int
    rule_only_markers: int
    success: bool
    error_message: str = ""
    bert_model_used: bool = False
    output_data: Optional[Dict] = None

class HybridBatchProcessor:
    """æ··åˆæ‰¹æ¬¡è™•ç†å™¨ - Linuså¼é«˜æ•ˆè¨­è¨ˆ"""
    
    def __init__(self, output_base_dir: str = "hybrid_output", model_path: str = None):
        self.output_base_dir = Path(output_base_dir)
        self.output_base_dir.mkdir(exist_ok=True)
        self.processing_stats: List[HybridProcessingStats] = []
        self.batch_start_time = 0
        self.batch_end_time = 0
        
        # åˆå§‹åŒ–æ··åˆæª¢æ¸¬å™¨
        print("ğŸ”§ åˆå§‹åŒ–æ··åˆæª¢æ¸¬å™¨...")
        self.detector = HybridLevelSymbolDetector(model_path)
        
        # æª¢æŸ¥ BERT æ¨¡å‹ç‹€æ…‹
        if self.detector.is_model_loaded():
            print("âœ… BERT æ¨¡å‹å·²è¼‰å…¥ï¼Œå°‡ä½¿ç”¨æ··åˆæª¢æ¸¬")
        else:
            print("âš ï¸ æœªè¼‰å…¥ BERT æ¨¡å‹ï¼Œå°‡åªä½¿ç”¨è¦å‰‡æª¢æ¸¬")
        
    def process_single_file(self, file_path: Path) -> HybridProcessingStats:
        """è™•ç†å–®ä¸€æª”æ¡ˆï¼ˆæ··åˆæª¢æ¸¬ï¼‰"""
        start_time = time.time()
        stats = HybridProcessingStats(
            file_path=str(file_path),
            file_name=file_path.name,
            start_time=start_time,
            end_time=0,
            processing_time=0,
            total_lines=0,
            candidate_lines=0,
            bert_processed_lines=0,
            total_markers=0,
            ultra_strict_markers=0,
            bert_refined_markers=0,
            rule_only_markers=0,
            success=False,
            bert_model_used=self.detector.is_model_loaded()
        )
        
        try:
            # è¼‰å…¥ JSON æª”æ¡ˆ
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'JFULL' not in data:
                stats.error_message = "æª”æ¡ˆä¸­æ²’æœ‰ 'JFULL' æ¬„ä½"
                return stats
            
            # åˆ†å‰²æ–‡æœ¬è¡Œ
            text_lines = data['JFULL'].split('\n')
            stats.total_lines = len(text_lines)
            
            # åŸ·è¡Œä¸‰å±¤æ··åˆæª¢æ¸¬
            results = self.detector.detect_hybrid_markers(text_lines)
            
            # çµ±è¨ˆçµæœ
            stats.total_markers = sum(1 for r in results if r.final_prediction)
            stats.ultra_strict_markers = sum(1 for r in results if r.method_used == "ultra_strict_pua")
            stats.bert_refined_markers = sum(1 for r in results if r.final_prediction and r.method_used == "soft_rule_bert")
            stats.rule_only_markers = sum(1 for r in results if r.final_prediction and r.method_used in ["ultra_strict_pua", "soft_rule_only"])
            stats.candidate_lines = sum(1 for r in results if r.rule_based_score > 0)
            stats.bert_processed_lines = sum(1 for r in results if r.bert_score > 0 and r.bert_score < 1.0)  # æ’é™¤çµ‚æ¥µåš´æ ¼çš„1.0åˆ†
            
            # åˆ†æçµæœ
            analysis = self.detector.analyze_detection_results()
            
            # æº–å‚™è¼¸å‡ºæ•¸æ“š
            markers_data = []
            for result in results:
                if result.final_prediction:
                    markers_data.append({
                        "line_number": result.line_number,
                        "symbol": result.detected_symbol,
                        "unicode_code": f"U+{ord(result.detected_symbol):04X}" if result.detected_symbol else None,
                        "category": result.symbol_category,
                        "content": result.line_text,
                        "rule_score": result.rule_based_score,
                        "bert_score": result.bert_score,
                        "method": result.method_used,
                        "is_pua": self.detector.rule_detector.is_pua_character(result.detected_symbol) if result.detected_symbol else False
                    })
            
            output_data = {
                "source_file": str(file_path),
                "detection_method": "hybrid_rule_bert",
                "bert_model_used": stats.bert_model_used,
                "timestamp": datetime.now().isoformat(),
                "processing_time": 0,  # å°‡åœ¨å¾Œé¢æ›´æ–°
                "statistics": {
                    "total_lines": stats.total_lines,
                    "candidate_lines": stats.candidate_lines,
                    "bert_processed_lines": stats.bert_processed_lines,
                    "total_markers": stats.total_markers,
                    "ultra_strict_markers": stats.ultra_strict_markers,
                    "bert_refined_markers": stats.bert_refined_markers,
                    "rule_only_markers": stats.rule_only_markers
                },
                "analysis": analysis,
                "markers": markers_data
            }
            
            stats.output_data = output_data
            stats.success = True
            
        except FileNotFoundError:
            stats.error_message = "æª”æ¡ˆä¸å­˜åœ¨"
        except json.JSONDecodeError:
            stats.error_message = "JSON æª”æ¡ˆæ ¼å¼éŒ¯èª¤"
        except Exception as e:
            stats.error_message = f"è™•ç†éŒ¯èª¤: {str(e)}"
        
        # æ›´æ–°æ™‚é–“çµ±è¨ˆ
        stats.end_time = time.time()
        stats.processing_time = stats.end_time - stats.start_time
        
        if stats.success and stats.output_data:
            stats.output_data["processing_time"] = stats.processing_time
        
        return stats
    
    def process_directory(self, input_dir: Path, output_subdir: str) -> Dict:
        """è™•ç†æ•´å€‹ç›®éŒ„"""
        print(f"ğŸ” æƒæç›®éŒ„: {input_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæª”æ¡ˆ
        json_files = list(input_dir.glob("*.json"))
        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
        
        if not json_files:
            return {"error": "æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ"}
        
        # å‰µå»ºè¼¸å‡ºå­ç›®éŒ„
        output_dir = self.output_base_dir / output_subdir
        output_dir.mkdir(exist_ok=True)
        
        # æ‰¹æ¬¡è™•ç†çµ±è¨ˆ
        batch_stats = {
            "detection_method": "hybrid_rule_bert",
            "bert_model_used": self.detector.is_model_loaded(),
            "input_directory": str(input_dir),
            "output_directory": str(output_dir),
            "total_files": len(json_files),
            "successful_files": 0,
            "failed_files": 0,
            "total_processing_time": 0,
            "total_lines": 0,
            "total_candidate_lines": 0,
            "total_bert_processed_lines": 0,
            "total_markers": 0,
            "total_ultra_strict_markers": 0,
            "total_bert_refined_markers": 0,
            "total_rule_only_markers": 0,
            "files": []
        }
        
        self.batch_start_time = time.time()
        
        # è™•ç†æ¯å€‹æª”æ¡ˆ
        for i, file_path in enumerate(json_files, 1):
            print(f"\nğŸ“„ è™•ç†æª”æ¡ˆ {i}/{len(json_files)}: {file_path.name}")
            
            # è™•ç†æª”æ¡ˆ
            stats = self.process_single_file(file_path)
            
            if stats.success:
                # ä¿å­˜å–®å€‹æª”æ¡ˆçµæœ
                output_file = output_dir / f"{file_path.stem}_hybrid_result.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(stats.output_data, f, ensure_ascii=False, indent=2)
                
                batch_stats["successful_files"] += 1
                batch_stats["total_lines"] += stats.total_lines
                batch_stats["total_candidate_lines"] += stats.candidate_lines
                batch_stats["total_bert_processed_lines"] += stats.bert_processed_lines
                batch_stats["total_markers"] += stats.total_markers
                batch_stats["total_ultra_strict_markers"] += stats.ultra_strict_markers
                batch_stats["total_bert_refined_markers"] += stats.bert_refined_markers
                batch_stats["total_rule_only_markers"] += stats.rule_only_markers
                
                print(f"âœ… æˆåŠŸ - æª¢æ¸¬åˆ° {stats.total_markers} å€‹æ¨™è¨˜ (çµ‚æ¥µåš´æ ¼: {stats.ultra_strict_markers}, BERT: {stats.bert_refined_markers}, è»Ÿè¦å‰‡: {stats.rule_only_markers})")
            else:
                batch_stats["failed_files"] += 1
                print(f"âŒ å¤±æ•— - {stats.error_message}")
            
            batch_stats["total_processing_time"] += stats.processing_time
            
            # è¨˜éŒ„æª”æ¡ˆçµ±è¨ˆ
            file_summary = {
                "file_name": stats.file_name,
                "success": stats.success,
                "processing_time": stats.processing_time,
                "total_lines": stats.total_lines,
                "candidate_lines": stats.candidate_lines,
                "bert_processed_lines": stats.bert_processed_lines,
                "total_markers": stats.total_markers,
                "bert_refined_markers": stats.bert_refined_markers,
                "rule_only_markers": stats.rule_only_markers,
                "error_message": stats.error_message
            }
            batch_stats["files"].append(file_summary)
            
            self.processing_stats.append(stats)
        
        self.batch_end_time = time.time()
        
        # ç”Ÿæˆæ‰¹æ¬¡å ±å‘Š
        self._generate_hybrid_batch_report(batch_stats, output_dir)
        
        return batch_stats
    
    def _generate_hybrid_batch_report(self, batch_stats: Dict, output_dir: Path):
        """ç”Ÿæˆæ··åˆæ‰¹æ¬¡è™•ç†å ±å‘Š"""
        avg_time = batch_stats["total_processing_time"] / batch_stats["total_files"] if batch_stats["total_files"] > 0 else 0
        success_rate = (batch_stats["successful_files"] / batch_stats["total_files"] * 100) if batch_stats["total_files"] > 0 else 0
        
        # è¨ˆç®—æ•ˆç‡æŒ‡æ¨™
        candidate_rate = (batch_stats["total_candidate_lines"] / batch_stats["total_lines"] * 100) if batch_stats["total_lines"] > 0 else 0
        bert_refinement_rate = (batch_stats["total_bert_refined_markers"] / batch_stats["total_markers"] * 100) if batch_stats["total_markers"] > 0 else 0
        
        report = f"""
============================================================
âš¡ æ··åˆæ‰¹æ¬¡å±¤ç´šæª¢æ¸¬å ±å‘Š (Linuså¼æ¼¸é€²éæ¿¾)
============================================================

ğŸ”§ æª¢æ¸¬é…ç½®:
  æª¢æ¸¬æ–¹æ³•: {batch_stats['detection_method']}
  BERT æ¨¡å‹: {'âœ… å·²è¼‰å…¥' if batch_stats['bert_model_used'] else 'âŒ æœªè¼‰å…¥'}
  è¼¸å…¥ç›®éŒ„: {batch_stats['input_directory']}
  è¼¸å‡ºç›®éŒ„: {batch_stats['output_directory']}

ğŸ“Š æ‰¹æ¬¡è™•ç†çµ±è¨ˆ:
  ç¸½æª”æ¡ˆæ•¸: {batch_stats['total_files']}
  æˆåŠŸè™•ç†: {batch_stats['successful_files']}
  è™•ç†å¤±æ•—: {batch_stats['failed_files']}
  æˆåŠŸç‡: {success_rate:.1f}%

â±ï¸  æ™‚é–“çµ±è¨ˆ:
  ç¸½è™•ç†æ™‚é–“: {batch_stats['total_processing_time']:.3f} ç§’
  å¹³å‡è™•ç†æ™‚é–“: {avg_time:.3f} ç§’/æª”æ¡ˆ
  æ‰¹æ¬¡ç¸½æ™‚é–“: {self.batch_end_time - self.batch_start_time:.3f} ç§’

ğŸ¯ æª¢æ¸¬çµ±è¨ˆ:
  ç¸½æ–‡æœ¬è¡Œæ•¸: {batch_stats['total_lines']:,}
  å€™é¸è¡Œæ•¸: {batch_stats['total_candidate_lines']:,} ({candidate_rate:.1f}%)
  BERT è™•ç†è¡Œæ•¸: {batch_stats['total_bert_processed_lines']:,}
  
ğŸ“ˆ æª¢æ¸¬çµæœ:
  ç¸½æ¨™è¨˜æ•¸: {batch_stats['total_markers']:,}
  BERT ç²¾ç…‰: {batch_stats['total_bert_refined_markers']:,} ({bert_refinement_rate:.1f}%)
  è¦å‰‡æª¢æ¸¬: {batch_stats['total_rule_only_markers']:,}
  å¹³å‡æ¨™è¨˜/æª”: {batch_stats['total_markers'] / batch_stats['successful_files'] if batch_stats['successful_files'] > 0 else 0:.1f}

âš¡ Linuså¼æ´å¯Ÿ:
  æ¼¸é€²éæ¿¾æ•ˆç‡: {candidate_rate:.1f}% çš„è¡Œé€²å…¥ BERT è™•ç†
  BERT è²¢ç»åº¦: {bert_refinement_rate:.1f}% çš„æ¨™è¨˜ç¶“éèªç¾©ç²¾ç…‰
  æ•´é«”æ€§èƒ½: å¹³è¡¡äº†æº–ç¢ºæ€§å’Œè¨ˆç®—æ•ˆç‡

ğŸ“‹ æª”æ¡ˆè™•ç†æ˜ç´° (å‰20å€‹):
"""
        
        # æŒ‰è™•ç†æ™‚é–“æ’åºé¡¯ç¤ºæª”æ¡ˆ
        sorted_files = sorted(batch_stats["files"], key=lambda x: x["processing_time"], reverse=True)
        for i, file_info in enumerate(sorted_files[:20], 1):
            status = "âœ…" if file_info["success"] else "âŒ"
            bert_info = f" (BERT: {file_info['bert_refined_markers']})" if batch_stats['bert_model_used'] else ""
            report += f"  {i:2}. {status} {file_info['file_name']:40} | {file_info['processing_time']:.3f}s | {file_info['total_markers']} æ¨™è¨˜{bert_info}\n"
        
        if len(sorted_files) > 20:
            report += f"      ... é‚„æœ‰ {len(sorted_files) - 20} å€‹æª”æ¡ˆ\n"
        
        print(report)
        
        # ä¿å­˜è©³ç´°å ±å‘Š
        with open(output_dir / 'batch_hybrid_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        
        # ä¿å­˜ JSON çµ±è¨ˆ
        with open(output_dir / 'batch_hybrid_stats.json', 'w', encoding='utf-8') as f:
            json.dump(batch_stats, f, ensure_ascii=False, indent=2)

def main():
    parser = argparse.ArgumentParser(description="æ··åˆæ‰¹æ¬¡å±¤ç´šæª¢æ¸¬å™¨")
    parser.add_argument("input_dir", help="è¼¸å…¥ç›®éŒ„è·¯å¾‘")
    parser.add_argument("--output", "-o", default="hybrid_output", help="è¼¸å‡ºç›®éŒ„ (é»˜èª: hybrid_output)")
    parser.add_argument("--subdir", "-s", help="è¼¸å‡ºå­ç›®éŒ„åç¨± (é»˜èª: ä½¿ç”¨è¼¸å…¥ç›®éŒ„å)")
    parser.add_argument("--model", "-m", help="BERT æ¨¡å‹è·¯å¾‘ (é»˜èª: bert_level_detector/best_model)")
    
    args = parser.parse_args()
    
    input_path = Path(args.input_dir)
    if not input_path.exists():
        print(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_path}")
        return
    
    if not input_path.is_dir():
        print(f"âŒ éŒ¯èª¤ï¼šè·¯å¾‘ä¸æ˜¯ç›®éŒ„: {input_path}")
        return
    
    # ç¢ºå®š BERT æ¨¡å‹è·¯å¾‘
    model_path = args.model
    if not model_path:
        default_model = Path("bert_level_detector/best_model")
        if default_model.exists():
            model_path = str(default_model)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é è¨­ BERT æ¨¡å‹ï¼Œå°‡åªä½¿ç”¨è¦å‰‡æª¢æ¸¬")
    
    # ç¢ºå®šè¼¸å‡ºå­ç›®éŒ„åç¨±
    output_subdir = args.subdir or input_path.name
    
    print("âš¡ å•Ÿå‹•æ··åˆæ‰¹æ¬¡å±¤ç´šæª¢æ¸¬å™¨")
    print("åŸºæ–¼ Linus 'æ¼¸é€²å¼éæ¿¾' åŸå‰‡")
    print("è¦å‰‡æª¢æ¸¬ + BERT èªç¾©ç†è§£ = æœ€ä½³æº–ç¢ºæ€§")
    print()
    
    # åˆå§‹åŒ–æ‰¹æ¬¡è™•ç†å™¨
    processor = HybridBatchProcessor(args.output, model_path)
    
    # åŸ·è¡Œæ‰¹æ¬¡è™•ç†
    batch_stats = processor.process_directory(input_path, output_subdir)
    
    if "error" in batch_stats:
        print(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {batch_stats['error']}")
        return
    
    print(f"\nğŸ‰ æ··åˆæ‰¹æ¬¡è™•ç†å®Œæˆ!")
    print(f"ğŸ“Š è™•ç†äº† {batch_stats['successful_files']}/{batch_stats['total_files']} å€‹æª”æ¡ˆ")
    print(f"âš¡ ç™¼ç¾ {batch_stats['total_markers']} å€‹æ¨™è¨˜")
    if batch_stats['bert_model_used']:
        print(f"ğŸ¤– BERT ç²¾ç…‰äº† {batch_stats['total_bert_refined_markers']} å€‹æ¨™è¨˜")
    print(f"ğŸ’¾ çµæœä¿å­˜åœ¨: {processor.output_base_dir / output_subdir}")

if __name__ == "__main__":
    main()
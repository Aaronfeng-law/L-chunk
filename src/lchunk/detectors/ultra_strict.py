#!/usr/bin/env python3
"""
çµ‚æ¥µåš´æ ¼æ ¼å¼å±¤ç´šæª¢æ¸¬å™¨
åŸºæ–¼  "é›¶å®¹å¿" åŸå‰‡ï¼š
"è¦å‰‡è¦éº¼æ˜¯çµ•å°çš„ï¼Œè¦éº¼å°±ä¸æ˜¯è¦å‰‡"

çµ‚æ¥µåš´æ ¼è¦å‰‡ï¼š
1. å¿…é ˆä»¥ \r\n é–‹é ­  
2. åªèƒ½æ˜¯å–®å€‹ Unicode å­—ç¬¦
3. å¿…é ˆç·Šè·Ÿ ã€(é “è™Ÿ)
4. å­—ç¬¦å¿…é ˆåœ¨é å®šç¾©ç¯„åœå…§ï¼ˆåŒ…å«PUAï¼‰
5. é›¶ä¾‹å¤–ï¼Œé›¶å¦¥å”
"""

import json
import re
from typing import List, Dict, Set, Tuple
from dataclasses import dataclass

@dataclass
class UltraStrictMarker:
    """çµ‚æ¥µåš´æ ¼æ ¼å¼çš„å±¤ç´šæ¨™è¨˜"""
    line_number: int
    symbol: str
    unicode_code: str
    category: str
    content: str
    is_pua: bool
    has_proper_newline: bool

class UltraStrictDetector:
    """çµ‚æ¥µåš´æ ¼æ ¼å¼æª¢æ¸¬å™¨"""
    
    def __init__(self):
        # å®šç¾©å…è¨±çš„å–®å­—ç¬¦ç¬¦è™Ÿç¯„åœï¼ˆé›¶å®¹å¿ç­–ç•¥ï¼‰
        self.valid_symbol_ranges = {
            # ä¸­æ–‡æ•¸å­—
            "ä¸­æ–‡æ•¸å­—": {
                "chars": {"ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"},
            },
            # å¤§å¯«ä¸­æ–‡æ•¸å­—
            "å¤§å¯«ä¸­æ–‡æ•¸å­—": {
                "chars": {"å£¹", "è²³", "åƒ", "è‚†", "ä¼", "é™¸", "æŸ’", "æŒ", "ç–", "æ‹¾"},
            },
            # å¤©å¹²
            "å¤©å¹²": {
                "chars": {"ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸"},
            },
            # åœ°æ”¯
            "åœ°æ”¯": {
                "chars": {"å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥"},
            },
            # åœå­—ç¬¦è™Ÿ - å¸¶åœˆæ•¸å­—
            "åœå­—æ•¸å­—": {
                "chars": {"ãˆ ", "ãˆ¡", "ãˆ¢", "ãˆ£", "ãˆ¤", "ãˆ¥", "ãˆ¦", "ãˆ§", "ãˆ¨", "ãˆ©"},
            },
            # å¸¶åœˆæ•¸å­—ç³»åˆ— - Unicodeæ¨™æº–åœˆæ•¸å­—
            "å¸¶åœˆæ•¸å­—": {
                "chars": {
                    # U+2460-U+2473: â‘  â‘¡ â‘¢ â‘£ â‘¤ â‘¥ â‘¦ â‘§ â‘¨ â‘© â‘ª â‘« â‘¬ â‘­ â‘® â‘¯ â‘° â‘± â‘² â‘³
                    "â‘ ", "â‘¡", "â‘¢", "â‘£", "â‘¤", "â‘¥", "â‘¦", "â‘§", "â‘¨", "â‘©", 
                    "â‘ª", "â‘«", "â‘¬", "â‘­", "â‘®", "â‘¯", "â‘°", "â‘±", "â‘²", "â‘³",
                    # U+24EA: â“ª (åœˆé›¶)
                    "â“ª"
                },
            },
            # åœå­—ç¬¦è™Ÿ - å¸¶åœˆæ‹¬è™Ÿæ•¸å­—
            "æ‹¬è™Ÿæ•¸å­—": {
                "chars": {"â‘´", "â‘µ", "â‘¶", "â‘·", "â‘¸", "â‘¹", "â‘º", "â‘»", "â‘¼", "â‘½", "â‘¾", "â‘¿", "â’€", "â’", "â’‚", "â’ƒ", "â’„", "â’…", "â’†", "â’‡"},
            },
            # å…¨å½¢æ•¸å­—
            "å…¨å½¢æ•¸å­—": {
                "chars": {"ï¼", "ï¼‘", "ï¼’", "ï¼“", "ï¼”", "ï¼•", "ï¼–", "ï¼—", "ï¼˜", "ï¼™"},
            },
            # ç¾…é¦¬æ•¸å­—
            "ç¾…é¦¬æ•¸å­—": {
                "chars": {"â… ", "â…¡", "â…¢", "â…£", "â…¤", "â…¥", "â…¦", "â…§", "â…¨", "â…©", "â…ª", "â…«"},
            }
        }
        
        # PUAç¬¦è™Ÿåˆ†çµ„ - åŸºæ–¼æ ¼å¼ç²¾ç¢ºåˆ†é¡ï¼ˆé›¶å®¹å¿åˆ†çµ„ï¼‰
        self.pua_symbol_groups = self._init_pua_groups()
        
        # PUAç¯„åœ (Private Use Area) - çµ‚æ¥µç²¾ç¢ºå®šç¾©
        self.pua_ranges = [
            (0xE000, 0xF8FF),   # Basic Multilingual Plane PUA
            (0xF0000, 0xFFFFD), # Supplementary PUA-A
            (0x100000, 0x10FFFD) # Supplementary PUA-B
        ]
        
        # å‰µå»ºæ‰€æœ‰æœ‰æ•ˆå­—ç¬¦çš„é›†åˆï¼ˆç”¨æ–¼å¿«é€ŸæŸ¥æ‰¾ï¼‰
        self.all_valid_chars = set()
        for category_info in self.valid_symbol_ranges.values():
            self.all_valid_chars.update(category_info["chars"])
    
    def _init_pua_groups(self) -> Dict:
        """åˆå§‹åŒ–PUAç¬¦è™Ÿåˆ†çµ„ - ç²¾ç¢ºåˆ†é¡
        
        åˆ†çµ„åŸå‰‡ï¼š
        1. å…¨å½¢ vs åŠå½¢åˆ†é–‹
        2. å°å¯« vs å¤§å¯«åˆ†é–‹  
        3. åœˆåœˆã€æ‹¬å¼§ã€å¥è™Ÿã€é “è™Ÿåˆ†é–‹
        4. ç¾…é¦¬æ•¸å­—ã€é˜¿æ‹‰ä¼¯æ•¸å­—ã€ä¸­æ–‡æ•¸å­—åˆ†é–‹
        5. æ¯çµ„éƒ½æœ‰æ˜ç¢ºçš„å±¤ç´šå®šç¾©
        """
        return {
            # === ç¾…é¦¬æ•¸å­—ç³»åˆ— ===
            "PUA_ç¾…é¦¬æ•¸å­—": {
                "ranges": [(0xF6C5, 0xF6CE)],  # F6C5~F6CE
                "description": "ç¾…é¦¬æ•¸å­—ï¼ˆPUAå®šç¾©ï¼‰",
            },
            
            # === åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—ç³»åˆ— ===
            "PUA_åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—_æ‹¬å¼§": {
                "ranges": [
                    (0xF6BB, 0xF6C4),  # F6BB~F6C4: åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—å¸¶æ‹¬å¼§(1~10)
                    (0xF4FB, 0xF4FF),  # F4FB~F4FF: åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§(11~15)
                    (0xF4EA, 0xF4EB)   # F4EA~F4EB: åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§(16~17)
                ],
                "description": "åŠå½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§ç³»åˆ—",
            },
            
            # === å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—ç³»åˆ— ===
            "PUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_åŸæ•¸å­—": {
                "ranges": [(0xF5E9, 0xF64C)],  # F5E9~F64C: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆ1~100ï¼‰
                "description": "å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—åŸå‹",
            },
            
            "PUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_å¥è™Ÿ": {
                "ranges": [(0xF585, 0xF5E8)],  # F585~F5E8: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—å¥è™Ÿï¼ˆ1~100ï¼‰
                "description": "å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—å¥è™Ÿ",
            },
            
            "PUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_åœˆåœˆ": {
                "ranges": [
                    (0xF6B1, 0xF6BA),  # F6B1~F6BA: é˜¿æ‹‰ä¼¯æ•¸å­—withåœˆåœˆ
                    (0xF521, 0xF584)   # F521~F584: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—withåœˆåœˆ
                ],
                "description": "å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—åœˆåœˆ",
            },
            
            "PUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_æ‹¬å¼§": {
                "ranges": [
                    (0xF514, 0xF520),  # F514~F520: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§ï¼ˆ78~90ï¼‰
                    (0xF500, 0xF501),  # F500~F501: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§ï¼ˆ91~92ï¼‰
                    (0xF4F9, 0xF4FA),  # F4F9~F4FA: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§ï¼ˆ93~94ï¼‰
                    (0xF4EF, 0xF4F4)   # F4EF~F4F4: å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§ï¼ˆ95~100ï¼‰
                ],
                "description": "å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—æ‹¬å¼§",
            },
            
            # === å°å¯«ä¸­æ–‡æ•¸å­—ç³»åˆ— ===
            "PUA_å°å¯«ä¸­æ–‡æ•¸å­—_é “è™Ÿ": {
                "ranges": [(0xF57F, 0xF6B0)],  # F57F~F6B0: å°å¯«åœ‹å­—æ•¸å­—é “è™Ÿï¼ŒäºŒä½æ•¸å‚ç›´æ’åˆ—ï¼ˆ1~50ï¼‰
                "description": "å°å¯«ä¸­æ–‡æ•¸å­—é “è™Ÿï¼ˆäºŒä½æ•¸å‚ç›´ï¼‰",
            },
            
            "PUA_å°å¯«ä¸­æ–‡æ•¸å­—_æ‹¬å¼§": {
                "ranges": [
                    (0xF64D, 0xF67E),  # F64D~F67E: å°å¯«åœ‹å­—æ•¸å­—æ‹¬å¼§ï¼ŒäºŒä½æ•¸å‚ç›´æ’åˆ—ï¼ˆ1~50ï¼‰
                    (0xF502, 0xF509)   # F502~F509: å°å¯«åœ‹å­—æ•¸å­—æ‹¬å¼§ï¼ŒäºŒä½æ•¸å‚ç›´æ’åˆ—ï¼ˆ83~87ï¼‰
                ],
                "description": "å°å¯«ä¸­æ–‡æ•¸å­—æ‹¬å¼§ï¼ˆäºŒä½æ•¸å‚ç›´ï¼‰",
            },
            
            # === å¤©å¹²ç³»åˆ— ===
            "PUA_å¤©å¹²_æ‹¬å¼§": {
                "ranges": [(0xF50A, 0xF513)],  # F50A~F513: å¤©å¹²æ‹¬å¼§
                "description": "å¤©å¹²æ‹¬å¼§",
            }
        }
    
    def is_pua_character(self, char: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºPUAå­—ç¬¦ï¼ˆçµ‚æ¥µç²¾ç¢ºï¼‰"""
        if not char:
            return False
        
        code_point = ord(char)
        for start, end in self.pua_ranges:
            if start <= code_point <= end:
                return True
        return False
    
    def is_valid_symbol(self, char: str) -> Tuple[bool, str]:
        """æª¢æŸ¥å­—ç¬¦æ˜¯å¦åœ¨æœ‰æ•ˆç¯„åœå…§ï¼ˆé›¶å®¹å¿ï¼‰"""
        # æª¢æŸ¥é å®šç¾©ç¯„åœ
        for category, info in self.valid_symbol_ranges.items():
            if char in info["chars"]:
                return True, category
        
        # æª¢æŸ¥PUAç²¾ç¢ºåˆ†çµ„
        pua_group = self.get_pua_group(char)
        if pua_group:
            return True, pua_group
        
        return False, "ç„¡æ•ˆå­—ç¬¦"
    
    def get_pua_group(self, char: str) -> str:
        """ç²å–PUAå­—ç¬¦çš„ç²¾ç¢ºåˆ†çµ„ï¼ˆé›¶å®¹å¿åˆ†é¡ï¼‰
        
        ç‰¹æ®Šåˆä½µè¦å‰‡ï¼šPUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_åŸæ•¸å­— â†’ å…¨å½¢æ•¸å­—
        """
        if not char:
            return ""
        
        code_point = ord(char)
        
        # æª¢æŸ¥æ¯å€‹PUAåˆ†çµ„
        for group_name, group_info in self.pua_symbol_groups.items():
            for start, end in group_info["ranges"]:
                if start <= code_point <= end:
                    # åˆä½µï¼šPUAå…¨å½¢æ•¸å­—èˆ‡æ¨™æº–å…¨å½¢æ•¸å­—çµ±ä¸€
                    if group_name == "PUA_å…¨å½¢é˜¿æ‹‰ä¼¯æ•¸å­—_åŸæ•¸å­—":
                        return "å…¨å½¢æ•¸å­—"  # åˆä½µåˆ°æ¨™æº–å…¨å½¢æ•¸å­—é¡åˆ¥
                    return group_name
        
        # å¦‚æœåœ¨PUAç¯„åœå…§ä½†ä¸åœ¨åˆ†çµ„ä¸­ï¼Œè¿”å›é€šç”¨PUAæ¨™è¨˜
        if self.is_pua_character(char):
            return "PUA_æœªåˆ†çµ„"
        
        return ""
    
    def get_symbol_level(self, char: str, category: str) -> int:
        """ç²å–ç¬¦è™Ÿçš„å±¤ç´šï¼ˆåŸºæ–¼ç²¾ç¢ºåˆ†çµ„ï¼‰
        
        ç‰¹æ®Šè™•ç†ï¼šåˆä½µå¾Œçš„å…¨å½¢æ•¸å­—é¡åˆ¥çµ±ä¸€å±¤ç´š
        """
        # é å®šç¾©ç¯„åœçš„å±¤ç´š
        if category in self.valid_symbol_ranges:
            return self.valid_symbol_ranges[category]["level"]
        
        # åˆä½µè™•ç†ï¼šå¦‚æœæ˜¯åˆä½µå¾Œçš„å…¨å½¢æ•¸å­—ï¼Œä½¿ç”¨é å®šç¾©çš„å±¤ç´š
        if category == "å…¨å½¢æ•¸å­—" and category in self.valid_symbol_ranges:
            return self.valid_symbol_ranges[category]["level"]
        
        # PUAåˆ†çµ„çš„å±¤ç´š
        if category in self.pua_symbol_groups:
            return self.pua_symbol_groups[category]["level"]
        
        # é»˜èªå±¤ç´š
        return 0
    
    def detect_ultra_strict_markers(self, text_lines: List[str]) -> List[UltraStrictMarker]:
        """çµ‚æ¥µåš´æ ¼æª¢æ¸¬å±¤ç´šæ¨™è¨˜"""
        markers = []
        
        # çµ‚æ¥µåš´æ ¼çš„æ­£å‰‡è¡¨é”å¼ï¼šå–®å€‹å­—ç¬¦ + é “è™Ÿï¼Œç„¡ä»»ä½•ä¾‹å¤–
        ultra_strict_pattern = r'^(.{1})ã€(.*)$'
        
        for line_num, line in enumerate(text_lines, 1):
            stripped_line = line.strip()
            
            # è·³éç©ºè¡Œï¼ˆé›¶å®¹å¿ï¼šç©ºè¡Œä¸æ˜¯æœ‰æ•ˆæ¨™è¨˜ï¼‰
            if not stripped_line:
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç¬¦åˆçµ‚æ¥µåš´æ ¼æ ¼å¼
            match = re.match(ultra_strict_pattern, stripped_line)
            if match:
                symbol = match.group(1)
                content_after_comma = match.group(2)
                
                # çµ‚æ¥µåš´æ ¼è¦æ±‚1ï¼šå¿…é ˆæ˜¯å–®å€‹å­—ç¬¦ï¼ˆå·²ç”±æ­£å‰‡ä¿è­‰ï¼‰
                # çµ‚æ¥µåš´æ ¼è¦æ±‚2ï¼šå­—ç¬¦å¿…é ˆåœ¨æœ‰æ•ˆç¯„åœå…§
                is_valid, category = self.is_valid_symbol(symbol)
                
                if is_valid:
                    # çµ‚æ¥µåš´æ ¼è¦æ±‚3ï¼šæª¢æŸ¥å‰ä¸€è¡Œæ˜¯å¦ä»¥\rçµå°¾ï¼ˆå¦‚æœä¸æ˜¯ç¬¬ä¸€è¡Œï¼‰
                    has_proper_newline = True
                    if line_num > 1:
                        prev_line = text_lines[line_num - 2]  # 0-indexed
                        has_proper_newline = prev_line.endswith('\r')
                    
                    # çµ‚æ¥µåš´æ ¼è¦æ±‚4ï¼šåªæœ‰ç¬¦åˆæ‰€æœ‰æ¢ä»¶çš„æ‰è¢«æ¥å—ï¼ˆé›¶å¦¥å”ï¼‰
                    if has_proper_newline:
                        marker = UltraStrictMarker(
                            line_number=line_num,
                            symbol=symbol,
                            unicode_code=f"U+{ord(symbol):04X}",
                            category=category,
                            content=stripped_line,
                            is_pua=self.is_pua_character(symbol),
                            has_proper_newline=has_proper_newline
                        )
                        markers.append(marker)
        
        return markers
    
    def analyze_ultra_strict_structure(self, markers: List[UltraStrictMarker]) -> Dict:
        """åˆ†æçµ‚æ¥µåš´æ ¼æª¢æ¸¬çš„çµæ§‹"""
        analysis = {
            "total_markers": len(markers),
            "by_category": {},
            "by_pua_group": {},
            "by_format_type": {},
            "by_level": {},
            "pua_count": 0,
            "unicode_distribution": {},
            "newline_compliance": 0,
            "structure": []
        }
        
        # æŒ‰é¡åˆ¥çµ±è¨ˆ
        for marker in markers:
            if marker.category not in analysis["by_category"]:
                analysis["by_category"][marker.category] = []
            
            analysis["by_category"][marker.category].append({
                "line": marker.line_number,
                "symbol": marker.symbol,
                "unicode": marker.unicode_code,
                "is_pua": marker.is_pua,
                "has_proper_newline": marker.has_proper_newline
            })
            
            # PUAè¨ˆæ•¸å’Œç²¾ç¢ºåˆ†çµ„çµ±è¨ˆ
            if marker.is_pua:
                analysis["pua_count"] += 1
                
                # PUAç²¾ç¢ºåˆ†çµ„çµ±è¨ˆ
                if marker.category not in analysis["by_pua_group"]:
                    analysis["by_pua_group"][marker.category] = {
                        "count": 0,
                        "description": "",
                        "symbols": []
                    }
                
                analysis["by_pua_group"][marker.category]["count"] += 1
                analysis["by_pua_group"][marker.category]["symbols"].append({
                    "line": marker.line_number,
                    "symbol": marker.symbol,
                    "unicode": marker.unicode_code
                })
                
                # ç²å–PUAåˆ†çµ„çš„è©³ç´°ä¿¡æ¯
                if marker.category in self.pua_symbol_groups:
                    group_info = self.pua_symbol_groups[marker.category]
                    analysis["by_pua_group"][marker.category]["format_type"] = group_info["format_type"]
                    analysis["by_pua_group"][marker.category]["level"] = group_info["level"]
                    analysis["by_pua_group"][marker.category]["description"] = group_info["description"]
            
            # Unicodeåˆ†å¸ƒ
            if marker.unicode_code not in analysis["unicode_distribution"]:
                analysis["unicode_distribution"][marker.unicode_code] = 0
            analysis["unicode_distribution"][marker.unicode_code] += 1
            
            # æ›è¡Œç¬¦åˆæ€§
            if marker.has_proper_newline:
                analysis["newline_compliance"] += 1
        
        # æŒ‰æ ¼å¼é¡å‹çµ±è¨ˆ
        for marker in markers:
            if marker.is_pua and marker.category in self.pua_symbol_groups:
                format_type = self.pua_symbol_groups[marker.category]["format_type"]
                if format_type not in analysis["by_format_type"]:
                    analysis["by_format_type"][format_type] = 0
                analysis["by_format_type"][format_type] += 1
        
        # æŒ‰å±¤ç´šçµ±è¨ˆï¼ˆåŒ…å«PUAï¼‰
        for marker in markers:
            level = self.get_symbol_level(marker.symbol, marker.category)
            if level not in analysis["by_level"]:
                analysis["by_level"][level] = 0
            analysis["by_level"][level] += 1
        
        # çµæ§‹åŒ–è¡¨ç¤º
        for marker in markers:
            level = self.get_symbol_level(marker.symbol, marker.category)
            format_type = ""
            if marker.is_pua and marker.category in self.pua_symbol_groups:
                format_type = self.pua_symbol_groups[marker.category]["format_type"]
            
            analysis["structure"].append({
                "line": marker.line_number,
                "symbol": marker.symbol,
                "unicode": marker.unicode_code,
                "category": marker.category,
                "is_pua": marker.is_pua,
                "content": marker.content[:80] + "..." if len(marker.content) > 80 else marker.content,
                "newline_ok": marker.has_proper_newline
            })
        
        return analysis
    
    def generate_ultra_strict_report(self, markers: List[UltraStrictMarker], analysis: Dict) -> str:
        """ç”Ÿæˆçµ‚æ¥µåš´æ ¼æª¢æ¸¬å ±å‘Š"""
        newline_compliance_rate = (analysis["newline_compliance"] / len(markers) * 100) if markers else 0
        
        report = f"""
============================================================
âš¡ çµ‚æ¥µåš´æ ¼æ ¼å¼å±¤ç´šæª¢æ¸¬å ±å‘Š (ç²¾ç¢ºåˆ†çµ„)
============================================================

ğŸ”’ çµ‚æ¥µåš´æ ¼è¦å‰‡ (é›¶å®¹å¿):
  âœ“ å¿…é ˆä»¥ \\r\\n é–‹é ­
  âœ“ åªèƒ½æ˜¯å–®å€‹ Unicode å­—ç¬¦  
  âœ“ å¿…é ˆç·Šè·Ÿ ã€(é “è™Ÿ)
  âœ“ å­—ç¬¦å¿…é ˆåœ¨é å®šç¾©ç¯„åœå…§ï¼ˆå«PUAç²¾ç¢ºåˆ†çµ„ï¼‰
  âœ“ é›¶ä¾‹å¤–ï¼Œé›¶å¦¥å”

ğŸ“Š æª¢æ¸¬çµæœ:
  ç¬¦åˆçµ‚æ¥µåš´æ ¼æ ¼å¼çš„æ¨™è¨˜: {len(markers)} å€‹
  PUAå­—ç¬¦æ¨™è¨˜: {analysis['pua_count']} å€‹
  æ›è¡Œç¬¦åˆæ€§: {analysis['newline_compliance']}/{len(markers)} ({newline_compliance_rate:.1f}%)
  
ğŸ“ˆ æŒ‰é¡åˆ¥åˆ†å¸ƒ:
"""
        
        for category, items in analysis["by_category"].items():
            is_pua = category.startswith("PUA_")
            pua_mark = " (PUA)" if is_pua else ""
            newline_ok = sum(1 for item in items if item["has_proper_newline"])
            report += f"  {category:35s}{pua_mark}: {len(items):3d} å€‹ (æ›è¡Œ: {newline_ok}/{len(items)})\n"
        
        # PUAç²¾ç¢ºåˆ†çµ„çµ±è¨ˆ
        if analysis["by_pua_group"]:
            report += f"\nğŸ¯ PUAç²¾ç¢ºåˆ†çµ„ (é›¶å®¹å¿åˆ†é¡):\n"
            for group_name, group_data in analysis["by_pua_group"].items():
                format_type = group_data.get("format_type", "æœªçŸ¥")
                level = group_data.get("level", 0)
                description = group_data.get("description", "")
                count = group_data["count"]
                report += f"  L{level} {format_type:15s}: {count:3d} å€‹ - {description}\n"
                
                # é¡¯ç¤ºè©²çµ„çš„å‰5å€‹ç¬¦è™Ÿä½œç‚ºç¤ºä¾‹
                symbols_preview = group_data["symbols"][:5]
                for symbol_info in symbols_preview:
                    report += f"    â””â”€ è¡Œ{symbol_info['line']:4d}: {symbol_info['symbol']} ({symbol_info['unicode']})\n"
                if len(group_data["symbols"]) > 5:
                    report += f"    â””â”€ ... é‚„æœ‰ {len(group_data['symbols']) - 5} å€‹\n"
        
        # æŒ‰æ ¼å¼é¡å‹çµ±è¨ˆ
        if analysis["by_format_type"]:
            report += f"\nğŸ“Š æŒ‰æ ¼å¼é¡å‹åˆ†å¸ƒ:\n"
            for format_type, count in sorted(analysis["by_format_type"].items()):
                report += f"  {format_type:15s}: {count:3d} å€‹\n"
        
        if analysis["by_level"]:
            report += f"\nğŸ“Š æŒ‰å±¤ç´šåˆ†å¸ƒ:\n"
            for level, count in sorted(analysis["by_level"].items()):
                report += f"  Level {level}: {count:3d} å€‹\n"
        
        report += f"\nğŸ”¤ Unicode å­—ç¬¦çµ±è¨ˆ (å‰20å€‹):\n"
        sorted_unicode = sorted(analysis["unicode_distribution"].items(), 
                              key=lambda x: x[1], reverse=True)
        for unicode_code, count in sorted_unicode[:20]:
            # æ‰¾åˆ°å°æ‡‰çš„å­—ç¬¦
            char = chr(int(unicode_code[2:], 16))
            pua_mark = " [PUA]" if any(marker.unicode_code == unicode_code and marker.is_pua 
                                     for marker in markers) else ""
            report += f"  {unicode_code}: '{char}' ({count} æ¬¡){pua_mark}\n"
        
        report += f"\nğŸ“‹ çµ‚æ¥µåš´æ ¼çµæ§‹é è¦½ (å‰15å€‹):\n"
        for i, item in enumerate(analysis["structure"][:15], 1):
            level_str = f"L{item['level']}"
            format_type = f"[{item['format_type']}]" if item['format_type'] else ""
            indent = "  " * (item['level'] if isinstance(item['level'], int) else 0)
            newline_status = "âœ“" if item["newline_ok"] else "âœ—"
            pua_status = "[PUA]" if item["is_pua"] else ""
            report += f"{indent}{level_str} è¡Œ{item['line']:4d} {newline_status}: {item['symbol']} ({item['unicode']}) {pua_status}{format_type} - {item['content'][:50]}...\n"
        
        if len(analysis["structure"]) > 15:
            report += f"  ... é‚„æœ‰ {len(analysis['structure']) - 15} å€‹æ¨™è¨˜\n"
        
        report += f"\nâš¡ çµ‚æ¥µåš´æ ¼æ€§åˆ†æ:\n"
        report += f"  æª¢æ¸¬ç²¾åº¦: 100% (ç¬¦åˆçµ‚æ¥µåš´æ ¼è¦å‰‡)\n"
        report += f"  é›¶å¦¥å”ç‡: 100% (ç„¡ä¾‹å¤–è™•ç†)\n"
        report += f"  å­—ç¬¦ç¯„åœ: {len(self.all_valid_chars)} å€‹é å®šç¾© + {len(self.pua_symbol_groups)} å€‹PUAåˆ†çµ„\n"
        report += f"  PUAç²¾ç¢ºåº¦: {len([g for g in analysis['by_pua_group']]) if 'by_pua_group' in analysis else 0} å€‹ç²¾ç¢ºåˆ†çµ„\n"
        
        return report

def main():
    INPUT_FILE = "data/sample/TPDM,111,æ˜“,564,20250113,1.json"
    
    print("âš¡ å•Ÿå‹•çµ‚æ¥µåš´æ ¼æ ¼å¼å±¤ç´šæª¢æ¸¬å™¨")
    print("è¦å‰‡ï¼š\\r\\n + å–®å­—ç¬¦ + ã€(é›¶ä¾‹å¤–)")
    print()
    
    try:
        # è®€å–JSONæ–‡ä»¶
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if 'JFULL' not in data:
            raise ValueError("JSONæª”æ¡ˆä¸­æ‰¾ä¸åˆ° 'JFULL' æ¬„ä½")
        
        text_lines = data['JFULL'].split('\n')  # ä¿ç•™åŸå§‹æ›è¡Œç¬¦
        print(f"ğŸ“„ è®€å–æ–‡æœ¬ï¼š{len(text_lines)} è¡Œ")
        
        # çµ‚æ¥µåš´æ ¼æª¢æ¸¬
        detector = UltraStrictDetector()
        markers = detector.detect_ultra_strict_markers(text_lines)
        
        print(f"âœ… çµ‚æ¥µåš´æ ¼æª¢æ¸¬å®Œæˆï¼šç™¼ç¾ {len(markers)} å€‹ç¬¦åˆæ ¼å¼çš„æ¨™è¨˜")
        
        # åˆ†æçµæ§‹
        analysis = detector.analyze_ultra_strict_structure(markers)
        
        # ç”Ÿæˆå ±å‘Š
        report = detector.generate_ultra_strict_report(markers, analysis)
        print(report)
        
        # ä¿å­˜çµæœ
        output_data = {
            "input_file": INPUT_FILE,
            "detection_method": "ultra_strict_format",
            "ultra_strict_rules": [
                "å¿…é ˆä»¥ \\r\\n é–‹é ­",
                "åªèƒ½æ˜¯å–®å€‹ Unicode å­—ç¬¦",
                "å¿…é ˆç·Šè·Ÿ ã€(é “è™Ÿ)",
                "å­—ç¬¦å¿…é ˆåœ¨é å®šç¾©ç¯„åœå…§ï¼ˆå«PUAï¼‰",
                "é›¶ä¾‹å¤–ï¼Œé›¶å¦¥å”"
            ],
            "total_markers": len(markers),
            "pua_markers": analysis["pua_count"],
            "newline_compliance": analysis["newline_compliance"],
            "markers": [
                {
                    "line": m.line_number,
                    "symbol": m.symbol,
                    "unicode": m.unicode_code,
                    "category": m.category,
                    "is_pua": m.is_pua,
                    "has_proper_newline": m.has_proper_newline,
                    "content": m.content
                } for m in markers
            ],
            "analysis": analysis
        }
        
        output_file = "ultra_strict_detection.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜: {output_file}")
        
        # æä¾›å­—ç¬¦ç¯„åœè³‡è¨Š
        print(f"\nğŸ”¤ æ”¯æ´çš„å­—ç¬¦ç¯„åœ:")
        for category, info in detector.valid_symbol_ranges.items():
            print(f"  {category:15s}: {len(info['chars'])} å€‹å­—ç¬¦")
        print(f"  PUAå­—ç¬¦ç¯„åœ: {len(detector.pua_ranges)} å€‹ç¯„åœ")
        
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {INPUT_FILE}")
    except json.JSONDecodeError:
        print("âŒ éŒ¯èª¤ï¼šJSON æ ¼å¼ä¸æ­£ç¢º")
    except Exception as e:
        print(f"âŒ åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨ (Intelligent Hybrid Detector)
å…ˆå­¸ç¿’å†æ‡‰ç”¨" åŸå‰‡ï¼šæ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨

è™•ç†æµç¨‹ï¼š
1. æ–‡ä»¶åˆ†å¡Šï¼šä½¿ç”¨ comprehensive_analysis åˆ†ææ–‡ä»¶çµæ§‹
2. å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬ï¼šç”¨ hybrid_detector æª¢æ¸¬æ‰€æœ‰ç¬¦è™Ÿ
3. è¦å‰‡å­¸ç¿’å€é–“ï¼šåœ¨ R-D æˆ– S-D å€é–“å»ºç«‹å±¤ç´šè¦å‰‡
4. å±¤ç´šè¦å‰‡å»ºç«‹ï¼šåˆ†æç¬¦è™Ÿé¡å‹å’Œå±¤ç´šæ¨¡å¼
5. å…¨æ–‡æ‡‰ç”¨ï¼šå°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°æ•´å€‹æ–‡ä»¶

"Good code teaches. Great code learns and then teaches." -
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import warnings
# warnings.filterwarnings('ignore')

# å°å…¥ç¾æœ‰æ¨¡çµ„
sys.path.append('.')
from .hybrid import HybridLevelSymbolDetector, HybridDetectionResult
from ..analyzers.splitter import process_single_file, find_section_patterns
from ..analyzers.comprehensive import analyze_filtered_dataset

@dataclass
class LevelingRule:
    """å±¤ç´šè¦å‰‡å®šç¾©"""
    symbol_category: str
    assigned_level: int
    confidence: float
    learning_source: str  # "R-D", "S-D", "å…¨æ–‡"
    occurrences: int
    examples: List[str]

@dataclass
class LineBasedChunk:
    """åŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ"""
    level: int
    start_line: int
    end_line: int
    chunk_type: str  # "header", "main_text", "facts", "reasons", "facts_and_reasons", "footer", "content", "leveling_symbol"
    content_lines: List[str]
    leveling_symbol: Optional[str] = None  # å¦‚æœæ˜¯å±¤ç´šç¬¦è™Ÿè¡Œ
    chunk_id: str = ""

@dataclass
class IntelligentDetectionResult:
    """è‡ªé©æ‡‰æª¢æ¸¬çµæœ"""
    filename: str
    file_structure: Dict  # comprehensive_analysis çµæœ
    learning_region: str  # "R-D", "S-D", "å…¨æ–‡"
    learned_rules: List[LevelingRule]
    full_detection_results: List[HybridDetectionResult]
    applied_hierarchy: Dict
    processing_stats: Dict
    line_based_chunks: Optional[List[LineBasedChunk]] = None  # æ–°å¢ï¼šåŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ

class IntelligentHybridDetector:
    """è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨"""
    
    def __init__(self, model_path: Optional[str] = None):
        # åˆå§‹åŒ–åŸºç¤æª¢æ¸¬å™¨ - åªåœ¨æœ‰æ¨¡å‹æ™‚æ‰è¼‰å…¥ BERT
        self.hybrid_detector = HybridLevelSymbolDetector(model_path)
        
        # è‡ªé©æ‡‰æª¢æ¸¬çµæœ
        self.detection_results = []
        
        print("ğŸ§  è‡ªé©æ‡‰æ··åˆæª¢æ¸¬å™¨å·²åˆå§‹åŒ–")
        print("ç­–ç•¥ï¼šæ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨")
    
    def detect_special_markers(self, lines: List[str]) -> Dict[str, List[int]]:
        """æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜ï¼šä¸»æ–‡(lv 0), ç†ç”±(lv 0), äº‹å¯¦(lv 0), äº‹å¯¦åŠç†ç”±(lv 0), date1(lv -2), date2(lv -2)"""
        markers = {
            'main_text': [],      # ä¸»æ–‡ (lv 0)
            'reasons': [],        # ç†ç”± (lv 0) 
            'facts': [],          # äº‹å¯¦ (lv 0)
            'facts_and_reasons': [], # äº‹å¯¦åŠç†ç”± (lv 0)
            'dates': []           # æ—¥æœŸ (lv -2)
        }
        
        patterns = find_section_patterns()
        
        for line_num, line in enumerate(lines):
            line_text = line.strip()
            if not line_text:
                continue
            
            # æ¨™æº–åŒ–æ–‡å­—ï¼šç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŠå½¢ç©ºç™½ã€å…¨å½¢ç©ºç™½ã€tabç­‰ï¼‰
            normalized_text = line_text.replace(' ', '').replace('ã€€', '').replace('\t', '')
            
            # æª¢æ¸¬ä¸»æ–‡ (æ”¯æ´ "ä¸»æ–‡", "ä¸»ã€€æ–‡", "ä¸» æ–‡" ç­‰æ ¼å¼)
            if normalized_text == 'ä¸»æ–‡':
                markers['main_text'].append(line_num)
                print(f"ğŸ“ æ‰¾åˆ°ä¸»æ–‡æ¨™è¨˜ (Lv 0): è¡Œ {line_num + 1} ã€Œ{line_text}ã€")
            
            # æª¢æ¸¬äº‹å¯¦ (æ”¯æ´ "äº‹å¯¦", "äº‹ã€€å¯¦", "äº‹ å¯¦" ç­‰æ ¼å¼)
            elif normalized_text == 'äº‹å¯¦':
                markers['facts'].append(line_num)
                print(f"ğŸ“ æ‰¾åˆ°äº‹å¯¦æ¨™è¨˜ (Lv 0): è¡Œ {line_num + 1} ã€Œ{line_text}ã€")
            
            # æª¢æ¸¬ç†ç”± (æ”¯æ´ "ç†ç”±", "ç†ã€€ç”±", "ç† ç”±" ç­‰æ ¼å¼)
            elif normalized_text == 'ç†ç”±':
                markers['reasons'].append(line_num)
                print(f"ğŸ“ æ‰¾åˆ°ç†ç”±æ¨™è¨˜ (Lv 0): è¡Œ {line_num + 1} ã€Œ{line_text}ã€")
            
            # æª¢æ¸¬äº‹å¯¦åŠç†ç”± (æ”¯æ´å„ç¨®ç©ºç™½å­—ç¬¦çµ„åˆ)
            elif normalized_text in ['äº‹å¯¦åŠç†ç”±', 'äº‹å¯¦å’Œç†ç”±']:
                markers['facts_and_reasons'].append(line_num)
                print(f"ğŸ“ æ‰¾åˆ°äº‹å¯¦åŠç†ç”±æ¨™è¨˜ (Lv 0): è¡Œ {line_num + 1} ã€Œ{line_text}ã€")
            
            # æª¢æ¸¬æ—¥æœŸ (ROCæ—¥æœŸæ ¼å¼)
            elif patterns['date_pattern'].search(line_text) or patterns['date_pattern_strict'].search(line_text):
                markers['dates'].append(line_num)
                print(f"ğŸ“ æ‰¾åˆ°æ—¥æœŸæ¨™è¨˜ (Lv -2): è¡Œ {line_num + 1} ã€Œ{line_text}ã€")
        
        return markers
    
    def create_line_based_chunks(self, lines: List[str], detection_results: List[HybridDetectionResult], 
                                learned_rules: List[LevelingRule]) -> List[LineBasedChunk]:
        """åŸºæ–¼è¡Œçš„åˆ†å¡Šæ–¹æ³•ï¼š
        1. æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜ï¼šä¸»æ–‡ã€ç†ç”±ã€äº‹å¯¦ã€äº‹å¯¦åŠç†ç”±ã€æ—¥æœŸ
        2. header (lv -3): ä¸»æ–‡ä¹‹å‰çš„è¡Œ
        3. footer (lv -3): æœ€å¾Œæ—¥æœŸä¹‹å¾Œçš„è¡Œ  
        4. content (lv -1): å…©å€‹å±¤ç´šç¬¦è™Ÿè¡Œä¹‹é–“çš„å…§å®¹
        5. leveling_symbol (lv 1,2,3...): æª¢æ¸¬åˆ°çš„å±¤ç´šç¬¦è™Ÿè¡Œ
        """
        print("ğŸ—ï¸ é–‹å§‹åŸºæ–¼è¡Œçš„åˆ†å¡Š...")
        
        # æ­¥é©Ÿ1: æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜
        special_markers = self.detect_special_markers(lines)
        
        # æ­¥é©Ÿ2: å»ºç«‹è¦å‰‡æ˜ å°„ (å¾å­¸ç¿’éšæ®µç²å¾—)
        level_mapping = {}
        for rule in learned_rules:
            level_mapping[rule.symbol_category] = rule.assigned_level

        # æ”¶é›†æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜è¡Œï¼ˆåŒ…å«æ—¥æœŸï¼‰
        special_line_set = set()
        for marker_lines in special_markers.values():
            special_line_set.update(marker_lines)

        def emit_content_segment(segment_indices: List[int]):
            if not segment_indices:
                return
            segment_lines = [lines[idx] for idx in segment_indices]
            chunks.append(LineBasedChunk(
                level=-1,
                start_line=segment_indices[0],
                end_line=segment_indices[-1],
                chunk_type="content",
                content_lines=segment_lines,
                chunk_id=f"content_{segment_indices[0]}_{segment_indices[-1]}"
            ))

        def collect_content_segments(start_idx: int, end_idx: int):
            current_indices: List[int] = []
            for idx in range(start_idx, end_idx):
                if idx in special_line_set or idx in leveling_symbol_lines:
                    if current_indices:
                        emit_content_segment(current_indices)
                        current_indices = []
                    continue
                current_indices.append(idx)
            if current_indices:
                emit_content_segment(current_indices)

        # æ­¥é©Ÿ3: æ¨™è¨˜æ‰€æœ‰å±¤ç´šç¬¦è™Ÿè¡Œ
        leveling_symbol_lines = {}  # line_number -> (symbol, category, level)
        for result in detection_results:
            if result.final_prediction:
                symbol_category = result.symbol_category
                assigned_level = level_mapping.get(symbol_category, 1)  # é è¨­å±¤ç´š1
                leveling_symbol_lines[result.line_number - 1] = (
                    result.detected_symbol, symbol_category, assigned_level
                )
        
        # æ­¥é©Ÿ4: ç¢ºå®šé—œéµåˆ†ç•Œé»
        # æ‰¾åˆ°ä¸»æ–‡ä½ç½® (Lv 0)
        main_text_line = special_markers['main_text'][0] if special_markers['main_text'] else None
        
        # æ‰¾åˆ°æœ€å¾Œçš„æ—¥æœŸä½ç½® (Lv -2)  
        last_date_line = max(special_markers['dates']) if special_markers['dates'] else None
        
        # æ­¥é©Ÿ5: å»ºç«‹åˆ†å¡Š
        chunks = []
        
        # Headerå€åŸŸ (Lv -3): ä¸»æ–‡ä¹‹å‰
        if main_text_line is not None and main_text_line > 0:
            header_content = lines[:main_text_line]
            chunks.append(LineBasedChunk(
                level=-3,
                start_line=0,
                end_line=main_text_line - 1,
                chunk_type="header",
                content_lines=header_content,
                chunk_id="header"
            ))
            print(f"ğŸ“ Headerå€åŸŸ: è¡Œ 1-{main_text_line} (Lv -3)")
        
        # è™•ç†ä¸»è¦å…§å®¹å€åŸŸ
        content_start = main_text_line if main_text_line is not None else 0
        content_end = last_date_line if last_date_line is not None else len(lines) - 1
        
        # ç‰¹æ®Šæ¨™è¨˜è™•ç† (Lv 0)
        for marker_type, line_numbers in special_markers.items():
            for line_num in line_numbers:
                if content_start <= line_num <= content_end:
                    chunk_type = marker_type if marker_type != 'dates' else 'date'
                    chunks.append(LineBasedChunk(
                        level=0,
                        start_line=line_num,
                        end_line=line_num,
                        chunk_type=chunk_type,
                        content_lines=[lines[line_num]],
                        chunk_id=f"{chunk_type}_{line_num}"
                    ))
        
        # å…§å®¹å€åŸŸåˆ†å¡Šï¼šæ ¹æ“šå±¤ç´šç¬¦è™Ÿè¡Œåˆ†å‰²
        sorted_symbol_lines = sorted(leveling_symbol_lines.keys())
        
        current_pos = content_start
        for symbol_line in sorted_symbol_lines:
            if symbol_line < content_start or symbol_line > content_end:
                continue
                
            # å±¤ç´šç¬¦è™Ÿè¡Œä¹‹å‰çš„å…§å®¹ (Lv -1)
            if current_pos < symbol_line:
                collect_content_segments(current_pos, symbol_line)
            
            # å±¤ç´šç¬¦è™Ÿè¡Œæœ¬èº«
            symbol, category, level = leveling_symbol_lines[symbol_line]
            chunks.append(LineBasedChunk(
                level=level,
                start_line=symbol_line,
                end_line=symbol_line,
                chunk_type="leveling_symbol",
                content_lines=[lines[symbol_line]],
                leveling_symbol=symbol,
                chunk_id=f"level_{level}_{symbol_line}"
            ))
            
            current_pos = symbol_line + 1
        
        # æœ€å¾Œä¸€å€‹å±¤ç´šç¬¦è™Ÿå¾Œçš„å…§å®¹
        if current_pos <= content_end:
            collect_content_segments(current_pos, content_end + 1)
        
        # Footerå€åŸŸ (Lv -3): æœ€å¾Œæ—¥æœŸä¹‹å¾Œ
        if last_date_line is not None and last_date_line < len(lines) - 1:
            footer_content = lines[last_date_line + 1:]
            chunks.append(LineBasedChunk(
                level=-3,
                start_line=last_date_line + 1,
                end_line=len(lines) - 1,
                chunk_type="footer",
                content_lines=footer_content,
                chunk_id="footer"
            ))
            print(f"ğŸ“ Footerå€åŸŸ: è¡Œ {last_date_line + 2}-{len(lines)} (Lv -3)")
        
        # æŒ‰è¡Œè™Ÿæ’åº
        chunks.sort(key=lambda x: x.start_line)
        
        print(f"âœ… å®ŒæˆåŸºæ–¼è¡Œçš„åˆ†å¡Š: {len(chunks)} å€‹åˆ†å¡Š")
        
        # çµ±è¨ˆåˆ†å¡Šé¡å‹
        chunk_stats = {}
        for chunk in chunks:
            level_type = f"Lv{chunk.level}_{chunk.chunk_type}"
            chunk_stats[level_type] = chunk_stats.get(level_type, 0) + 1
        
        print("ğŸ“Š åˆ†å¡Šçµ±è¨ˆ:")
        for level_type, count in sorted(chunk_stats.items()):
            print(f"   {level_type}: {count} å€‹")
        
        return chunks
    
    def concatenate_level_content(self, chunks: List[LineBasedChunk]) -> Dict[str, List[str]]:
        """åˆä½µç›¸åŒå±¤ç´šçš„å…§å®¹ (æ­¥é©Ÿ5: Concat the content of Lv -1 between lv 0 1 2 3 4 and so on)"""
        print("ğŸ”— åˆä½µç›¸åŒå±¤ç´šçš„å…§å®¹...")
        
        level_content = {}
        
        for chunk in chunks:
            level_key = f"Lv_{chunk.level}"
            if level_key not in level_content:
                level_content[level_key] = []
            
            # åˆä½µå…§å®¹ï¼Œä¸¦è¨˜éŒ„åˆ†å¡Šä¿¡æ¯
            chunk_info = f"[{chunk.chunk_type}:{chunk.start_line+1}-{chunk.end_line+1}]"
            level_content[level_key].append(chunk_info)
            level_content[level_key].extend(chunk.content_lines)
        
        # é¡¯ç¤ºåˆä½µçµæœçµ±è¨ˆ
        print("ğŸ“‹ å±¤ç´šå…§å®¹åˆä½µçµæœ:")
        for level, content in sorted(level_content.items()):
            line_count = len([line for line in content if not line.startswith('[')])
            print(f"   {level}: {line_count} è¡Œå…§å®¹")
        
        return level_content
    
    def analyze_file_structure(self, file_path: Path) -> Tuple[bool, Dict]:
        """åˆ†ææª”æ¡ˆçµæ§‹ - ä½¿ç”¨ comprehensive_analysis çš„é‚è¼¯"""
        try:
            # ä½¿ç”¨ judgment_splitter è™•ç†å–®å€‹æª”æ¡ˆ
            success, result = process_single_file(file_path)
            
            if not success:
                return False, {}
            
            # è®€å–åŸå§‹æ•¸æ“š
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # åˆ†æç« ç¯€çµæ§‹ - result æ˜¯å­—å…¸
            sections = result.get('sections', {}) if isinstance(result, dict) else {}
            has_main_text = bool(sections.get('main_text', []))
            has_facts = bool(sections.get('facts', []))
            has_reasons = bool(sections.get('reasons', []))
            has_facts_and_reasons = bool(sections.get('facts_and_reasons', []))
            
            # ç¢ºå®šå­¸ç¿’å€é–“é¡å‹
            learning_region = None
            learning_lines = []
            
            if has_facts_and_reasons:
                # S-D å€é–“ï¼šå¾ facts_and_reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "S-D"
                fr_lines = sections.get('facts_and_reasons', [])
                if fr_lines:
                    # ç²å– facts_and_reasons é–‹å§‹çš„è¡Œè™Ÿ
                    full_lines = data['JFULL'].split('\n')
                    fr_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [l.strip() for l in fr_lines[:3]]:
                            fr_start_line = i
                            break
                    
                    if fr_start_line is not None:
                        learning_lines = full_lines[fr_start_line:]
            
            elif has_reasons:
                # R-D å€é–“ï¼šå¾ reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "R-D"
                reasons_lines = sections.get('reasons', [])
                if reasons_lines:
                    full_lines = data['JFULL'].split('\n')
                    reasons_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [l.strip() for l in reasons_lines[:3]]:
                            reasons_start_line = i
                            break
                    
                    if reasons_start_line is not None:
                        learning_lines = full_lines[reasons_start_line:]
            
            if not learning_region:
                # æ²’æœ‰ R æˆ– S ç« ç¯€ï¼Œä½¿ç”¨å…¨æ–‡
                learning_region = "å…¨æ–‡"
                learning_lines = data['JFULL'].split('\n')
            
            structure_info = {
                'sections': sections,
                'has_main_text': has_main_text,
                'has_facts': has_facts,
                'has_reasons': has_reasons,
                'has_facts_and_reasons': has_facts_and_reasons,
                'learning_region': learning_region,
                'learning_lines': learning_lines,
                'full_text_lines': data['JFULL'].split('\n'),
                'total_lines': len(data['JFULL'].split('\n'))
            }
            
            return True, structure_info
        
        except Exception as e:
            print(f"âŒ åˆ†ææª”æ¡ˆçµæ§‹å¤±æ•—: {e}")
            return False, {}
    
    def learn_leveling_rules(self, learning_lines: List[str], learning_region: str) -> List[LevelingRule]:
        """åœ¨å­¸ç¿’å€é–“å»ºç«‹å±¤ç´šè¦å‰‡ - å®Œå…¨å‹•æ…‹å­¸ç¿’
        
        ä¸å†ä¾è³´ä»»ä½•é å®šç¾©å±¤ç´šï¼Œå®Œå…¨åŸºæ–¼æ–‡ä»¶æœ¬èº«çš„ç¬¦è™Ÿå‡ºç¾é †åº
        """
        print(f"ğŸ“ åœ¨ {learning_region} å€é–“å­¸ç¿’å±¤ç´šè¦å‰‡...")
        print(f"   å­¸ç¿’ç¯„åœ: {len(learning_lines)} è¡Œ")
        
        # åœ¨å­¸ç¿’å€é–“åŸ·è¡Œæª¢æ¸¬
        learning_results = self.hybrid_detector.detect_hybrid_markers(learning_lines)
        
        # ç²å–å­¸ç¿’å€é–“çš„å±¤ç´šåˆ†æ
        self.hybrid_detector.detection_results = learning_results
        hierarchy_analysis = self.hybrid_detector.detect_hierarchy_levels()
        
        if not hierarchy_analysis or not hierarchy_analysis.get('level_mapping'):
            print("âš ï¸ å­¸ç¿’å€é–“æœªç™¼ç¾æœ‰æ•ˆçš„å±¤ç´šè¦å‰‡")
            return []
        
        # å»ºç«‹è¦å‰‡ - å®Œå…¨åŸºæ–¼å­¸ç¿’çš„å±¤ç´š
        rules = []
        level_mapping = hierarchy_analysis['level_mapping']
        
        print(f"âœ… å­¸ç¿’åˆ° {len(level_mapping)} ç¨®ç¬¦è™Ÿé¡å‹çš„å±¤ç´šè¦å‰‡")
        
        for symbol_category, level_info in level_mapping.items():
            rule = LevelingRule(
                symbol_category=symbol_category,
                assigned_level=level_info['assigned_level'],
                confidence=level_info['count'] / len([r for r in learning_results if r.final_prediction]),
                learning_source=learning_region,
                occurrences=level_info['count'],
                examples=[ex['text'][:50] + '...' for ex in level_info['examples'][:3]]
            )
            rules.append(rule)
            
            print(f"   ğŸ“‹ {symbol_category}: Level {rule.assigned_level} (ä¿¡å¿ƒåº¦: {rule.confidence:.3f})")
        
        return rules

    def apply_leveling_rules(self, full_results: List[HybridDetectionResult], 
                           learned_rules: List[LevelingRule]) -> Dict:
        """å°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°å…¨æ–‡æª¢æ¸¬çµæœ"""
        print("ğŸ”§ å°‡å­¸ç¿’è¦å‰‡æ‡‰ç”¨åˆ°å…¨æ–‡...")
        
        # å»ºç«‹è¦å‰‡æ˜ å°„
        rule_mapping = {}
        for rule in learned_rules:
            rule_mapping[rule.symbol_category] = rule.assigned_level
        
        # æ‡‰ç”¨è¦å‰‡åˆ°å…¨æ–‡çµæœ
        enhanced_hierarchy = []
        unknown_categories = set()
        next_available_level = max(rule_mapping.values()) + 1 if rule_mapping else 1
        
        for result in full_results:
            if not result.final_prediction:
                continue
            
            symbol_category = result.symbol_category
            
            if symbol_category in rule_mapping:
                # ä½¿ç”¨å­¸ç¿’åˆ°çš„è¦å‰‡
                assigned_level = rule_mapping[symbol_category]
            else:
                # æ–°çš„ç¬¦è™Ÿé¡å‹ï¼Œåˆ†é…æ–°å±¤ç´š
                if symbol_category not in unknown_categories:
                    rule_mapping[symbol_category] = next_available_level
                    unknown_categories.add(symbol_category)
                    next_available_level += 1
                
                assigned_level = rule_mapping[symbol_category]
            
            enhanced_hierarchy.append({
                'line_number': result.line_number,
                'detected_symbol': result.detected_symbol,
                'symbol_category': symbol_category,
                'assigned_level': assigned_level,
                'is_learned_rule': symbol_category not in unknown_categories,
                'line_text': result.line_text,
                'method_used': result.method_used,
                'bert_score': result.bert_score
            })
        
        # å‰µå»ºå±¤ç´šæ˜ å°„çµ±è¨ˆ
        level_stats = {}
        for item in enhanced_hierarchy:
            category = item['symbol_category']
            if category not in level_stats:
                level_stats[category] = {
                    'assigned_level': item['assigned_level'],
                    'count': 0,
                    'is_learned': item['is_learned_rule'],
                    'examples': []
                }
            level_stats[category]['count'] += 1
            if len(level_stats[category]['examples']) < 3:
                level_stats[category]['examples'].append({
                    'line': item['line_number'],
                    'symbol': item['detected_symbol'],
                    'text': item['line_text'][:50] + '...'
                })
        
        print(f"âœ… è¦å‰‡æ‡‰ç”¨å®Œæˆ:")
        print(f"   å·²çŸ¥è¦å‰‡: {len(rule_mapping) - len(unknown_categories)} ç¨®")
        print(f"   æ–°ç™¼ç¾: {len(unknown_categories)} ç¨®")
        print(f"   ç¸½å±¤ç´šç¬¦è™Ÿ: {len(enhanced_hierarchy)} å€‹")
        
        return {
            'enhanced_hierarchy': enhanced_hierarchy,
            'level_mapping': level_stats,
            'rule_coverage': (len(rule_mapping) - len(unknown_categories)) / len(rule_mapping) if rule_mapping else 0,
            'total_levels': len(set(item['assigned_level'] for item in enhanced_hierarchy)),
            'total_symbols': len(enhanced_hierarchy)
        }
    
    def process_single_file(self, file_path: Path) -> Optional[IntelligentDetectionResult]:
        """è™•ç†å–®å€‹æª”æ¡ˆ - å®Œæ•´çš„è‡ªé©æ‡‰æª¢æ¸¬æµç¨‹ + åŸºæ–¼è¡Œçš„åˆ†å¡Š"""
        print(f"\nğŸ” è‡ªé©æ‡‰æª¢æ¸¬: {file_path.name}")
        
        # æ­¥é©Ÿ1: æ–‡ä»¶åˆ†å¡Š
        success, structure_info = self.analyze_file_structure(file_path)
        if not success:
            print(f"âŒ æª”æ¡ˆçµæ§‹åˆ†æå¤±æ•—")
            return None
        
        learning_region = structure_info['learning_region']
        print(f"ğŸ“Š æª”æ¡ˆçµæ§‹: {learning_region} æ¨¡å¼")
        
        # æ­¥é©Ÿ2: å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬
        print("ğŸ” åŸ·è¡Œå…¨æ–‡å±¤ç´šç¬¦è™Ÿæª¢æ¸¬...")
        full_text_lines = structure_info['full_text_lines']
        full_detection_results = self.hybrid_detector.detect_hybrid_markers(full_text_lines)
        
        # æ­¥é©Ÿ3: è¦å‰‡å­¸ç¿’å€é–“
        learning_lines = structure_info['learning_lines']
        learned_rules = self.learn_leveling_rules(learning_lines, learning_region)
        
        # æ­¥é©Ÿ4: å±¤ç´šè¦å‰‡å»ºç«‹èˆ‡å…¨æ–‡æ‡‰ç”¨
        applied_hierarchy = self.apply_leveling_rules(full_detection_results, learned_rules)
        
        # æ­¥é©Ÿ5: åŸºæ–¼è¡Œçš„åˆ†å¡Š (æ–°å¢)
        print("ğŸ—ï¸ åŸ·è¡ŒåŸºæ–¼è¡Œçš„åˆ†å¡Š...")
        line_based_chunks = self.create_line_based_chunks(full_text_lines, full_detection_results, learned_rules)
        
        # æ­¥é©Ÿ6: åˆä½µç›¸åŒå±¤ç´šå…§å®¹ (æ–°å¢)
        level_content = self.concatenate_level_content(line_based_chunks)
        
        # è™•ç†çµ±è¨ˆ
        processing_stats = {
            'total_lines': structure_info['total_lines'],
            'learning_lines': len(learning_lines),
            'total_symbols_detected': len([r for r in full_detection_results if r.final_prediction]),
            'learned_rules_count': len(learned_rules),
            'rule_coverage': applied_hierarchy['rule_coverage'],
            'final_levels': applied_hierarchy['total_levels'],
            'line_based_chunks_count': len(line_based_chunks),  # æ–°å¢
            'level_content_summary': {k: len([line for line in v if not line.startswith('[')]) 
                                    for k, v in level_content.items()}  # æ–°å¢
        }
        
        result = IntelligentDetectionResult(
            filename=file_path.name,
            file_structure=structure_info,
            learning_region=learning_region,
            learned_rules=learned_rules,
            full_detection_results=full_detection_results,
            applied_hierarchy=applied_hierarchy,
            processing_stats=processing_stats,
            line_based_chunks=line_based_chunks  # æ–°å¢
        )
        
        return result
    
    def process_sample_directory(self, sample_dir: Path):
        """è™•ç† sample ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ"""
        print(f"ğŸš€ è‡ªé©æ‡‰æ‰¹é‡æª¢æ¸¬: {sample_dir}")
        print("="*80)
        
        if not sample_dir.exists():
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {sample_dir}")
            return
        
        json_files = list(sample_dir.glob("*.json"))
        if not json_files:
            print(f"âŒ åœ¨ {sample_dir} ä¸­æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")
        
        all_results = []
        learning_region_stats = {'S-D': 0, 'R-D': 0, 'å…¨æ–‡': 0}
        
        for i, json_file in enumerate(json_files, 1):
            print(f"\n[{i}/{len(json_files)}] è™•ç†: {json_file.name}")
            
            result = self.process_single_file(json_file)
            if result:
                all_results.append(result)
                learning_region_stats[result.learning_region] += 1
            else:
                print(f"âŒ è™•ç†å¤±æ•—: {json_file.name}")
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        self.generate_batch_report(all_results, learning_region_stats)
    
    def generate_batch_report(self, results: List[IntelligentDetectionResult], 
                            region_stats: Dict):
        """ç”Ÿæˆæ‰¹é‡è™•ç†å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        report_file = output_dir / f"adaptive_detection_report_{timestamp}.md"
        
        report = f"""# è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
è™•ç†æª”æ¡ˆ: {len(results)} å€‹

## ğŸ“Š æ•´é«”çµ±è¨ˆ

### å­¸ç¿’å€é–“åˆ†å¸ƒ
- **S-D å€é–“** (äº‹å¯¦ç†ç”±åˆä½µ): {region_stats['S-D']} æª”æ¡ˆ
- **R-D å€é–“** (ç†ç”±ç« ç¯€): {region_stats['R-D']} æª”æ¡ˆ  
- **å…¨æ–‡æª¢æ¸¬**: {region_stats['å…¨æ–‡']} æª”æ¡ˆ

### è™•ç†çµ±è¨ˆ
"""
        
        if results:
            total_lines = sum(r.processing_stats['total_lines'] for r in results)
            total_symbols = sum(r.processing_stats['total_symbols_detected'] for r in results)
            avg_coverage = sum(r.processing_stats['rule_coverage'] for r in results) / len(results)
            
            report += f"- ç¸½è¡Œæ•¸: {total_lines:,}\n"
            report += f"- ç¸½ç¬¦è™Ÿæ•¸: {total_symbols:,}\n"
            report += f"- å¹³å‡è¦å‰‡è¦†è“‹ç‡: {avg_coverage:.1%}\n"
            
            # å„æª”æ¡ˆè©³ç´°çµæœ
            report += "\n## ğŸ“‹ å„æª”æ¡ˆæª¢æ¸¬çµæœ\n\n"
            
            for i, result in enumerate(results, 1):
                stats = result.processing_stats
                hierarchy = result.applied_hierarchy
                
                report += f"### {i}. {result.filename}\n"
                report += f"- **å­¸ç¿’æ¨¡å¼**: {result.learning_region}\n"
                report += f"- **ç¸½è¡Œæ•¸**: {stats['total_lines']:,}\n"
                report += f"- **å­¸ç¿’ç¯„åœ**: {stats['learning_lines']:,} è¡Œ\n"
                report += f"- **æª¢æ¸¬ç¬¦è™Ÿ**: {stats['total_symbols_detected']:,} å€‹\n"
                report += f"- **å­¸ç¿’è¦å‰‡**: {stats['learned_rules_count']} ç¨®\n"
                report += f"- **è¦å‰‡è¦†è“‹**: {stats['rule_coverage']:.1%}\n"
                report += f"- **æœ€çµ‚å±¤ç´š**: {stats['final_levels']} å±¤\n"
                report += f"- **åŸºæ–¼è¡Œåˆ†å¡Š**: {stats.get('line_based_chunks_count', 0)} å€‹\n"
                
                # é¡¯ç¤ºå±¤ç´šå…§å®¹çµ±è¨ˆ
                if 'level_content_summary' in stats:
                    report += f"\n**å±¤ç´šå…§å®¹çµ±è¨ˆ:**\n"
                    for level, line_count in sorted(stats['level_content_summary'].items()):
                        report += f"  - {level}: {line_count} è¡Œ\n"
                
                # é¡¯ç¤ºå­¸ç¿’åˆ°çš„è¦å‰‡
                if result.learned_rules:
                    report += f"\n**å­¸ç¿’è¦å‰‡:**\n"
                    for rule in result.learned_rules[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        report += f"  - {rule.symbol_category}: L{rule.assigned_level} (ä¿¡å¿ƒåº¦: {rule.confidence:.3f})\n"
                
                # é¡¯ç¤ºå±¤ç´šçµæ§‹é è¦½
                if hierarchy.get('enhanced_hierarchy'):
                    report += f"\n**å±¤ç´šçµæ§‹é è¦½:**\n"
                    for item in hierarchy['enhanced_hierarchy'][:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        learned_mark = "âœ“" if item['is_learned_rule'] else "âœ—"
                        indent = "  " * item['assigned_level']
                        report += f"  {indent}L{item['assigned_level']} {learned_mark} è¡Œ{item['line_number']:4}: {item['detected_symbol']} - {item['line_text'][:40]}...\n"
                
                report += "\n"
        
        # ä¿å­˜å ±å‘Š
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… è‡ªé©æ‡‰æª¢æ¸¬å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜è©³ç´°æ•¸æ“š
        json_file = output_dir / f"adaptive_detection_data_{timestamp}.json"
        json_data = []
        
        for result in results:
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            chunk_data = []
            if result.line_based_chunks:
                for chunk in result.line_based_chunks:
                    chunk_data.append({
                        'level': chunk.level,
                        'start_line': chunk.start_line,
                        'end_line': chunk.end_line,
                        'chunk_type': chunk.chunk_type,
                        'content_lines_count': len(chunk.content_lines),
                        'leveling_symbol': chunk.leveling_symbol,
                        'chunk_id': chunk.chunk_id
                    })
            
            json_data.append({
                'filename': result.filename,
                'learning_region': result.learning_region,
                'processing_stats': result.processing_stats,
                'learned_rules': [
                    {
                        'symbol_category': rule.symbol_category,
                        'assigned_level': rule.assigned_level,
                        'confidence': rule.confidence,
                        'learning_source': rule.learning_source,
                        'occurrences': rule.occurrences,
                        'examples': rule.examples
                    } for rule in result.learned_rules
                ],
                'applied_hierarchy': result.applied_hierarchy,
                'line_based_chunks': chunk_data  # æ–°å¢
            })
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š è©³ç´°æ•¸æ“šå·²ä¿å­˜: {json_file}")

def main():
    """ä¸»å‡½æ•¸ - è‡ªé©æ‡‰æª¢æ¸¬æ¼”ç¤º"""
    print("ğŸ§  è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨")
    print("åŸºæ–¼  'å…ˆå­¸ç¿’å†æ‡‰ç”¨' åŸå‰‡")
    print("æ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨")
    print("="*80)
    
    # åˆå§‹åŒ–è‡ªé©æ‡‰æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = IntelligentHybridDetector(model_path if Path(model_path).exists() else None)

    # è™•ç† sample ç›®éŒ„
    sample_dir = Path("data/processed/sample")
    detector.process_sample_directory(sample_dir)

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨ (Intelligent Hybrid Detector)
å…ˆå­¸ç¿’å†æ‡‰ç”¨" åŸå‰‡ï¼šæ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨

è™•ç†æµç¨‹ï¼š
1. æ–‡ä»¶åˆ†å¡Šï¼šä½¿ç”¨ comprehensive_analysis åˆ†ææ–‡ä»¶çµæ§‹
2. å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬ï¼šç”¨ hybrid_detector æª¢æ¸¬æ‰€æœ‰ç¬¦è™Ÿ
3. è¦å‰‡å­¸ç¿’å€é–“ï¼šåœ¨ R-D æˆ– S-D å€é–“å»ºç«‹å±¤ç´šè¦å‰‡
4. å±¤ç´šè¦å‰‡å»ºç«‹ï¼šåˆ†æç¬¦è™Ÿé¡å‹å’Œå±¤ç´šæ¨¡å¼
5. å…¨æ–‡æ‡‰ç”¨ï¼šå°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°æ•´å€‹æ–‡ä»¶

"Good code teaches. Great code learns and then teaches." -
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import warnings
# warnings.filterwarnings('ignore')

# å°å…¥ç¾æœ‰æ¨¡çµ„
sys.path.append('.')
from .hybrid import HybridLevelSymbolDetector, HybridDetectionResult
from ..analyzers.splitter import process_single_file, find_section_patterns
from ..analyzers.comprehensive import analyze_filtered_dataset

@dataclass
class LevelingRule:
    """å±¤ç´šè¦å‰‡å®šç¾©"""
    symbol_category: str
    assigned_level: int
    confidence: float
    learning_source: str  # "R-D", "S-D", "å…¨æ–‡"
    occurrences: int
    examples: List[str]

@dataclass
class IntelligentDetectionResult:
    """è‡ªé©æ‡‰æª¢æ¸¬çµæœ"""
    filename: str
    file_structure: Dict  # comprehensive_analysis çµæœ
    learning_region: str  # "R-D", "S-D", "å…¨æ–‡"
    learned_rules: List[LevelingRule]
    full_detection_results: List[HybridDetectionResult]
    applied_hierarchy: Dict
    processing_stats: Dict

class IntelligentHybridDetector:
    """è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨"""
    
    def __init__(self, model_path: Optional[str] = None):
        # åˆå§‹åŒ–åŸºç¤æª¢æ¸¬å™¨ - åªåœ¨æœ‰æ¨¡å‹æ™‚æ‰è¼‰å…¥ BERT
        self.hybrid_detector = HybridLevelSymbolDetector(model_path if model_path else None)
        
        # è‡ªé©æ‡‰æª¢æ¸¬çµæœ
        self.detection_results = []
        
        print("ğŸ§  è‡ªé©æ‡‰æ··åˆæª¢æ¸¬å™¨å·²åˆå§‹åŒ–")
        print("ç­–ç•¥ï¼šæ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨")
    
    def analyze_file_structure(self, file_path: Path) -> Tuple[bool, Dict]:
        """åˆ†ææª”æ¡ˆçµæ§‹ - ä½¿ç”¨ comprehensive_analysis çš„é‚è¼¯"""
        try:
            # ä½¿ç”¨ judgment_splitter è™•ç†å–®å€‹æª”æ¡ˆ
            success, result = process_single_file(file_path)
            
            if not success:
                return False, {}
            
            # è®€å–åŸå§‹æ•¸æ“š
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # åˆ†æç« ç¯€çµæ§‹
            sections = result['sections']
            has_main_text = bool(sections.get('main_text', []))
            has_facts = bool(sections.get('facts', []))
            has_reasons = bool(sections.get('reasons', []))
            has_facts_and_reasons = bool(sections.get('facts_and_reasons', []))
            
            # ç¢ºå®šå­¸ç¿’å€é–“é¡å‹
            learning_region = None
            learning_lines = []
            
            if has_facts_and_reasons:
                # S-D å€é–“ï¼šå¾ facts_and_reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "S-D"
                fr_lines = sections.get('facts_and_reasons', [])
                if fr_lines:
                    # ç²å– facts_and_reasons é–‹å§‹çš„è¡Œè™Ÿ
                    full_lines = data['JFULL'].split('\n')
                    fr_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [l.strip() for l in fr_lines[:3]]:
                            fr_start_line = i
                            break
                    
                    if fr_start_line is not None:
                        learning_lines = full_lines[fr_start_line:]
            
            elif has_reasons:
                # R-D å€é–“ï¼šå¾ reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "R-D"
                reasons_lines = sections.get('reasons', [])
                if reasons_lines:
                    full_lines = data['JFULL'].split('\n')
                    reasons_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [l.strip() for l in reasons_lines[:3]]:
                            reasons_start_line = i
                            break
                    
                    if reasons_start_line is not None:
                        learning_lines = full_lines[reasons_start_line:]
            
            if not learning_region:
                # æ²’æœ‰ R æˆ– S ç« ç¯€ï¼Œä½¿ç”¨å…¨æ–‡
                learning_region = "å…¨æ–‡"
                learning_lines = data['JFULL'].split('\n')
            
            structure_info = {
                'sections': sections,
                'has_main_text': has_main_text,
                'has_facts': has_facts,
                'has_reasons': has_reasons,
                'has_facts_and_reasons': has_facts_and_reasons,
                'learning_region': learning_region,
                'learning_lines': learning_lines,
                'full_text_lines': data['JFULL'].split('\n'),
                'total_lines': len(data['JFULL'].split('\n'))
            }
            
            return True, structure_info
        
        except Exception as e:
            print(f"âŒ åˆ†ææª”æ¡ˆçµæ§‹å¤±æ•—: {e}")
            return False, {}
    
    def learn_leveling_rules(self, learning_lines: List[str], learning_region: str) -> List[LevelingRule]:
        """åœ¨å­¸ç¿’å€é–“å»ºç«‹å±¤ç´šè¦å‰‡ - å®Œå…¨å‹•æ…‹å­¸ç¿’
        
        ä¸å†ä¾è³´ä»»ä½•é å®šç¾©å±¤ç´šï¼Œå®Œå…¨åŸºæ–¼æ–‡ä»¶æœ¬èº«çš„ç¬¦è™Ÿå‡ºç¾é †åº
        """
        print(f"ğŸ“ åœ¨ {learning_region} å€é–“å­¸ç¿’å±¤ç´šè¦å‰‡...")
        print(f"   å­¸ç¿’ç¯„åœ: {len(learning_lines)} è¡Œ")
        
        # åœ¨å­¸ç¿’å€é–“åŸ·è¡Œæª¢æ¸¬
        learning_results = self.hybrid_detector.detect_hybrid_markers(learning_lines)
        
        # ç²å–å­¸ç¿’å€é–“çš„å±¤ç´šåˆ†æ
        self.hybrid_detector.detection_results = learning_results
        hierarchy_analysis = self.hybrid_detector.detect_hierarchy_levels()
        
        if not hierarchy_analysis or not hierarchy_analysis.get('level_mapping'):
            print("âš ï¸ å­¸ç¿’å€é–“æœªç™¼ç¾æœ‰æ•ˆçš„å±¤ç´šè¦å‰‡")
            return []
        
        # å»ºç«‹è¦å‰‡ - å®Œå…¨åŸºæ–¼å­¸ç¿’çš„å±¤ç´š
        rules = []
        level_mapping = hierarchy_analysis['level_mapping']
        
        print(f"âœ… å­¸ç¿’åˆ° {len(level_mapping)} ç¨®ç¬¦è™Ÿé¡å‹çš„å±¤ç´šè¦å‰‡")
        
        for symbol_category, level_info in level_mapping.items():
            rule = LevelingRule(
                symbol_category=symbol_category,
                assigned_level=level_info['assigned_level'],
                confidence=level_info['count'] / len([r for r in learning_results if r.final_prediction]),
                learning_source=learning_region,
                occurrences=level_info['count'],
                examples=[ex['text'][:50] + '...' for ex in level_info['examples'][:3]]
            )
            rules.append(rule)
            
            print(f"   ğŸ“‹ {symbol_category}: Level {rule.assigned_level} (ä¿¡å¿ƒåº¦: {rule.confidence:.3f})")
        
        return rules
    
    def apply_leveling_rules(self, full_results: List[HybridDetectionResult], 
                           learned_rules: List[LevelingRule]) -> Dict:
        """å°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°å…¨æ–‡æª¢æ¸¬çµæœ"""
        print("ğŸ”§ å°‡å­¸ç¿’è¦å‰‡æ‡‰ç”¨åˆ°å…¨æ–‡...")
        
        # å»ºç«‹è¦å‰‡æ˜ å°„
        rule_mapping = {}
        for rule in learned_rules:
            rule_mapping[rule.symbol_category] = rule.assigned_level
        
        # æ‡‰ç”¨è¦å‰‡åˆ°å…¨æ–‡çµæœ
        enhanced_hierarchy = []
        unknown_categories = set()
        next_available_level = max(rule_mapping.values()) + 1 if rule_mapping else 1
        
        for result in full_results:
            if not result.final_prediction:
                continue
            
            symbol_category = result.symbol_category
            
            if symbol_category in rule_mapping:
                # ä½¿ç”¨å­¸ç¿’åˆ°çš„è¦å‰‡
                assigned_level = rule_mapping[symbol_category]
            else:
                # æ–°çš„ç¬¦è™Ÿé¡å‹ï¼Œåˆ†é…æ–°å±¤ç´š
                if symbol_category not in unknown_categories:
                    rule_mapping[symbol_category] = next_available_level
                    unknown_categories.add(symbol_category)
                    next_available_level += 1
                
                assigned_level = rule_mapping[symbol_category]
            
            enhanced_hierarchy.append({
                'line_number': result.line_number,
                'detected_symbol': result.detected_symbol,
                'symbol_category': symbol_category,
                'assigned_level': assigned_level,
                'is_learned_rule': symbol_category not in unknown_categories,
                'line_text': result.line_text,
                'method_used': result.method_used,
                'bert_score': result.bert_score
            })
        
        # å‰µå»ºå±¤ç´šæ˜ å°„çµ±è¨ˆ
        level_stats = {}
        for item in enhanced_hierarchy:
            category = item['symbol_category']
            if category not in level_stats:
                level_stats[category] = {
                    'assigned_level': item['assigned_level'],
                    'count': 0,
                    'is_learned': item['is_learned_rule'],
                    'examples': []
                }
            level_stats[category]['count'] += 1
            if len(level_stats[category]['examples']) < 3:
                level_stats[category]['examples'].append({
                    'line': item['line_number'],
                    'symbol': item['detected_symbol'],
                    'text': item['line_text'][:50] + '...'
                })
        
        print(f"âœ… è¦å‰‡æ‡‰ç”¨å®Œæˆ:")
        print(f"   å·²çŸ¥è¦å‰‡: {len(rule_mapping) - len(unknown_categories)} ç¨®")
        print(f"   æ–°ç™¼ç¾: {len(unknown_categories)} ç¨®")
        print(f"   ç¸½å±¤ç´šç¬¦è™Ÿ: {len(enhanced_hierarchy)} å€‹")
        
        return {
            'enhanced_hierarchy': enhanced_hierarchy,
            'level_mapping': level_stats,
            'rule_coverage': (len(rule_mapping) - len(unknown_categories)) / len(rule_mapping) if rule_mapping else 0,
            'total_levels': len(set(item['assigned_level'] for item in enhanced_hierarchy)),
            'total_symbols': len(enhanced_hierarchy)
        }
    
    def process_single_file(self, file_path: Path) -> Optional[IntelligentDetectionResult]:
        """è™•ç†å–®å€‹æª”æ¡ˆ - å®Œæ•´çš„è‡ªé©æ‡‰æª¢æ¸¬æµç¨‹"""
        print(f"\nğŸ” è‡ªé©æ‡‰æª¢æ¸¬: {file_path.name}")
        
        # æ­¥é©Ÿ1: æ–‡ä»¶åˆ†å¡Š
        success, structure_info = self.analyze_file_structure(file_path)
        if not success:
            print(f"âŒ æª”æ¡ˆçµæ§‹åˆ†æå¤±æ•—")
            return None
        
        learning_region = structure_info['learning_region']
        print(f"ğŸ“Š æª”æ¡ˆçµæ§‹: {learning_region} æ¨¡å¼")
        
        # æ­¥é©Ÿ2: å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬
        print("ğŸ” åŸ·è¡Œå…¨æ–‡å±¤ç´šç¬¦è™Ÿæª¢æ¸¬...")
        full_text_lines = structure_info['full_text_lines']
        full_detection_results = self.hybrid_detector.detect_hybrid_markers(full_text_lines)
        
        # æ­¥é©Ÿ3: è¦å‰‡å­¸ç¿’å€é–“
        learning_lines = structure_info['learning_lines']
        learned_rules = self.learn_leveling_rules(learning_lines, learning_region)
        
        # æ­¥é©Ÿ4: å±¤ç´šè¦å‰‡å»ºç«‹èˆ‡å…¨æ–‡æ‡‰ç”¨
        applied_hierarchy = self.apply_leveling_rules(full_detection_results, learned_rules)
        
        # è™•ç†çµ±è¨ˆ
        processing_stats = {
            'total_lines': structure_info['total_lines'],
            'learning_lines': len(learning_lines),
            'total_symbols_detected': len([r for r in full_detection_results if r.final_prediction]),
            'learned_rules_count': len(learned_rules),
            'rule_coverage': applied_hierarchy['rule_coverage'],
            'final_levels': applied_hierarchy['total_levels']
        }
        
        result = IntelligentDetectionResult(
            filename=file_path.name,
            file_structure=structure_info,
            learning_region=learning_region,
            learned_rules=learned_rules,
            full_detection_results=full_detection_results,
            applied_hierarchy=applied_hierarchy,
            processing_stats=processing_stats
        )
        
        return result
    
    def process_sample_directory(self, sample_dir: Path):
        """è™•ç† sample ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ"""
        print(f"ğŸš€ è‡ªé©æ‡‰æ‰¹é‡æª¢æ¸¬: {sample_dir}")
        print("="*80)
        
        if not sample_dir.exists():
            print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {sample_dir}")
            return
        
        json_files = list(sample_dir.glob("*.json"))
        if not json_files:
            print(f"âŒ åœ¨ {sample_dir} ä¸­æ²’æœ‰æ‰¾åˆ° JSON æª”æ¡ˆ")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")
        
        all_results = []
        learning_region_stats = {'S-D': 0, 'R-D': 0, 'å…¨æ–‡': 0}
        
        for i, json_file in enumerate(json_files, 1):
            print(f"\n[{i}/{len(json_files)}] è™•ç†: {json_file.name}")
            
            result = self.process_single_file(json_file)
            if result:
                all_results.append(result)
                learning_region_stats[result.learning_region] += 1
            else:
                print(f"âŒ è™•ç†å¤±æ•—: {json_file.name}")
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        self.generate_batch_report(all_results, learning_region_stats)
    
    def generate_batch_report(self, results: List[IntelligentDetectionResult], 
                            region_stats: Dict):
        """ç”Ÿæˆæ‰¹é‡è™•ç†å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        report_file = output_dir / f"adaptive_detection_report_{timestamp}.md"
        
        report = f"""# è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
è™•ç†æª”æ¡ˆ: {len(results)} å€‹

## ğŸ“Š æ•´é«”çµ±è¨ˆ

### å­¸ç¿’å€é–“åˆ†å¸ƒ
- **S-D å€é–“** (äº‹å¯¦ç†ç”±åˆä½µ): {region_stats['S-D']} æª”æ¡ˆ
- **R-D å€é–“** (ç†ç”±ç« ç¯€): {region_stats['R-D']} æª”æ¡ˆ  
- **å…¨æ–‡æª¢æ¸¬**: {region_stats['å…¨æ–‡']} æª”æ¡ˆ

### è™•ç†çµ±è¨ˆ
"""
        
        if results:
            total_lines = sum(r.processing_stats['total_lines'] for r in results)
            total_symbols = sum(r.processing_stats['total_symbols_detected'] for r in results)
            avg_coverage = sum(r.processing_stats['rule_coverage'] for r in results) / len(results)
            
            report += f"- ç¸½è¡Œæ•¸: {total_lines:,}\n"
            report += f"- ç¸½ç¬¦è™Ÿæ•¸: {total_symbols:,}\n"
            report += f"- å¹³å‡è¦å‰‡è¦†è“‹ç‡: {avg_coverage:.1%}\n"
            
            # å„æª”æ¡ˆè©³ç´°çµæœ
            report += "\n## ğŸ“‹ å„æª”æ¡ˆæª¢æ¸¬çµæœ\n\n"
            
            for i, result in enumerate(results, 1):
                stats = result.processing_stats
                hierarchy = result.applied_hierarchy
                
                report += f"### {i}. {result.filename}\n"
                report += f"- **å­¸ç¿’æ¨¡å¼**: {result.learning_region}\n"
                report += f"- **ç¸½è¡Œæ•¸**: {stats['total_lines']:,}\n"
                report += f"- **å­¸ç¿’ç¯„åœ**: {stats['learning_lines']:,} è¡Œ\n"
                report += f"- **æª¢æ¸¬ç¬¦è™Ÿ**: {stats['total_symbols_detected']:,} å€‹\n"
                report += f"- **å­¸ç¿’è¦å‰‡**: {stats['learned_rules_count']} ç¨®\n"
                report += f"- **è¦å‰‡è¦†è“‹**: {stats['rule_coverage']:.1%}\n"
                report += f"- **æœ€çµ‚å±¤ç´š**: {stats['final_levels']} å±¤\n"
                
                # é¡¯ç¤ºå­¸ç¿’åˆ°çš„è¦å‰‡
                if result.learned_rules:
                    report += f"\n**å­¸ç¿’è¦å‰‡:**\n"
                    for rule in result.learned_rules[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        report += f"  - {rule.symbol_category}: L{rule.assigned_level} (ä¿¡å¿ƒåº¦: {rule.confidence:.3f})\n"
                
                # é¡¯ç¤ºå±¤ç´šçµæ§‹é è¦½
                if hierarchy.get('enhanced_hierarchy'):
                    report += f"\n**å±¤ç´šçµæ§‹é è¦½:**\n"
                    for item in hierarchy['enhanced_hierarchy'][:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        learned_mark = "âœ“" if item['is_learned_rule'] else "âœ—"
                        indent = "  " * item['assigned_level']
                        report += f"  {indent}L{item['assigned_level']} {learned_mark} è¡Œ{item['line_number']:4}: {item['detected_symbol']} - {item['line_text'][:40]}...\n"
                
                report += "\n"
        
        # ä¿å­˜å ±å‘Š
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… è‡ªé©æ‡‰æª¢æ¸¬å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # ä¿å­˜è©³ç´°æ•¸æ“š
        json_file = output_dir / f"adaptive_detection_data_{timestamp}.json"
        json_data = []
        
        for result in results:
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            json_data.append({
                'filename': result.filename,
                'learning_region': result.learning_region,
                'processing_stats': result.processing_stats,
                'learned_rules': [
                    {
                        'symbol_category': rule.symbol_category,
                        'assigned_level': rule.assigned_level,
                        'confidence': rule.confidence,
                        'learning_source': rule.learning_source,
                        'occurrences': rule.occurrences,
                        'examples': rule.examples
                    } for rule in result.learned_rules
                ],
                'applied_hierarchy': result.applied_hierarchy
            })
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š è©³ç´°æ•¸æ“šå·²ä¿å­˜: {json_file}")

def main():
    """ä¸»å‡½æ•¸ - è‡ªé©æ‡‰æª¢æ¸¬æ¼”ç¤º"""
    print("ğŸ§  è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨")
    print("åŸºæ–¼  'å…ˆå­¸ç¿’å†æ‡‰ç”¨' åŸå‰‡")
    print("æ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨")
    print("="*80)
    
    # åˆå§‹åŒ–è‡ªé©æ‡‰æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = IntelligentHybridDetector(model_path if Path(model_path).exists() else None)

    # è™•ç† filtered ç›®éŒ„
    filtered_dir = Path("data/processed/filtered")
    detector.process_sample_directory(filtered_dir)

if __name__ == "__main__":
    main()

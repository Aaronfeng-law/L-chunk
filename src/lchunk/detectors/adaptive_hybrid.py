#!/usr/bin/env python3
"""
è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨ (Adaptive Hybrid Detector)
å…ˆå­¸ç¿’å†æ‡‰ç”¨" åŸå‰‡ï¼šæ–‡ä»¶åˆ†å¡Š â†’ è¦å‰‡å­¸ç¿’ â†’ å…¨æ–‡æ‡‰ç”¨

è™•ç†æµç¨‹ï¼š
1. æ–‡ä»¶åˆ†å¡Šï¼šä½¿ç”¨ comprehensive_analysis åˆ†ææ–‡ä»¶çµæ§‹
2. å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬ï¼šç”¨ hybrid_detector æª¢æ¸¬æ‰€æœ‰ç¬¦è™Ÿ
3. è¦å‰‡å­¸ç¿’å€é–“ï¼šåœ¨ R-D æˆ– S-D å€é–“å»ºç«‹å±¤ç´šè¦å‰‡
4. å±¤ç´šè¦å‰‡å»ºç«‹ï¼šåˆ†æç¬¦è™Ÿé¡å‹å’Œå±¤ç´šæ¨¡å¼
5. å…¨æ–‡æ‡‰ç”¨ï¼šå°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°æ•´å€‹æ–‡ä»¶

"""

import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import warnings

# å°å…¥ç¾æœ‰æ¨¡çµ„
sys.path.append(".")
from .hybrid import HybridLevelSymbolDetector, HybridDetectionResult
from ..analyzers.splitter import process_single_file, find_section_patterns
from ..analyzers.comprehensive import analyze_filtered_dataset


logger = logging.getLogger(__name__)


@dataclass
class LevelingRule:
    """å±¤ç´šè¦å‰‡å®šç¾©"""

    symbol_category: str
    assigned_level: int
    confidence: float
    learning_source: str  # "R-D", "S-D", "å…¨æ–‡"
    occurrences: int
    examples: List[str]


@dataclass
class LineBasedChunk:
    """åŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ"""

    level: int
    start_line: int
    end_line: int
    chunk_type: str  # "header", "main_text", "facts", "reasons", "facts_and_reasons", "footer", "content", "leveling_symbol"
    content_lines: List[str]
    leveling_symbol: Optional[str] = None  # å¦‚æœæ˜¯å±¤ç´šç¬¦è™Ÿè¡Œ
    chunk_id: str = ""


@dataclass
class AdaptiveDetectionResult:
    """è‡ªé©æ‡‰æª¢æ¸¬çµæœ"""

    filename: str
    file_structure: Dict  # comprehensive_analysis çµæœ
    learning_region: str  # "R-D", "S-D", "å…¨æ–‡"
    learned_rules: List[LevelingRule]
    full_detection_results: List[HybridDetectionResult]
    applied_hierarchy: Dict
    processing_stats: Dict
    line_based_chunks: Optional[List[LineBasedChunk]] = None  # æ–°å¢ï¼šåŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ


class AdaptiveHybridDetector:
    """è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å™¨"""

    def __init__(self, model_path: Optional[str] = None):
        # åˆå§‹åŒ–åŸºç¤æª¢æ¸¬å™¨ - åªåœ¨æœ‰æ¨¡å‹æ™‚æ‰è¼‰å…¥ BERT
        self.hybrid_detector = HybridLevelSymbolDetector(model_path)

        # è‡ªé©æ‡‰æª¢æ¸¬çµæœ
        self.detection_results = []

        logger.debug(
            "Adaptive hybrid detector initialized (chunking -> rule learning -> application)."
        )

    def detect_special_markers(self, lines: List[str]) -> Dict[str, List[int]]:
        """æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜ï¼šä¸»æ–‡(lv 0), ç†ç”±(lv 0), äº‹å¯¦(lv 0), äº‹å¯¦åŠç†ç”±(lv 0), é™„éŒ„(lv 0), date1(lv -2), date2(lv -2)"""
        markers = {
            "main_text": [],  # ä¸»æ–‡ (lv 0)
            "reasons": [],  # ç†ç”± (lv 0)
            "facts": [],  # äº‹å¯¦ (lv 0)
            "facts_and_reasons": [],  # äº‹å¯¦åŠç†ç”± (lv 0)
            "appendix": [],  # é™„éŒ„ (lv 0) - æ–°å¢
            "dates": [],  # æ—¥æœŸ (lv -2)
        }

        patterns = find_section_patterns()

        # ç¬¬ä¸€éšæ®µï¼šæª¢æ¸¬æ‰€æœ‰æ—¥æœŸè¡Œä»¥ç¢ºå®šDate2ä½ç½®
        date_lines = []
        for line_num, line in enumerate(lines):
            line_text = line.strip()
            if not line_text:
                continue
            if patterns["date_pattern"].search(line_text) or patterns[
                "date_pattern_strict"
            ].search(line_text):
                date_lines.append(line_num)
                markers["dates"].append(line_num)
                logger.debug(
                    "Detected 'date' marker at line %d: %s", line_num + 1, line_text
                )

        # ç¢ºå®šæœ€å¾Œä¸€å€‹æ—¥æœŸè¡Œï¼ˆDate2ï¼‰
        last_date_line = max(date_lines) if date_lines else None

        # ç¬¬äºŒéšæ®µï¼šæª¢æ¸¬å…¶ä»–ç‰¹æ®Šæ¨™è¨˜
        for line_num, line in enumerate(lines):
            line_text = line.strip()
            if not line_text:
                continue

            # æ¨™æº–åŒ–æ–‡å­—ï¼šç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŠå½¢ç©ºç™½ã€å…¨å½¢ç©ºç™½ã€tabç­‰ï¼‰
            normalized_text = (
                line_text.replace(" ", "").replace("ã€€", "").replace("\t", "")
            )

            # æª¢æ¸¬ä¸»æ–‡ (æ”¯æ´ "ä¸»æ–‡", "ä¸»ã€€æ–‡", "ä¸» æ–‡" ç­‰æ ¼å¼)
            if normalized_text == "ä¸»æ–‡":
                markers["main_text"].append(line_num)
                logger.debug(
                    "Detected 'main_text' marker at line %d: %s",
                    line_num + 1,
                    line_text,
                )

            # æª¢æ¸¬äº‹å¯¦ (æ”¯æ´ "äº‹å¯¦", "äº‹ã€€å¯¦", "äº‹ å¯¦" ç­‰æ ¼å¼)
            elif normalized_text == "äº‹å¯¦":
                markers["facts"].append(line_num)
                logger.debug(
                    "Detected 'facts' marker at line %d: %s", line_num + 1, line_text
                )

            # æª¢æ¸¬ç†ç”± (æ”¯æ´ "ç†ç”±", "ç†ã€€ç”±", "ç† ç”±" ç­‰æ ¼å¼)
            elif normalized_text == "ç†ç”±":
                markers["reasons"].append(line_num)
                logger.debug(
                    "Detected 'reasons' marker at line %d: %s", line_num + 1, line_text
                )

            # æª¢æ¸¬äº‹å¯¦åŠç†ç”± (æ”¯æ´å„ç¨®ç©ºç™½å­—ç¬¦çµ„åˆ)
            elif normalized_text in ["äº‹å¯¦åŠç†ç”±", "äº‹å¯¦å’Œç†ç”±", "äº‹å¯¦èˆ‡ç†ç”±"]:
                markers["facts_and_reasons"].append(line_num)
                logger.debug(
                    "Detected 'facts_and_reasons' marker at line %d: %s",
                    line_num + 1,
                    line_text,
                )

            # æª¢æ¸¬é™„éŒ„ - é™åˆ¶æ¢ä»¶ï¼š
            # 1. å¿…é ˆåœ¨Date2ä¹‹å¾Œï¼ˆå¦‚æœæœ‰Date2çš„è©±ï¼‰
            # 2. é™„éŒ„é—œéµè©å¿…é ˆæ˜¯è¡Œé¦–çš„å‰å…©å€‹å­—
            elif last_date_line is not None and line_num > last_date_line:
                # æª¢æŸ¥è¡Œé¦–æ˜¯å¦ä»¥é™„éŒ„é—œéµè©é–‹å§‹ï¼ˆå‰å…©å€‹å­—ï¼‰
                if len(normalized_text) >= 2:
                    first_two_chars = normalized_text[:2]
                    if first_two_chars in ["é™„éŒ„", "é™„ä»¶", "é™„åœ–", "é™„è¡¨"]:
                        markers["appendix"].append(line_num)
                        logger.debug(
                            "Detected 'appendix' marker at line %d (after Date2): %s",
                            line_num + 1,
                            line_text,
                        )

        return markers

    def create_line_based_chunks(
        self,
        lines: List[str],
        detection_results: List[HybridDetectionResult],
        learned_rules: List[LevelingRule],
    ) -> List[LineBasedChunk]:
        """åŸºæ–¼è¡Œçš„åˆ†å¡Šæ–¹æ³•ï¼š
        1. æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜ï¼šä¸»æ–‡ã€ç†ç”±ã€äº‹å¯¦ã€äº‹å¯¦åŠç†ç”±ã€æ—¥æœŸ
        2. header (lv -3): ä¸»æ–‡ä¹‹å‰çš„è¡Œ
        3. footer (lv -3): æœ€å¾Œæ—¥æœŸä¹‹å¾Œçš„è¡Œ
        4. content (lv -1): å…©å€‹å±¤ç´šç¬¦è™Ÿè¡Œä¹‹é–“çš„å…§å®¹
        5. leveling_symbol (lv 1,2,3...): æª¢æ¸¬åˆ°çš„å±¤ç´šç¬¦è™Ÿè¡Œ
        """
        logger.debug("Starting line-based chunking.")

        # æ­¥é©Ÿ1: æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜
        special_markers = self.detect_special_markers(lines)

        # æ­¥é©Ÿ2: å»ºç«‹è¦å‰‡æ˜ å°„ (å¾å­¸ç¿’éšæ®µç²å¾—)
        level_mapping = {}
        for rule in learned_rules:
            level_mapping[rule.symbol_category] = rule.assigned_level

        # æ”¶é›†æ‰€æœ‰ç‰¹æ®Šæ¨™è¨˜è¡Œï¼ˆåŒ…å«æ—¥æœŸï¼‰
        special_line_set = set()
        for marker_lines in special_markers.values():
            special_line_set.update(marker_lines)

        def emit_content_segment(segment_indices: List[int]):
            if not segment_indices:
                return
            segment_lines = [lines[idx] for idx in segment_indices]
            chunks.append(
                LineBasedChunk(
                    level=-1,
                    start_line=segment_indices[0],
                    end_line=segment_indices[-1],
                    chunk_type="content",
                    content_lines=segment_lines,
                    chunk_id=f"content_{segment_indices[0]}_{segment_indices[-1]}",
                )
            )

        def collect_content_segments(start_idx: int, end_idx: int):
            current_indices: List[int] = []
            for idx in range(start_idx, end_idx):
                if idx in special_line_set or idx in leveling_symbol_lines:
                    if current_indices:
                        emit_content_segment(current_indices)
                        current_indices = []
                    continue
                current_indices.append(idx)
            if current_indices:
                emit_content_segment(current_indices)

        # æ­¥é©Ÿ3: æ¨™è¨˜æ‰€æœ‰å±¤ç´šç¬¦è™Ÿè¡Œ
        leveling_symbol_lines = {}  # line_number -> (symbol, category, level)
        for result in detection_results:
            if result.final_prediction:
                symbol_category = result.symbol_category
                assigned_level = level_mapping.get(symbol_category, 1)  # é è¨­å±¤ç´š1
                leveling_symbol_lines[result.line_number - 1] = (
                    result.detected_symbol,
                    symbol_category,
                    assigned_level,
                )

        # æ­¥é©Ÿ4: ç¢ºå®šé—œéµåˆ†ç•Œé»
        # æ‰¾åˆ°ä¸»æ–‡ä½ç½® (Lv 0)
        main_text_line = (
            special_markers["main_text"][0] if special_markers["main_text"] else None
        )

        # æ‰¾åˆ°æœ€å¾Œçš„æ—¥æœŸä½ç½® (Lv -2)
        last_date_line = (
            max(special_markers["dates"]) if special_markers["dates"] else None
        )

        # æ­¥é©Ÿ5: å»ºç«‹åˆ†å¡Š
        chunks = []

        # Headerå€åŸŸ (Lv -3): ä¸»æ–‡ä¹‹å‰
        if main_text_line is not None and main_text_line > 0:
            header_content = lines[:main_text_line]
            chunks.append(
                LineBasedChunk(
                    level=-3,
                    start_line=0,
                    end_line=main_text_line - 1,
                    chunk_type="header",
                    content_lines=header_content,
                    chunk_id="header",
                )
            )
            logger.debug("Header segment captured (lines 1-%d).", main_text_line)

        # ç¢ºå®šå…§å®¹å€åŸŸçš„çµæŸé»
        content_end = len(lines) - 1

        # å¦‚æœæœ‰é™„éŒ„æ¨™è¨˜ï¼Œå…§å®¹å€åŸŸæ‡‰è©²åœ¨ç¬¬ä¸€å€‹é™„éŒ„æ¨™è¨˜ä¹‹å‰çµæŸ
        first_appendix_line = (
            min(special_markers["appendix"]) if special_markers["appendix"] else None
        )
        if first_appendix_line is not None:
            content_end = first_appendix_line - 1

        # è™•ç†ä¸»è¦å…§å®¹å€åŸŸ
        content_start = main_text_line if main_text_line is not None else 0

        # ç‰¹æ®Šæ¨™è¨˜è™•ç† (Lv 0)
        for marker_type, line_numbers in special_markers.items():
            for line_num in line_numbers:
                if content_start <= line_num <= content_end:
                    # è¨­å®šå±¤ç´šï¼šé™„éŒ„å’Œå…¶ä»–ä¸»è¦ç« ç¯€éƒ½æ˜¯ level 0ï¼Œæ—¥æœŸæ˜¯ level -2
                    if marker_type == "dates":
                        chunk_level = -2
                        chunk_type = "date"
                    else:
                        chunk_level = 0
                        chunk_type = marker_type

                    chunks.append(
                        LineBasedChunk(
                            level=chunk_level,
                            start_line=line_num,
                            end_line=line_num,
                            chunk_type=chunk_type,
                            content_lines=[lines[line_num]],
                            chunk_id=f"{chunk_type}_{line_num}",
                        )
                    )

        # å…§å®¹å€åŸŸåˆ†å¡Šï¼šæ ¹æ“šå±¤ç´šç¬¦è™Ÿè¡Œåˆ†å‰²
        sorted_symbol_lines = sorted(leveling_symbol_lines.keys())

        current_pos = content_start
        for symbol_line in sorted_symbol_lines:
            if symbol_line < content_start or symbol_line > content_end:
                continue

            # å±¤ç´šç¬¦è™Ÿè¡Œä¹‹å‰çš„å…§å®¹ (Lv -1)
            if current_pos < symbol_line:
                collect_content_segments(current_pos, symbol_line)

            # å±¤ç´šç¬¦è™Ÿè¡Œæœ¬èº«
            symbol, category, level = leveling_symbol_lines[symbol_line]
            chunks.append(
                LineBasedChunk(
                    level=level,
                    start_line=symbol_line,
                    end_line=symbol_line,
                    chunk_type="leveling_symbol",
                    content_lines=[lines[symbol_line]],
                    leveling_symbol=symbol,
                    chunk_id=f"level_{level}_{symbol_line}",
                )
            )

            current_pos = symbol_line + 1

        # æœ€å¾Œä¸€å€‹å±¤ç´šç¬¦è™Ÿå¾Œçš„å…§å®¹
        if current_pos <= content_end:
            collect_content_segments(current_pos, content_end + 1)

        # è™•ç†é™„éŒ„å€åŸŸï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if first_appendix_line is not None:
            appendix_start = first_appendix_line
            appendix_end = len(lines) - 1

            # æ‰¾åˆ°ä¸‹ä¸€å€‹ä¸»è¦çµæ§‹æ¨™è¨˜ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            next_major_marker = None
            for marker_type, line_numbers in special_markers.items():
                if marker_type not in ["appendix", "dates"]:
                    for line_num in line_numbers:
                        if line_num > first_appendix_line:
                            next_major_marker = (
                                min(next_major_marker, line_num)
                                if next_major_marker
                                else line_num
                            )

            if next_major_marker:
                appendix_end = next_major_marker - 1

            # è™•ç†é™„éŒ„å€åŸŸçš„å…§å®¹
            appendix_current_pos = appendix_start
            for appendix_line in special_markers["appendix"]:
                if appendix_line < appendix_start or appendix_line > appendix_end:
                    continue

                # é™„éŒ„æ¨™è¨˜è¡Œä¹‹å‰çš„å…§å®¹
                if appendix_current_pos < appendix_line:
                    appendix_content_indices = []
                    for idx in range(appendix_current_pos, appendix_line):
                        if (
                            idx not in special_line_set
                            and idx not in leveling_symbol_lines
                        ):
                            appendix_content_indices.append(idx)

                    if appendix_content_indices:
                        appendix_content_lines = [
                            lines[idx] for idx in appendix_content_indices
                        ]
                        chunks.append(
                            LineBasedChunk(
                                level=-1,
                                start_line=appendix_content_indices[0],
                                end_line=appendix_content_indices[-1],
                                chunk_type="content",
                                content_lines=appendix_content_lines,
                                chunk_id=f"content_{appendix_content_indices[0]}_{appendix_content_indices[-1]}",
                            )
                        )

                # é™„éŒ„æ¨™è¨˜è¡Œæœ¬èº«
                chunks.append(
                    LineBasedChunk(
                        level=0,
                        start_line=appendix_line,
                        end_line=appendix_line,
                        chunk_type="appendix",
                        content_lines=[lines[appendix_line]],
                        chunk_id=f"appendix_{appendix_line}",
                    )
                )

                appendix_current_pos = appendix_line + 1

            # æœ€å¾Œä¸€å€‹é™„éŒ„æ¨™è¨˜å¾Œçš„å…§å®¹
            if appendix_current_pos <= appendix_end:
                appendix_content_indices = []
                for idx in range(appendix_current_pos, appendix_end + 1):
                    if idx not in special_line_set and idx not in leveling_symbol_lines:
                        appendix_content_indices.append(idx)

                if appendix_content_indices:
                    appendix_content_lines = [
                        lines[idx] for idx in appendix_content_indices
                    ]
                    chunks.append(
                        LineBasedChunk(
                            level=-1,
                            start_line=appendix_content_indices[0],
                            end_line=appendix_content_indices[-1],
                            chunk_type="content",
                            content_lines=appendix_content_lines,
                            chunk_id=f"content_{appendix_content_indices[0]}_{appendix_content_indices[-1]}",
                        )
                    )

        # Footerå€åŸŸ (Lv -3): æœ€å¾Œæ—¥æœŸä¹‹å¾Œï¼Œä¸”åœ¨é™„éŒ„å€åŸŸä¹‹å‰çš„å…§å®¹
        if last_date_line is not None:
            footer_start = last_date_line + 1
            footer_end = (
                first_appendix_line - 1 if first_appendix_line else len(lines) - 1
            )

            if footer_start <= footer_end:
                footer_lines = []
                for line_idx in range(footer_start, footer_end + 1):
                    if (
                        line_idx not in special_line_set
                        and line_idx not in leveling_symbol_lines
                    ):
                        footer_lines.append(lines[line_idx])

                if footer_lines:
                    chunks.append(
                        LineBasedChunk(
                            level=-3,
                            start_line=footer_start,
                            end_line=footer_end,
                            chunk_type="footer",
                            content_lines=footer_lines,
                            chunk_id="footer",
                        )
                    )
                    logger.debug(
                        "Footer segment captured (lines %d-%d), excluded special markers and appendix content.",
                        footer_start + 1,
                        footer_end + 1,
                    )

        # æŒ‰è¡Œè™Ÿæ’åº
        chunks.sort(key=lambda x: x.start_line)

        logger.info("Constructed %d line-based chunks.", len(chunks))

        # çµ±è¨ˆåˆ†å¡Šé¡å‹
        chunk_stats = {}
        for chunk in chunks:
            level_type = f"Lv{chunk.level}_{chunk.chunk_type}"
            chunk_stats[level_type] = chunk_stats.get(level_type, 0) + 1

        logger.debug("Chunk statistics:")
        for level_type, count in sorted(chunk_stats.items()):
            logger.debug("   %s: %d", level_type, count)

        return chunks

    @staticmethod
    def _chunk_to_machine_node(chunk: LineBasedChunk) -> Dict[str, Any]:
        """å°‡åˆ†å¡Šè½‰æ›ç‚ºæ©Ÿå™¨å¯è®€ç¯€é»ã€‚"""
        return {
            "level": chunk.level,
            "chunk_type": chunk.chunk_type,
            "start_line": chunk.start_line + 1,
            "end_line": chunk.end_line + 1,
            "content_lines": list(chunk.content_lines),
            "leveling_symbol": chunk.leveling_symbol,
            "chunk_id": chunk.chunk_id,
            "children": [],
        }

    def build_machine_tree(self, chunks: List[LineBasedChunk]) -> List[Dict[str, Any]]:
        """å»ºç«‹æ©Ÿå™¨å¯è®€å±¤ç´šæ¨¹ï¼Œä¸¦å°‡ Lv -1 å…§å®¹é™„åŠ åˆ°å°æ‡‰çš„ä¸Šå±¤ (Lv >= 1)ã€‚"""
        if not chunks:
            return []

        ordered_chunks = sorted(chunks, key=lambda item: item.start_line)
        tree: List[Dict[str, Any]] = []
        stack: List[Dict[str, Any]] = []

        for chunk in ordered_chunks:
            if chunk.level >= 0:
                node = self._chunk_to_machine_node(chunk)
                while stack and stack[-1]["level"] >= chunk.level:
                    stack.pop()

                if stack:
                    stack[-1]["children"].append(node)
                else:
                    tree.append(node)

                stack.append(node)
            elif chunk.level == -1:
                recipient: Dict[str, Any] | None = None
                for candidate in reversed(stack):
                    if candidate["level"] >= 1:
                        recipient = candidate
                        break

                if recipient is not None:
                    recipient["content_lines"].extend(chunk.content_lines)
                    recipient["end_line"] = max(
                        recipient["end_line"], chunk.end_line + 1
                    )
                else:
                    tree.append(self._chunk_to_machine_node(chunk))
            else:
                # Header/Footer/Date ç­‰ä¿ç•™ç‚ºç¨ç«‹ç¯€é»
                tree.append(self._chunk_to_machine_node(chunk))

        return tree

    def build_machine_payload(self, result: AdaptiveDetectionResult) -> Dict[str, Any]:
        """çµ„è£æ©Ÿå™¨å¯è®€è¼¸å‡ºæ‰€éœ€çš„ payloadã€‚"""
        # machine_tree = self.build_machine_tree(result.line_based_chunks or [])
        # learned_rules = [
        #     {
        #         "symbol_category": rule.symbol_category,
        #         "assigned_level": rule.assigned_level,
        #         "confidence": rule.confidence,
        #         "learning_source": rule.learning_source,
        #         "occurrences": rule.occurrences,
        #         "examples": rule.examples,
        #     }
        #     for rule in result.learned_rules
        # ]

        return {
            "filename": result.filename,
            # "file_structure": result.file_structure,
            # "learning_region": result.learning_region,
            # "processing_stats": result.processing_stats,
            # "learned_rules": learned_rules,
            # "applied_hierarchy": result.applied_hierarchy,
            # "hierarchy": machine_tree,
            "line_based_chunks": [
                {
                    "level": chunk.level,
                    "start_line": chunk.start_line,
                    "end_line": chunk.end_line,
                    "chunk_type": chunk.chunk_type,
                    "content_lines": list(chunk.content_lines),
                    "leveling_symbol": chunk.leveling_symbol,
                    "chunk_id": chunk.chunk_id,
                }
                for chunk in result.line_based_chunks or []
            ],
        }

    def export_machine_result(
        self, result: AdaptiveDetectionResult, output_dir: Path
    ) -> Path:
        """è¼¸å‡ºæ©Ÿå™¨å¯è®€ JSON æª”ï¼Œå°‡ Lv -1 å…§å®¹é™„åŠ æ–¼ä¸Šå±¤å±¤ç´š (æ’é™¤ Lv 0)ã€‚"""
        output_dir.mkdir(parents=True, exist_ok=True)
        payload = self.build_machine_payload(result)

        target_name = f"{Path(result.filename).stem}_machine.json"
        target_path = output_dir / target_name
        target_path.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        return target_path

    def concatenate_level_content(
        self, chunks: List[LineBasedChunk]
    ) -> Dict[str, List[str]]:
        """åˆä½µç›¸åŒå±¤ç´šçš„å…§å®¹ (æ­¥é©Ÿ5: Concat the content of Lv -1 between lv 0 1 2 3 4 and so on)"""
        logger.debug("Merging content for identical levels.")

        level_content = {}

        for chunk in chunks:
            level_key = f"Lv_{chunk.level}"
            if level_key not in level_content:
                level_content[level_key] = []

            # åˆä½µå…§å®¹ï¼Œä¸¦è¨˜éŒ„åˆ†å¡Šä¿¡æ¯
            chunk_info = (
                f"[{chunk.chunk_type}:{chunk.start_line + 1}-{chunk.end_line + 1}]"
            )
            level_content[level_key].append(chunk_info)
            level_content[level_key].extend(chunk.content_lines)

        # é¡¯ç¤ºåˆä½µçµæœçµ±è¨ˆ
        logger.debug("Merged content statistics:")
        for level, content in sorted(level_content.items()):
            line_count = len([line for line in content if not line.startswith("[")])
            logger.debug("   %s: %d lines", level, line_count)

        return level_content

    def analyze_file_structure(self, file_path: Path) -> Tuple[bool, Dict]:
        """åˆ†ææª”æ¡ˆçµæ§‹ - ä½¿ç”¨ comprehensive_analysis çš„é‚è¼¯"""
        try:
            # ä½¿ç”¨ judgment_splitter è™•ç†å–®å€‹æª”æ¡ˆ
            success, result = process_single_file(file_path)

            if not success:
                return False, {}

            # è®€å–åŸå§‹æ•¸æ“š
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # åˆ†æç« ç¯€çµæ§‹ - result æ˜¯å­—å…¸
            sections = result.get("sections", {}) if isinstance(result, dict) else {}
            has_main_text = bool(sections.get("main_text", []))
            has_facts = bool(sections.get("facts", []))
            has_reasons = bool(sections.get("reasons", []))
            has_facts_and_reasons = bool(sections.get("facts_and_reasons", []))

            # ç¢ºå®šå­¸ç¿’å€é–“é¡å‹
            learning_region = None
            learning_lines = []

            if has_facts_and_reasons:
                # S-D å€é–“ï¼šå¾ facts_and_reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "S-D"
                fr_lines = sections.get("facts_and_reasons", [])
                if fr_lines:
                    # ç²å– facts_and_reasons é–‹å§‹çš„è¡Œè™Ÿ
                    full_lines = data["JFULL"].split("\n")
                    fr_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [
                            l.strip() for l in fr_lines[:3]
                        ]:
                            fr_start_line = i
                            break

                    if fr_start_line is not None:
                        learning_lines = full_lines[fr_start_line:]

            elif has_reasons:
                # R-D å€é–“ï¼šå¾ reasons åˆ°æ–‡ä»¶æœ«å°¾
                learning_region = "R-D"
                reasons_lines = sections.get("reasons", [])
                if reasons_lines:
                    full_lines = data["JFULL"].split("\n")
                    reasons_start_line = None
                    for i, line in enumerate(full_lines):
                        if line.strip() and line.strip() in [
                            l.strip() for l in reasons_lines[:3]
                        ]:
                            reasons_start_line = i
                            break

                    if reasons_start_line is not None:
                        learning_lines = full_lines[reasons_start_line:]

            if not learning_region:
                # æ²’æœ‰ R æˆ– S ç« ç¯€ï¼Œä½¿ç”¨å…¨æ–‡
                learning_region = "å…¨æ–‡"
                learning_lines = data["JFULL"].split("\n")

            structure_info = {
                "sections": sections,
                "has_main_text": has_main_text,
                "has_facts": has_facts,
                "has_reasons": has_reasons,
                "has_facts_and_reasons": has_facts_and_reasons,
                "learning_region": learning_region,
                "learning_lines": learning_lines,
                "full_text_lines": data["JFULL"].split("\n"),
                "total_lines": len(data["JFULL"].split("\n")),
            }

            return True, structure_info

        except Exception as exc:
            logger.exception("Failed to analyze file structure for %s", file_path)
            return False, {}

    def learn_leveling_rules(
        self, learning_lines: List[str], learning_region: str, verbose: bool = False
    ) -> List[LevelingRule]:
        """åœ¨å­¸ç¿’å€é–“å»ºç«‹å±¤ç´šè¦å‰‡ - å®Œå…¨å‹•æ…‹å­¸ç¿’

        ä¸å†ä¾è³´ä»»ä½•é å®šç¾©å±¤ç´šï¼Œå®Œå…¨åŸºæ–¼æ–‡ä»¶æœ¬èº«çš„ç¬¦è™Ÿå‡ºç¾é †åº
        """
        logger.info("Learning hierarchy rules within region %s.", learning_region)
        logger.debug("Learning span contains %d lines.", len(learning_lines))

        # åœ¨å­¸ç¿’å€é–“åŸ·è¡Œæª¢æ¸¬
        learning_results = self.hybrid_detector.detect_hybrid_markers(
            learning_lines, verbose=verbose
        )

        # ç²å–å­¸ç¿’å€é–“çš„å±¤ç´šåˆ†æ
        self.hybrid_detector.detection_results = learning_results
        hierarchy_analysis = self.hybrid_detector.detect_hierarchy_levels()

        if not hierarchy_analysis or not hierarchy_analysis.get("level_mapping"):
            logger.warning("No valid hierarchy rules found in the learning region.")
            return []

        # å»ºç«‹è¦å‰‡ - å®Œå…¨åŸºæ–¼å­¸ç¿’çš„å±¤ç´š
        rules = []
        level_mapping = hierarchy_analysis["level_mapping"]

        logger.info(
            "Learned hierarchy rules for %d symbol categories.", len(level_mapping)
        )

        for symbol_category, level_info in level_mapping.items():
            rule = LevelingRule(
                symbol_category=symbol_category,
                assigned_level=level_info["assigned_level"],
                confidence=level_info["count"]
                / len([r for r in learning_results if r.final_prediction]),
                learning_source=learning_region,
                occurrences=level_info["count"],
                examples=[ex["text"][:50] + "..." for ex in level_info["examples"][:3]],
            )
            rules.append(rule)

            logger.debug(
                "   %s -> level %d (confidence %.3f)",
                symbol_category,
                rule.assigned_level,
                rule.confidence,
            )

        return rules

    def apply_leveling_rules(
        self,
        full_results: List[HybridDetectionResult],
        learned_rules: List[LevelingRule],
    ) -> Dict:
        """å°‡å­¸ç¿’åˆ°çš„è¦å‰‡æ‡‰ç”¨åˆ°å…¨æ–‡æª¢æ¸¬çµæœ"""
        logger.debug("Applying learned hierarchy rules to the full document.")

        # å»ºç«‹è¦å‰‡æ˜ å°„
        rule_mapping = {}
        for rule in learned_rules:
            rule_mapping[rule.symbol_category] = rule.assigned_level

        # æ‡‰ç”¨è¦å‰‡åˆ°å…¨æ–‡çµæœ
        enhanced_hierarchy = []
        unknown_categories = set()
        next_available_level = max(rule_mapping.values()) + 1 if rule_mapping else 1

        for result in full_results:
            if not result.final_prediction:
                continue

            symbol_category = result.symbol_category

            if symbol_category in rule_mapping:
                # ä½¿ç”¨å­¸ç¿’åˆ°çš„è¦å‰‡
                assigned_level = rule_mapping[symbol_category]
            else:
                # æ–°çš„ç¬¦è™Ÿé¡å‹ï¼Œåˆ†é…æ–°å±¤ç´š
                if symbol_category not in unknown_categories:
                    rule_mapping[symbol_category] = next_available_level
                    unknown_categories.add(symbol_category)
                    next_available_level += 1

                assigned_level = rule_mapping[symbol_category]

            enhanced_hierarchy.append(
                {
                    "line_number": result.line_number,
                    "detected_symbol": result.detected_symbol,
                    "symbol_category": symbol_category,
                    "assigned_level": assigned_level,
                    "is_learned_rule": symbol_category not in unknown_categories,
                    "line_text": result.line_text,
                    "method_used": result.method_used,
                    "bert_score": result.bert_score,
                }
            )

        # å‰µå»ºå±¤ç´šæ˜ å°„çµ±è¨ˆ
        level_stats = {}
        for item in enhanced_hierarchy:
            category = item["symbol_category"]
            if category not in level_stats:
                level_stats[category] = {
                    "assigned_level": item["assigned_level"],
                    "count": 0,
                    "is_learned": item["is_learned_rule"],
                    "examples": [],
                }
            level_stats[category]["count"] += 1
            if len(level_stats[category]["examples"]) < 3:
                level_stats[category]["examples"].append(
                    {
                        "line": item["line_number"],
                        "symbol": item["detected_symbol"],
                        "text": item["line_text"][:50] + "...",
                    }
                )

        logger.info("Rule application completed:")
        logger.info(
            "   Known rules: %d types", len(rule_mapping) - len(unknown_categories)
        )
        logger.info("   New discoveries: %d types", len(unknown_categories))
        logger.info("   Total hierarchy symbols: %d", len(enhanced_hierarchy))

        return {
            "enhanced_hierarchy": enhanced_hierarchy,
            "level_mapping": level_stats,
            "rule_coverage": (len(rule_mapping) - len(unknown_categories))
            / len(rule_mapping)
            if rule_mapping
            else 0,
            "total_levels": len(
                set(item["assigned_level"] for item in enhanced_hierarchy)
            ),
            "total_symbols": len(enhanced_hierarchy),
        }

    def process_single_file(
        self, file_path: Path, verbose: bool = False
    ) -> Optional[AdaptiveDetectionResult]:
        """è™•ç†å–®å€‹æª”æ¡ˆ - å®Œæ•´çš„è‡ªé©æ‡‰æª¢æ¸¬æµç¨‹ + åŸºæ–¼è¡Œçš„åˆ†å¡Š"""
        if verbose:
            logger.info("Processing file: %s", file_path.name)

        # æ­¥é©Ÿ1: æ–‡ä»¶åˆ†å¡Š
        success, structure_info = self.analyze_file_structure(file_path)
        if not success:
            if verbose:
                logger.error("File structure analysis failed for %s", file_path)
            return None

        learning_region = structure_info["learning_region"]
        if verbose:
            logger.info("Detected document structure region: %s", learning_region)

        # æ­¥é©Ÿ2: å…¨æ–‡å±¤ç´šç¬¦è™Ÿåµæ¸¬
        if verbose:
            logger.debug("Running full-document hierarchy detection.")
        full_text_lines = structure_info["full_text_lines"]
        full_detection_results = self.hybrid_detector.detect_hybrid_markers(
            full_text_lines, verbose=verbose
        )

        # æ­¥é©Ÿ3: è¦å‰‡å­¸ç¿’å€é–“
        learning_lines = structure_info["learning_lines"]
        learned_rules = self.learn_leveling_rules(
            learning_lines, learning_region, verbose=verbose
        )

        # æ­¥é©Ÿ4: å±¤ç´šè¦å‰‡å»ºç«‹èˆ‡å…¨æ–‡æ‡‰ç”¨
        applied_hierarchy = self.apply_leveling_rules(
            full_detection_results, learned_rules
        )

        # æ­¥é©Ÿ5: åŸºæ–¼è¡Œçš„åˆ†å¡Š (æ–°å¢)
        if verbose:
            logger.debug("Executing line-based chunk generation.")
        line_based_chunks = self.create_line_based_chunks(
            full_text_lines, full_detection_results, learned_rules
        )

        # æ­¥é©Ÿ6: åˆä½µç›¸åŒå±¤ç´šå…§å®¹ (æ–°å¢)
        level_content = self.concatenate_level_content(line_based_chunks)

        # è™•ç†çµ±è¨ˆ
        processing_stats = {
            "total_lines": structure_info["total_lines"],
            "learning_lines": len(learning_lines),
            "total_symbols_detected": len(
                [r for r in full_detection_results if r.final_prediction]
            ),
            "learned_rules_count": len(learned_rules),
            "rule_coverage": applied_hierarchy["rule_coverage"],
            "final_levels": applied_hierarchy["total_levels"],
            "line_based_chunks_count": len(line_based_chunks),  # æ–°å¢
            "level_content_summary": {
                k: len([line for line in v if not line.startswith("[")])
                for k, v in level_content.items()
            },  # æ–°å¢
        }

        result = AdaptiveDetectionResult(
            filename=file_path.name,
            file_structure=structure_info,
            learning_region=learning_region,
            learned_rules=learned_rules,
            full_detection_results=full_detection_results,
            applied_hierarchy=applied_hierarchy,
            processing_stats=processing_stats,
            line_based_chunks=line_based_chunks,  # æ–°å¢
        )

        return result

    def process_sample_directory(
        self,
        sample_dir: Path,
        output_dir: Optional[Path] = None,
        max_files: Optional[int] = None,
        verbose: bool = False,
        generate_reports: bool = True,
    ):
        """è™•ç† sample ç›®éŒ„ä¸­çš„æ‰€æœ‰æª”æ¡ˆ"""
        if verbose:
            logger.info(
                "Starting batch adaptive detection for directory: %s", sample_dir
            )

        if not sample_dir.exists():
            logger.error("Directory does not exist: %s", sample_dir)
            return

        json_files = sorted(p for p in sample_dir.glob("*.json") if p.is_file())
        if max_files is not None:
            json_files = json_files[:max_files]
        if not json_files:
            if verbose:
                logger.warning("No JSON files found under %s", sample_dir)
            return

        if verbose:
            logger.info("Found %d JSON file(s) to process.", len(json_files))

        all_results = []
        learning_region_stats = {"S-D": 0, "R-D": 0, "å…¨æ–‡": 0}
        exported_files = []  # Track exported machine-readable files

        # Simple progress bar
        print(f"Processing {len(json_files)} files...")

        for i, json_file in enumerate(json_files, 1):
            if not verbose:
                # Simple progress indicator
                progress = f"[{i}/{len(json_files)}] {json_file.name}"
                print(f"\r{progress:<60}", end="", flush=True)
            else:
                logger.info(
                    "Processing file %d/%d: %s", i, len(json_files), json_file.name
                )

            result = self.process_single_file(json_file, verbose=verbose)
            if result:
                all_results.append(result)
                learning_region_stats[result.learning_region] += 1

                # Export individual machine-readable result for each file
                try:
                    export_path = self.export_machine_result(
                        result, output_dir or Path("output")
                    )
                    exported_files.append(export_path)
                    if verbose:
                        logger.debug(
                            "Exported machine result for %s to %s",
                            json_file.name,
                            export_path,
                        )
                except Exception as exc:
                    if verbose:
                        logger.error(
                            "Failed to export machine result for %s: %s",
                            json_file.name,
                            exc,
                        )
            else:
                if verbose:
                    logger.error("Processing failed for %s", json_file.name)

        if not verbose:
            print()  # New line after progress

        # ç”Ÿæˆç¶œåˆå ±å‘Š
        if generate_reports:
            self.generate_batch_report(all_results, learning_region_stats, output_dir)

        if verbose:
            logger.info(
                "Successfully processed %d files and exported %d machine results",
                len(all_results),
                len(exported_files),
            )
        else:
            if generate_reports:
                print(
                    f"âœ… Completed: {len(all_results)} files processed, {len(exported_files)} machine results exported"
                )
            else:
                print(
                    f"âœ… Completed: {len(all_results)} files processed, {len(exported_files)} machine results exported"
                )

    def generate_batch_report(
        self,
        results: List[AdaptiveDetectionResult],
        region_stats: Dict,
        output_dir: Optional[Path] = None,
    ):
        """ç”Ÿæˆæ‰¹é‡è™•ç†å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        target_dir = Path(output_dir) if output_dir else Path("output")
        target_dir.mkdir(parents=True, exist_ok=True)
        report_file = target_dir / f"adaptive_detection_report_{timestamp}.md"

        report = f"""# è‡ªé©æ‡‰æ··åˆå±¤ç´šç¬¦è™Ÿæª¢æ¸¬å ±å‘Š
ç”Ÿæˆæ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
è™•ç†æª”æ¡ˆ: {len(results)} å€‹

## ğŸ“Š æ•´é«”çµ±è¨ˆ

### å­¸ç¿’å€é–“åˆ†å¸ƒ
- **S-D å€é–“** (äº‹å¯¦ç†ç”±åˆä½µ): {region_stats["S-D"]} æª”æ¡ˆ
- **R-D å€é–“** (ç†ç”±ç« ç¯€): {region_stats["R-D"]} æª”æ¡ˆ
- **å…¨æ–‡æª¢æ¸¬**: {region_stats["å…¨æ–‡"]} æª”æ¡ˆ

### è™•ç†çµ±è¨ˆ
"""

        if results:
            total_lines = sum(r.processing_stats["total_lines"] for r in results)
            total_symbols = sum(
                r.processing_stats["total_symbols_detected"] for r in results
            )
            avg_coverage = sum(
                r.processing_stats["rule_coverage"] for r in results
            ) / len(results)

            report += f"- ç¸½è¡Œæ•¸: {total_lines:,}\n"
            report += f"- ç¸½ç¬¦è™Ÿæ•¸: {total_symbols:,}\n"
            report += f"- å¹³å‡è¦å‰‡è¦†è“‹ç‡: {avg_coverage:.1%}\n"

            # å„æª”æ¡ˆè©³ç´°çµæœ
            report += "\n## ğŸ“‹ å„æª”æ¡ˆæª¢æ¸¬çµæœ\n\n"

            for i, result in enumerate(results, 1):
                stats = result.processing_stats
                hierarchy = result.applied_hierarchy

                report += f"### {i}. {result.filename}\n"
                report += f"- **å­¸ç¿’æ¨¡å¼**: {result.learning_region}\n"
                report += f"- **ç¸½è¡Œæ•¸**: {stats['total_lines']:,}\n"
                report += f"- **å­¸ç¿’ç¯„åœ**: {stats['learning_lines']:,} è¡Œ\n"
                report += f"- **æª¢æ¸¬ç¬¦è™Ÿ**: {stats['total_symbols_detected']:,} å€‹\n"
                report += f"- **å­¸ç¿’è¦å‰‡**: {stats['learned_rules_count']} ç¨®\n"
                report += f"- **è¦å‰‡è¦†è“‹**: {stats['rule_coverage']:.1%}\n"
                report += f"- **æœ€çµ‚å±¤ç´š**: {stats['final_levels']} å±¤\n"
                report += (
                    f"- **åŸºæ–¼è¡Œåˆ†å¡Š**: {stats.get('line_based_chunks_count', 0)} å€‹\n"
                )

                # é¡¯ç¤ºå±¤ç´šå…§å®¹çµ±è¨ˆ
                if "level_content_summary" in stats:
                    report += f"\n**å±¤ç´šå…§å®¹çµ±è¨ˆ:**\n"
                    for level, line_count in sorted(
                        stats["level_content_summary"].items()
                    ):
                        report += f"  - {level}: {line_count} è¡Œ\n"

                # é¡¯ç¤ºå­¸ç¿’åˆ°çš„è¦å‰‡
                if result.learned_rules:
                    report += f"\n**å­¸ç¿’è¦å‰‡:**\n"
                    for rule in result.learned_rules[:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        report += f"  - {rule.symbol_category}: L{rule.assigned_level} (ä¿¡å¿ƒåº¦: {rule.confidence:.3f})\n"

                # é¡¯ç¤ºå±¤ç´šçµæ§‹é è¦½
                if hierarchy.get("enhanced_hierarchy"):
                    report += f"\n**å±¤ç´šçµæ§‹é è¦½:**\n"
                    for item in hierarchy["enhanced_hierarchy"][:5]:  # åªé¡¯ç¤ºå‰5å€‹
                        learned_mark = "âœ“" if item["is_learned_rule"] else "âœ—"
                        indent = "  " * item["assigned_level"]
                        report += f"  {indent}L{item['assigned_level']} {learned_mark} è¡Œ{item['line_number']:4}: {item['detected_symbol']} - {item['line_text'][:40]}...\n"

                report += "\n"

        # ä¿å­˜å ±å‘Š
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)

        logger.info("Adaptive detection report saved to %s", report_file)

        # ä¿å­˜è©³ç´°æ•¸æ“š
        json_file = target_dir / f"adaptive_detection_data_{timestamp}.json"
        json_data = []

        for result in results:
            # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            chunk_data = []
            if result.line_based_chunks:
                for chunk in result.line_based_chunks:
                    chunk_data.append(
                        {
                            "level": chunk.level,
                            "start_line": chunk.start_line,
                            "end_line": chunk.end_line,
                            "chunk_type": chunk.chunk_type,
                            "content_lines": list(chunk.content_lines),
                            "leveling_symbol": chunk.leveling_symbol,
                            "chunk_id": chunk.chunk_id,
                        }
                    )

            json_data.append(
                {
                    "filename": result.filename,
                    "learning_region": result.learning_region,
                    "processing_stats": result.processing_stats,
                    "learned_rules": [
                        {
                            "symbol_category": rule.symbol_category,
                            "assigned_level": rule.assigned_level,
                            "confidence": rule.confidence,
                            "learning_source": rule.learning_source,
                            "occurrences": rule.occurrences,
                            "examples": rule.examples,
                        }
                        for rule in result.learned_rules
                    ],
                    "applied_hierarchy": result.applied_hierarchy,
                    "line_based_chunks": chunk_data,  # æ–°å¢
                }
            )

        with open(json_file, "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        logger.info("Detailed detection data saved to %s", json_file)


def main():
    """ä¸»å‡½æ•¸ - è‡ªé©æ‡‰æª¢æ¸¬æ¼”ç¤º"""
    logger.info("Adaptive detector demo following the 'learn then apply' principle.")
    logger.info(
        "Workflow: document chunking -> rule learning -> document-wide application."
    )

    # åˆå§‹åŒ–è‡ªé©æ‡‰æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = AdaptiveHybridDetector(model_path if Path(model_path).exists() else None)

    # è™•ç† sample ç›®éŒ„
    sample_dir = Path("data/processed/sample")
    detector.process_sample_directory(sample_dir, verbose=False)


if __name__ == "__main__":
    main()

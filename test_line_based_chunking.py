#!/usr/bin/env python3
"""
æ¸¬è©¦åŸºæ–¼è¡Œçš„åˆ†å¡ŠåŠŸèƒ½
é©—è­‰æ–°çš„ line-based approach æ˜¯å¦æŒ‰ç…§è¦æ±‚å·¥ä½œï¼š
1. æª¢æ¸¬ç‰¹æ®Šæ¨™è¨˜ (ä¸»æ–‡ã€ç†ç”±ã€äº‹å¯¦ã€äº‹å¯¦åŠç†ç”±ã€æ—¥æœŸ)
2. Header/Footer å€åŸŸåŠƒåˆ† (Lv -3)
3. å…§å®¹å€åŸŸåˆ†å‰² (Lv -1)
4. å±¤ç´šç¬¦è™Ÿæª¢æ¸¬èˆ‡åˆ†é… (Lv 1,2,3...)
5. ç›¸åŒå±¤ç´šå…§å®¹åˆä½µ
"""

import sys
import json
from pathlib import Path

# æ·»åŠ  src è·¯å¾‘
sys.path.append('src')

from lchunk.detectors.adaptive_hybrid import IntelligentHybridDetector

def test_line_based_chunking():
    """æ¸¬è©¦åŸºæ–¼è¡Œçš„åˆ†å¡ŠåŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦åŸºæ–¼è¡Œçš„åˆ†å¡ŠåŠŸèƒ½")
    print("="*80)
    
    # é¸æ“‡æ¸¬è©¦æª”æ¡ˆ
    sample_dir = Path("data/samples")
    test_files = list(sample_dir.glob("*.json"))
    
    if not test_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ°æ¸¬è©¦æª”æ¡ˆ")
        return
    
    # é¸æ“‡ç¬¬ä¸€å€‹æª”æ¡ˆé€²è¡Œè©³ç´°æ¸¬è©¦
    test_file = test_files[0]
    print(f"ğŸ“ æ¸¬è©¦æª”æ¡ˆ: {test_file.name}")
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = IntelligentHybridDetector(model_path if Path(model_path).exists() else None)
    
    # è™•ç†æª”æ¡ˆ
    result = detector.process_single_file(test_file)
    
    if not result:
        print("âŒ æª”æ¡ˆè™•ç†å¤±æ•—")
        return
    
    print(f"\nâœ… æª”æ¡ˆè™•ç†æˆåŠŸ")
    print(f"ğŸ“Š å­¸ç¿’å€é–“: {result.learning_region}")
    print(f"ğŸ”¢ å­¸ç¿’è¦å‰‡æ•¸: {len(result.learned_rules)}")
    
    # æª¢æŸ¥åŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ
    if result.line_based_chunks:
        print(f"\nğŸ—ï¸ åŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ: {len(result.line_based_chunks)} å€‹åˆ†å¡Š")
        
        # æŒ‰å±¤ç´šåˆ†çµ„é¡¯ç¤º
        level_groups = {}
        for chunk in result.line_based_chunks:
            level = chunk.level
            if level not in level_groups:
                level_groups[level] = []
            level_groups[level].append(chunk)
        
        print("\nğŸ“‹ å„å±¤ç´šåˆ†å¡Šè©³æƒ…:")
        for level in sorted(level_groups.keys()):
            chunks = level_groups[level]
            print(f"\n  ğŸ¯ Level {level}: {len(chunks)} å€‹åˆ†å¡Š")
            
            for i, chunk in enumerate(chunks[:3]):  # åªé¡¯ç¤ºå‰3å€‹
                content_preview = chunk.content_lines[0][:50] + "..." if chunk.content_lines else ""
                symbol_info = f" [{chunk.leveling_symbol}]" if chunk.leveling_symbol else ""
                print(f"    {i+1}. {chunk.chunk_type}{symbol_info}: è¡Œ{chunk.start_line+1}-{chunk.end_line+1}")
                print(f"       å…§å®¹: {content_preview}")
            
            if len(chunks) > 3:
                print(f"    ... é‚„æœ‰ {len(chunks) - 3} å€‹åˆ†å¡Š")
        
        # é¡¯ç¤ºè™•ç†çµ±è¨ˆ
        stats = result.processing_stats
        print(f"\nğŸ“ˆ è™•ç†çµ±è¨ˆ:")
        print(f"  - ç¸½è¡Œæ•¸: {stats['total_lines']}")
        print(f"  - å­¸ç¿’è¡Œæ•¸: {stats['learning_lines']}")
        print(f"  - æª¢æ¸¬ç¬¦è™Ÿæ•¸: {stats['total_symbols_detected']}")
        print(f"  - å­¸ç¿’è¦å‰‡æ•¸: {stats['learned_rules_count']}")
        print(f"  - åŸºæ–¼è¡Œåˆ†å¡Šæ•¸: {stats.get('line_based_chunks_count', 0)}")
        
        if 'level_content_summary' in stats:
            print(f"\nğŸ“Š å±¤ç´šå…§å®¹çµ±è¨ˆ:")
            for level, line_count in sorted(stats['level_content_summary'].items()):
                print(f"  - {level}: {line_count} è¡Œ")
    
    else:
        print("âŒ æœªç”ŸæˆåŸºæ–¼è¡Œçš„åˆ†å¡Šçµæœ")
    
    # æ¸¬è©¦ç‰¹æ®Šæ¨™è¨˜æª¢æ¸¬
    print(f"\nğŸ” æ¸¬è©¦ç‰¹æ®Šæ¨™è¨˜æª¢æ¸¬...")
    
    # è®€å–æª”æ¡ˆå…§å®¹
    with open(test_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    lines = data['JFULL'].split('\n')
    special_markers = detector.detect_special_markers(lines)
    
    print(f"ğŸ¯ ç‰¹æ®Šæ¨™è¨˜æª¢æ¸¬çµæœ:")
    for marker_type, line_numbers in special_markers.items():
        if line_numbers:
            print(f"  - {marker_type}: {len(line_numbers)} å€‹")
            for line_num in line_numbers[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                print(f"    è¡Œ {line_num+1}: {lines[line_num].strip()}")
            if len(line_numbers) > 3:
                print(f"    ... é‚„æœ‰ {len(line_numbers) - 3} å€‹")
        else:
            print(f"  - {marker_type}: æœªæ‰¾åˆ°")

def test_multiple_files():
    """æ¸¬è©¦å¤šå€‹æª”æ¡ˆçš„æ‰¹é‡è™•ç†"""
    print(f"\nğŸš€ æ¸¬è©¦æ‰¹é‡è™•ç†...")
    
    # åˆå§‹åŒ–æª¢æ¸¬å™¨
    model_path = "models/bert/level_detector/best_model"
    detector = IntelligentHybridDetector(model_path if Path(model_path).exists() else None)
    
    # è™•ç† samples ç›®éŒ„
    sample_dir = Path("data/samples") 
    if sample_dir.exists():
        detector.process_sample_directory(sample_dir)
    else:
        print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {sample_dir}")

if __name__ == "__main__":
    print("ğŸ§ª åŸºæ–¼è¡Œçš„åˆ†å¡ŠåŠŸèƒ½æ¸¬è©¦")
    print("="*80)
    
    # æ¸¬è©¦å–®å€‹æª”æ¡ˆçš„è©³ç´°åˆ†å¡Š
    test_line_based_chunking()
    
    # æ¸¬è©¦æ‰¹é‡è™•ç†
    # test_multiple_files()
    
    print("\nâœ… æ¸¬è©¦å®Œæˆ")